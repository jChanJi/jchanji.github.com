[{"title":"Redis持久化","url":"/year/09/26/redis-persistence/","content":"\n## 前言\n> redis 的持久化方式哦有两种 RDB和AOF,本文重要介绍这两种方式的原理。\n\n<!--more-->\n\n### RDB持久化\n>服务器在载入RDB文件的期间服务器会一直处于阻塞的状态。\n\n#### SAVE\n> SAVE命令由主进程执行，会阻塞服务器的主进程 \n\n#### BGSAVE\n> BGSAVE会派生出一个子进程然后由子进程创建RDB文件，完成创建RDB文件后向父进程发送信号，而父进程通过轮询等待子进程的信号。\n\n#### RDB和AOF的执行顺序\n> 如果开启了AOF功能服务器将会优先使用AOF文件还原数据\n\n![RDB和AOF执行顺序](https://raw.githubusercontent.com/jChanJi/static_resource/master/redis/redis-persistence.png)\n\n#### BGSAVE执行时的服务器状态\n在BGSAVE执行期间：\n1. SAVE命令将会被拒绝，防止父进程和子进程同时执行两个rdbSave，产生竞争\n2. BGSAV命令将会被拒绝，两个BGSAVE命令也会产生竞争\n3. BGREWRITEAOF命令将会被延时到BGSAVE执行之后执行，相反如果先BGREWRITEAOF执行过程中BGSAVE 命令将会被拒绝。这两个命令都由子进程执行，不会产生竞争，但是为了性能考虑，不能同时有两个子进程产生大量的IO。\n\n### AOF(append only file)持久化\n> AOF持久化保存Redis服务器所执行的写命令来记录数据库状态\n\n![AOF持久化](https://raw.githubusercontent.com/jChanJi/static_resource/master/redis/redis-aof-persistence.png)\n\n#### AOF持久化的实现\n>持久化的实现可以分为三个部分：命令的追加、文件的写入、文件的同步。\n\n##### 命令追加\n>服务器执行完写命令后会以协议格式讲过命令追加到aof_buf缓冲区的末尾\n\n##### 文件写入\n>在每次结束事件循环前都会调用flushAppendOnlyFile函数考虑是否将aof_buf缓冲区内容写入和保存到AOF文件里。\n##### 文件同步\n>flushAppendOnlyFile函数的行为主要有服务器配置的appendfsync选项的值来决定的。\n\n1. always:每一个事件循环都会清空aof_buf缓冲区，效率最低但也是最安全的，只会丢失一个循环中的命令指令。\n2. everysec:每一秒钟清空一下缓冲区，对AOF文件进行同步，这个同步操作是由一个线程专门负责执行的。效率较高，只会丢失一秒钟的指令。\n3. no：将aof_buf缓冲区的内容写入带AOF文件中，但是不对AOF文件进行同步，何时同步由操作系统决定。写入速度是最快的，单次同步时长是三种模式中最长的，平均来说和everysec模式效率类似，但是出现故障停机时将丢失同步AOF文件后的所有写命令。\n\n#### AOF重写\n>随着AOF文件中的内容越来越多文件体积会越来越大，为了给AOF文件瘦身，Redis提供了重写的功能。AOF的重写不需要对现有的AOF文件有任何的读取、分析或者写入操作，重写AOF文件的内容是通过读取数据库状态（数据）实现的。重写过程：\n\n1. 创建新AOF文件\n2. 遍历数据库，忽略已经过期的键、根据键的类型对键进行重写（五种基本类型）、如果有过期时间过期时间也要进行重写\n3. AOF重写使用了一个子进程，在重写过程中父进程将重写期间的命令存放到AOF重写缓冲区中，重写完成后将缓冲区内容写到新的AOF文件中\n4. 对新的AOF文件尽心改名，覆盖现有的AOF文件\n\n> 在AOF重写过程中父进程有三个工作：1、执行客户端发送的命令。2、将执行的命令追加到AOF缓冲区、3、将执行后的写命令追加到AOF重写缓冲区。","tags":["Redis"],"categories":["-Redis"]},{"title":"java基础-IO-ByteArrayOutStream-02","url":"/year/08/15/java_io_ByteArrayOutputStream/","content":"\n## 前言\n>本篇主要介绍OutPutStream和ByteArrayOutStream。\n\n<!--more-->\n\n## OutputStream\n### public abstract void write(int b)\n将指定的字节写入到输入流中，输入的参数b只取低八位的数字，高24位的数字将被忽略。具体的实现在其子类中。\n### public void write(byte b[])\n调用public void write(byte b[], int off, int len)方法，off=0，len=b.length\n\n### public void write(byte b[], int off, int len)\n将制定的字节数组写入到输入流中，开始位置为off，长度为len.其内部原理和InputStream一样也是一个长度为len,步长为1的for循环，每次调用write(int b)方法写入一个字节。\n```java\n    for (int i = 0 ; i < len ; i++) {\n            write(b[off + i]);\n        }\n```\n\n### public void flush()\n刷新输出流，并且强制的将所有的输出缓冲流都写出，如果输入流的目标是底层操作系统提供的抽象，那么flush()只能保证先前写入的流通过操作系统写入底层设备而不能保证输入的流写入了如硬盘这样的物理设备上。flush（）和close()的区别在于flush()仅是清空缓冲区而close()先清空缓冲区后关闭流释放资源。需要注意的是，直接使用close()并不会丢失数据flush（）经常使用在对实时性比较高的网络传输中。OutputStream中此方法无效\n### public void close()\n关闭流，释放资源，被关闭的流无法再被重新打开。OutputStream中此方法无效\n\n## ByteArrayOutStream\n### 参数\n1. protected byte buf[];\n    >存数据的缓冲区\n2. protected int count;\n    >缓冲区的有效字节数，用来字节数组输出流的计数\n3. private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8;\n    >数组分配的最大的数量，一些虚拟机会存储一些头信息在数组所以会-8。\n\n### public ByteArrayOutputStream()\n构造函数，默认分配32字节给缓冲数组buf，内部调用的是public ByteArrayOutputStream(int size)函数\n\n### public ByteArrayOutputStream(int size)\n给缓冲数组分配指定字节大小的空间\n\n### private void ensureCapacity(int minCapacity)\n如果实际容量小于minCapacity，进行扩容操作。使用grow(minCapacity)方法。\n\n\n\n### private void grow(int minCapacity)\n扩容的流程是先将实际的缓冲区大小左移一位（扩大两倍）用newCapacity存储，再用newCapacity和需要的最小容量minCapacity相比，如果小于minCapacity的话，则将minCapacity赋值给newCapacity。如果newCapacity超过了数组的最大容量（MAX_ARRAY_SIZE）调用 hugeCapacity(int minCapacity)方法。最后将buf中的数据复制到长度为newCapacity的数组中并赋值给buf。\n\n### private static int hugeCapacity(int minCapacity)\n用minCapacity和MAX_ARRAY_SIZE相比，如果minCapacity比MAX_ARRAY_SIZE大，则将Integer.MAX_VALUE赋值给newCapacity否则将MAX_ARRAY_SIZE赋值给newCapacity。\n```markdown\n    private static int hugeCapacity(int minCapacity) {\n        if (minCapacity < 0) // overflow\n            throw new OutOfMemoryError();\n        return (minCapacity > MAX_ARRAY_SIZE) ?\n            Integer.MAX_VALUE :\n            MAX_ARRAY_SIZE;\n    }\n```\n\n### public synchronized void write(int b)\n将指定的字节写入到缓冲区中，在写入之前会先使用ensureCapacity(count + 1)方法验证容量是否足够。\n\n### public synchronized void write(byte b[], int off, int len)\n将off位置的len长度字节数组写入到缓冲区中，写入之前也要对数组进行容量的验证。实质也是数组的复制，调用的 System.arraycopy(b, off, buf, count, len)方法。\n\n### public synchronized void writeTo(OutputStream out)\n将**输出流**写入字**节数组输出流**中\n\n### public synchronized void reset()\n重置字节数组输出流的当前计数为0\n\n### public synchronized byte toByteArray()\n将字节数组输入流转换成字节数组，实际上就是返回字节数组输出流。\n\n### public synchronized int size()\n返回count，当前输出流的大小\n\n## 参考文献\n1. [揭开Java IO流中的flush()的神秘面纱](https://blog.csdn.net/dabing69221/article/details/16996877) \n2. [java io系列03之 ByteArrayOutputStream的简介,源码分析和示例(包括OutputStream)](http://www.cnblogs.com/skywang12345/p/io_03.html)\n3. jdk源码-OutputStream、ByteArrayOutStream","tags":["IO"],"categories":["-java"]},{"title":"java基础-IO-ByteArrayInputStream-01","url":"/year/08/13/java_io_ByteArrayInputStream/","content":"\n## 前言\n>此篇为java基础笔记的第一篇，java基础系列文章将主要记录学习IO、集合、多线程等内容，大部分是对比较函数的介绍和源码的分析。由于时间有限，函数中输入值的范围以及异常处理不加以详细介绍,以实现原理和容易出错点为主。\n\n<!--more-->\n\n## IO总体结构图\n>此图源自[赵彦军](https://www.cnblogs.com/zhaoyanjun/p/6292384.html)的博客\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/java/java_io_1.png\"></div>\n\n## InputStream\n### public abstract int read()\n读取输入流中的下一个字节的数据，返回读取流的大小，范围为0-255，如果读到了流的尾部将返回-1。此方法将被阻塞的,直到有可获取的数据或者检测到了流的结尾或者抛出异常，所以返回的字节大小不总是255，如果检测到尾部则获取的是最后不满一个字节长度的数据，抛出异常时返回的异常之前读取的字节数。\n\n### public int read(byte b[])\n将输入流全部读取带字节数组b中，放回的是实际的数组的长度，此方法调用的是public int read(byte b[], int off, int len)，off为0，len为b.length\n\n### public int read(byte b[], int off, int len)\n从指定字节位置读取指定长度的数据，并存到字节数组中，返回的是实际的读取的长度（因为此方法也是阻塞的），方法中其实是一个总数为len的for循环，每次循环调用一次read()方法读取下一字节的数据。同样，该方法数阻塞的。当off或者len为负数，len超过b.length-off的大小时会报超出索引边界异常。\n\n### public long skip(long n)\n跳过指定长度的字节，返回的是实际跳过的字节的长度（因为也是阻塞的），skip方法会存在跳过的字节数小于预期的情况。如果不对返回值进行处理的话，很容易忽视这个问题，导致结果错误。类中定义了 private static final int MAX_SKIP_BUFFER_SIZE = 2048,当跳过的字节数大于2048时会循环的调用 read(skipBuffer, 0, (int)Math.min(size, remaining))方法。代码如下\n```markdwon\n        int size = (int)Math.min(MAX_SKIP_BUFFER_SIZE, remaining);\n        byte[] skipBuffer = new byte[size];\n        while (remaining > 0) {\n            nr = read(skipBuffer, 0, (int)Math.min(size, remaining));\n            if (nr < 0) {\n                break;\n            }\n            remaining -= nr;\n        }\n```\n当跳过的字节数太大时，这有可能是影响性能的地方。\n\n###  public int available()\navailable()方法是非阻塞的，返回的是流数据的字节大小。在获取本地文件的时候也许不会有什么问题，但是在接受网络传输的流时由于数非阻塞的，返回的值和发送的总数据字节数有可能不相等。为网络通讯往往是间断性的，一串字节往往分几批进行发送。\n\n###  public void close()\n关闭流，释放资源\n\n### public synchronized void mark(int readlimit)\n标记流的当前位置，readlimit为在标记失效之前，标记位置之后最多可以读取对少字节的数据。当markSupported()返回true的时候，调用reset时会返回调用mark之后读取的所有数据，如果在调用 reset 之前可以从流中读取多于 readlimit 的字节，则根本不需要该流记住任何数据。 InputStream中不支持mark，reset操作，具体在其子类中实现。对于关闭的流mark没有效果。\n\n### public synchronized void reset()\n调用reset方法将会回到上一次使用mark的位置，\n\n### public boolean markSupported()\n测试输入流是否有mark和reset方法。\n\n## ByteArrayInputStream\nByteArrayInputStream的操作在我看来本质是是字节数组的复制。\n### 变量\n1. protected byte buf[];\n    >字节数组，用来存储从流中读取的字节\n2. protected int pos;\n    >字节流中读取下一个字节的位置\n3. protected int mark = 0;\n    >字节流中目前被标记的位置\n4. protected int count;\n    >字节流的长度\n\n### public ByteArrayInputStream(byte buf[])\n将传入的字节数组参数保存在buf参数中\n\n### public ByteArrayInputStream(byte buf[], int offset, int length)\n将读入的字节流数组buf[]存到this.buf中并设置读取下一个字节的位置pos为偏移量offset，this.buf的长度为min(offset + length,buf.length)\n### public synchronized int read()\n读取下一个字节，返回的读取的字节的大小，此方法是非阻塞的。当到输入流的结尾时会返回-1\n### public synchronized int read(byte b[], int off, int len)\n将输入的字节流从off位置开始读取len长度的字节，其原理其实是数组的复制\n```markdwon\n System.arraycopy(buf, pos, b, off, len);\n```\n将b从off位置复制len长度的字节到buf中，从buf的pos位置开始\n\n### public synchronized long skip(long n)\n跳过n个字节，其实是将pos加n个数\n### public boolean markSupported()\nByteArrayInputStream的markSupported()方法返回的永远是true\n### public void mark(int readAheadLimit)\n将pos的位置记录下来，放在mark变量中\n### public synchronized void reset()\n将mark的值赋值给pos\n\n## 参考文章\n1. [java io系列02之 ByteArrayInputStream的简介,源码分析和示例(包括InputStream)](http://www.cnblogs.com/skywang12345/p/io_02.html)\n2. [Java IO流学习总结一：输入输出流](https://www.cnblogs.com/zhaoyanjun/p/6292384.html)\n3. [jdk document-InputStream](https://docs.oracle.com/javase/8/docs/api/)\n4. [jdk document-ByteArrayInputStream](https://docs.oracle.com/javase/8/docs/api/)\n5. jdk源码-InputStream、ByteArrayInputStream\n","tags":["IO"],"categories":["-java"]},{"title":"Number1：java基础-<? extends T> and <? super T>","url":"/year/08/12/java_article1/","content":"\n## 前言\n>本篇主要讲解<? extends T> 和 <? super T>的一些基础概念，参考了[传送门](https://www.cnblogs.com/drizzlewithwind/p/6100164.html)文章。\n\n<!--more-->\n\n## 基本术语\n1. PECS原则\n>PECS（Producer Extends Consumer Super）原则,频繁往外读取内容的，适合用上界Extends。经常往里插入的，适合用下界Super。\n2. 基类\n>包含所有实体共性的class类型，即父类\n3. 派生类\n>利用继承机制，新的类可以从已有的类中派生，即子类\n4. 其他术语(参考Effective Java)\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/java/java3.png\"/></div>\n\n## <? extends T> and <? super T>的作用范围\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/java/java1.png\" /></div>\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/java/java2.png\"/></div>\n## 程序代码即讲解\n```markdown\npublic class Main {\n\n    public static void main(String[] args) {\n        System.out.println(\"Hello World!\");\n    }\n\n    private class Food {\n    }\n\n    /**\n     *水果类\n     */\n    private class Fruit extends Food {\n    }\n\n    /**\n     *苹果类，继承水果类\n     */\n    private class Apple extends Fruit {\n    }\n\n    /**\n     * 盘子\n     * @param <T>\n     */\n    private class Plate<T> {\n        private T item;\n\n        private void setItem(T item) {\n            this.item = item;\n        }\n\n        private T getItem() {\n            return this.item;\n        }\n    }\n\n    /**\n     * 新建水果类\n     */\n    Fruit fruit = new Fruit();\n    /**\n     * 新建苹果类，父类可以接受子类（基类可以接受派生类，Fruit可以接受Apple）\n     */\n    Fruit apple = new Apple();\n\n    /**\n     * Apple继承Fruit，但是Plate<Apple>不继承Plate<Fruit>,编译报错\n     */\n    Plate<Fruit> fruitPlate = new Plate<Apple>();\n\n    /**\n     *?代表不确定什么类型，extends Fruit:继承Fruit,规定了类的上限\n     *Plate<? extends Fruit>  是Plate<Apple>，Plate<Fruit>的父类\n     */\n    Plate<? extends Fruit> fruitPlate2 = new Plate<Apple>();\n\n    /**\n     *Plate<? super Fruit>是Plate<Fruit>的父类，但Plate<? super Fruit>不是Plate<Apple>的父类，所以编译报错\n     */\n    Plate<? super Fruit> fruitPlate3 = new Plate<Apple>();\n    /**\n     * Plate<? super Fruit>是 Plate<Fruit>()的父类，可以理解为用右边<>中的类型代替左边的?看是否合法，对象有set方法，但是get方法部分失效，只能够存在Object对象中\n     */\n    Plate<? super Fruit> fruitPlate4 = new Plate<Fruit>();\n    Plate<? super Fruit> fruitPlate5 = new Plate<Food>();\n    Plate<? super Fruit> fruitPlate6 = new Plate<Apple>();\n\n    public void getAndSet() {\n        //<? extends Fruit>的set方法失效，因为编译器只知道容器内是它的子类，但是不知道是具体的哪一个子类\n        fruitPlate2.setItem(new Apple());\n        fruitPlate2.setItem(new Fruit());\n        //取出来的东西也只能放在Fruit或者他的父类当中,因为取出来的是Fruit的子类,所以用Fruit或者他的父类接受都可以\n        Fruit friut = fruitPlate2.getItem();\n        Food food = fruitPlate2.getItem();\n        //<? super Fruit>的set方法有效，相当于将子类赋值给父类,将粒度小的赋值给粒度大的\n        fruitPlate4.setItem(new Fruit());\n        fruitPlate4.setItem(new Apple());\n        //<? super Fruit>的get方法只能用Object接受，相当于父类赋值给子类，取出来的粒度大的无法赋值给粒度小的\n        Apple apple = fruitPlate4.getItem();\n        Fruit fruit = fruitPlate4.getItem();\n        Object object2 = fruitPlate4.getItem();\n    }\n}\n```","tags":["范型"],"categories":["-java"]},{"title":"第三章：MySQL性能管理及架构设计-基准测试","url":"/year/08/11/Mysql_chapter3/","content":"\n## 前言\n>在优化mysql性能之前需要学习下基准测试以便对性能进行评估。基准测试不关注业务的逻辑，所使用的查询和业务的逻辑没有任何关系，而压力测试则是针对不同的主题，所使用的查询也是真实用到的数据。\n\n<!--more-->\n\n## 如何进行基准测试\n\n### 基准测试的目的\n1. 建立Mysql服务器的性能基准线。确定当前Mysql服务器运行情况\n2. 模拟比当前系统更高的负载， 以找出系统的拓展瓶颈增加数据库并发，观察QPS,TPS变化，确定并发量与性能最优关系\n3. 测试不同的硬件、软件和操作系统配置\n4. 证明新采购的硬件设备是否配置正确\n\n### 如何进行基准测试\n1. 对整个系统进行基准测试\n    1. 能够测试整个系统性能，包括web服务器缓存、数据库\n    2. 能够反映出系统中各个组件接口间的性能问题，体现真实性能情况\n    3. 测试设计复杂，消耗时间长\n2. 单独的对Mysql进行基准测试\n    1. 测试设计简单，所需耗费时间短\n    2. 无法全面了解整个系统的性能基准线\n### 常见的指标\n1. 单位时间内所处理的事务数（TPS）\n2. 单位时间内处理的查询数（QPS）\n3. 响应时间\n> 平均响应时间、最小响应时间、最大响应时间、各时间所占百分比，主要看百分比响应时间\n4. 并发量,同时处理的查询请求的数量。正在工作中的并发的操作数或者和同时工作的数量\n\n## 步骤\n1. 对整个系统还是某一组件\n2. 使用什么样的数据（生产环境的数据、生成的数据）\n3. 准备基准测试以及数据收集脚本。CPU使用率、IO、网络流量、状态与计数器信息\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter3-1.png\"></div>\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter3-2.png\"></div>\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter3-3.png\"></div>\n4. 运行基准测试\n5. 保存及分析基准测试结果\n\n## 容易忽略的问题\n1. 使用生产环境数据时只使用了部分的数据\n2. 在多用户的场景中只做单用户的测试，推荐多线程并发测试\n3. 在单服务器上测试分布式应用，推荐使用相同的架构\n4. 反复执行同一查询，容易缓存命中，无法反映真实的查询性能。\n\n## 基准测试工具\n### mysqlslap\n> mysql自带的基准测试工具，随mysql一起安装,缺点在于没有在自增id列上加上索引，对服务器的cup、IO、内存的测试体现不出来\n\n#### 特点\n1. 可以模拟服务器负载，并输出相关统计信息\n2. 可以指定也可以自动生成查询语句\n\n#### 常用参数说明\n1. --auto-generate-sql 由系统自动生成sql脚本进行测试\n2. --auto-generate-sql-add-autoincrement 在生成的表中增加自动ID\n3. --auto-generate-load-type 指定测试中使用的查询类型\n4. --auto-generate-sql-write-number 指定初始化数据时生成对的数据量\n5. -concurrency指定并发线程的数量\n6. --engine 指定测试表的存储引擎，可以使用逗号分割多个存储引擎\n7. --no-drop 指定不清理测试数据\n8. --iteratons 指定测试运行的次数\n9. --number-of-queries 指定每个线程执行的查询的数量\n10. --debug-info 指定输出额外的内存以及CPU统计信息\n11. --number-int-cols 指定测试表中包含的int类型列的数量\n12. --number-char-cols指定测试表中包含的varchar类型的数量\n13. --create-schema 制定了用于执行测试的数据库的名称\n14. --query 用于指定自定义sql的脚本\n15. --only-print 并不运行脚本，而是把生成的脚本打印出来\n\n#### 测试命令\n```markdown\nmysqlslap -uroot -p --concurrency=1,50,100,200 --iterations=3 --number-int-cols=5 --number-char-cols=5 --auto-generate-sql --auto-generate-sql-add-autoincrement --engine=myisam,innodb --number-of-queries=10 --create-schema=sbtest\n```\n### sysbench\n>更为通用的测试工具，比mysqlslap更强大，使用方法可以自行查找\n","tags":["MySQL"],"categories":["-DataBase"]},{"title":"第二章：MySQL性能管理及架构设计-影响Mysql性能的因素","url":"/year/08/10/Mysql_chapter2/","content":"\n## 前言\n>本章主要介绍影响Mysql性能的一些因素以及一些性能优化的参数的配置\n\n<!--more-->\n\n## 综述\n1. cpu、内存、磁盘等硬件\n2. 操作系统以及其参数优化\n3. 数据库存储引擎的选择\n\n    > MyISAM：不支持事务、表级锁\n\n    > InnoDB: 支持事务、行级锁、事务的ACID特性\n\n4. 数据库参数配置\n5. 数据库表结构设计、sql语句的编写\n\n## CentOS 系统参数优化\n### 内核相关参数(/etc/sysctl.conf)\n> 相关参数直接增加到文件末尾\n\n1. 每个端口最大的监听队列长度：net.core.somaxconn =65535（>2048）\n\n2. net.core.netdev_max_backlog = 65535\n\n    > 三次握手后网络端口由监听状态变成可连接状态，每个网络接口接受数据包的速率比内核处理数据包的速率快的时候，允许被发送到队列中的数据包的数目\n\n3. net.ipv4.tcp_max_syn_backlog = 65535\n\n    > 还未获得连接的请求可保存在队列中的最大数目\n\n4. 处理tcp连接等待状态的时间,加快tcp连接的回收:\n\n    > net.ipv4.tcp_fin_timeout = 10\n\n    > net.ipv4.tcp_tw_resuse = 1\n\n    > net.ipv4.tcp_tw_recycle = 1\n\n5. tcp连接接受和发送缓冲区的最小值和最大值\n\n    > net.core.wmem_default = 87380\n\n    > net.core.wmem_max = 16777216\n\n    > net.core.rmem_default = 87380\n\n    > net.core.rmem.max = 16777216\n\n6. 失效的tcp连接占用系统资源的数量\n\n    > net.ipv4.tcp_keepalive_time = 120 :tcp发送keepalive时间的间隔\n\n    > net.ipv4.tcp_keepalive_intvl = 30 : 当探测点消息未获得响应时重发该消息的时间间隔\n\n    > net.ipv4.tcp_keepalive_probes = 3 ：认定tcp连接失效之前，最多发送多少个keepalive参数消息\n\n7. kernel.shnmax = 4294967295\n    >linux中最重要的参数之一，用于定义单个共享的内存段的最大值，这个应该设置的足够大，以便在一个共享内存下容纳下整个的Innodb缓冲池的大小，取物理内存-1byte\n\n8. vm.swappiness =0\n    > 除非虚拟内存完全满了，否则不使用交换区\n\n### 增加资源限制(/etc/security/limit.conf)\n1. 控制打开文件数量的限制\n其中 *代表对所有用户有效， soft指当前系统生效的设置,hard指当前系统中所能设置的最大值，nofile 表示所限制的资源是打开文件的最大数目，65535就是限制的数目,这个文件的修改需要重启系统才能生效\n\n    > * soft nofile 65535\n\n    > * hard nofile 65535\n\n### 磁盘调度策略(/sys/block/devname/queue/scheduler)\n> 默认使用cfq完全公平的磁盘调度策略，使用下面的命令查看侧畔的当前调度策略\n```markdown\ncat /sys/block/sda/queue/scheduler\n\nnoop anticipatory deadline [cfq]\n```\n>将磁盘的调度策略改成deadline\n```markdown\necho deadline > /sys/block/sda/queue/scheduler\n```\n\n## 文件系统对性能的影响\n\n| window | linux |\n|--------|-------|\n|FAR 、NTFS|EXT3、EXT4、XFS|\n\n其中windows服务器只有NTFS,linux服务器三个中XFS比其他的性能要高\n\n### Ext3/4系统的挂载参数(/etc/fstab)\n1. 常用配置\n\n    > /dev/sda1/ext4 noatime,nodiratime,data=writeback 11\n\n    > noatime用于禁止记录文件的访问时间，nodiratime:用于禁止记录读取目录的时间\n\n2. data=writeback | ordered | journal\n    1. writeback：只有元数据写入日志，元数据写入和数据写入不同步,最快的一种配置，InnoDb有自己的事务日志所以，对于InnoDB来说是最好的选择\n    2. ordered：只会记录元数据，但是提供一些一致性的保障，写元数据之前会先写数据，使他们抱持一致，比writeback更加慢些，但是更加安全\n    3. journal：在数据写入到文件之前将先写到例子当中，最慢的一种\n\n## MySQL 体系结构\n### 体系结构图\n\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/1myqlarti.png\"></div>\n\n\n### MyISAM存储引擎\n> MySQL5.5之前版本默认存储引擎，大部分系统表和临时表会使用这种存储引擎，这里的临时表指的是在排序、分组操作中，但数量超过一定大小后，由查询优化器建立的临时表。MyISANM存储引擎\n由MUYD和MYI组成\n\n#### 特性\n1. 表级锁\n>修改表的时候都会对整张表加上锁，读取数据的时候会表加上共享锁，所以读写的并发性并不好，只读操作的话还可以接受，因为共享锁不会阻塞共享锁。\n2. 表损坏修复\n> 支持对由于意外关闭二损坏的表的恢复，这里的恢复并不是事务恢复，因为他并不是事务存储引擎，所以进行修复的时候会造成数据丢失。\n```markdown\ncheck table tablename\nrepair table tablename\n```\n3.支持全文索引和对text,blog建立前500个词的前缀索引<br/>\n4.支持数据压缩\n\n    ```markdown\n    myisampack -b -f myISam.MYI\n    ```\n\n> -f 表示强制压缩，由于数据太小不能压缩，这里使用-f强制压缩，是导致压缩后的muIsam.MYD文件更大。对于已经压缩的表只能进行读操作\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/myIsam_zip.png\"></div>\n\n#### 限制\n版本 < MySQL5.0时默认表的大小为4G,如果要存储大表则要修改MAX_Rows和AVG_ROW_LENGTH,相乘的大小为表的大小。版本>MySQL5.0时默认支持为256T。\n\n#### 适用场景\n1. 非事务型应用（不涉及财务、报表、）\n2. 只读类应用（只读）\n3. 空间类应用，MySQL5.7之前MyISAM是唯一支持空间函数的存储引擎的，如果存储GPS等空间类数据或者使用空间函数只能用MyISAM。\n\n### InnoDB存储引擎\n>Innodb使用表空间进行数据存储，innodb_file_per_table参数，on:独立表空间：tablenameibd;off:系统表空间：ibdataX,其中X代表一个数字，从1开始。\n\n#### 系统表空间和独立表空间要如何选择\n\n1. MySQL5.5之前的表默认的存储在系统表空间中，当系统的磁盘空间不够时，需要删除一些日志文件或者其他不重要的数据时，系统表空间的大小并不会减小，所以删除数据会浪费很大的空间\n2. 独立表空间可以通过optimize table 命令收缩系统文件\n3. 在对多个表进行刷新时，由于数据在系统表空间中是顺序进行刷新的，所以会产生IO瓶颈\n4. 独立表空间可以同时向多个文件刷新数据\n\n#### 从系统表空间迁移到独立表空间的步骤\n1. 使用mysqldump 导出所有数据库表数据\n2. 停止Mysql服务，修改参数,并删除Innodb相关文件\n3. 重启Mysql服务，重建Innodb系统表空间\n4. 重新导入数据\n\n#### 系统表空间存放的数据\n\n1. Innopdb数据字典信息\n\n2. Undo回滚段,Innodb临时表\n\n> 在Mysql5.7之后被移出了系统表，到那时还是有很多人默认的存放在系统表中。Undo段在Mysql5.6中就已经支持\n\n#### Innodb特性\n\n1. 事务性，完全支持事务的ACID特性（Atomicity原子性、Consistency一致性、Isolation隔离性、Durability持久性），Redo Log 和 Undo Log\n>重做日志和回滚日志，redo log用于实现事务的持久性，由内存中的重做日志(innodb_log_buffer_size设置大小,每隔一秒写到磁盘上)缓冲区和重做日志文件（ib_logfile,由innodb_log_files_in_group决定）。Undo log 用于对未提交事务进行回滚和多版本并发控制。实现了事务的原子性、一致性、持久性\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter2-1.png\"></div>\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter2-2.png\"></div>\n2. 支持行级锁，可以最大程度的支持并发，行级锁是存储引擎层实现的\n>锁主要作用是管理共享资源的并发控制，用于实现事务的隔离性。主要分类共享锁（读锁），独占锁（写锁）\n3. Innodb状态检查\n> show engine innodb status\n\n### CSV存储引擎\n> 数据以文本方式存储在文件中，。csv文件存储表内容，.csm文件存储表的元数据，.frm文件存储表结构信息。\n\n#### 特点\n1. 以csv格式进行数据存储\n2. 所有的列必须都是不能为NULL的\n3. 不支持索引\n4. 可以对数据文件直接编辑\n\n#### 使用场景\n>作为数据交换的中间表，数据 - > csv文件 - > MYSql数据目录，\n\n### Archive存储引擎\n\n>Archive存储引擎会缓存所有的写，并对数据进行压缩，比MYISAM和Innodb压缩\n\n#### 特点\n1. 以zlib对表数据进行压缩，磁盘I/O更少\n2. 数据存储在ARZ为后缀的文件中\n3. 只支持insert和select操作\n4. 只支持在自增ID上加索引\n\n#### 使用场景\n日志和数据采集类应用\n\n### Memory存储引擎\n\n>也称为HEAP存储引擎，所以数据保存在内存中\n\n#### 功能特点\n1. 支持HASH索引和BTree 索引，HASH适合等值索引，BTree适合范围查找\n2. 所有的字段都为固定的长度varchar(10) = char(10)\n3. 不支持BLOG和TEXT等大字段\n4. 存储引擎使用表级锁\n5. 表的最大大小由max_heap_table_size参数决定\n\n#### Memory存储引擎表和临时表\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter2-3.png\"></div>\n\n#### 使用场景\n1. 用于参照或者是映射表，例如邮编和地区的对应表\n2. 用于保存数据分析中产生的中间结果\n3. 用于缓存周期性聚合数据的结果集\n\n>因为Memory表的数据是易丢失的所以要保证数据必须是可以再生的，使用主从复制的策略也是不行的，主库使用Memory存储引擎而从库复制使用Innodb存储引擎保存主库的信息。因为主库重启时会重建Memory表，从数据库中对应的表也会被重建，数据一样会丢失。\n\n### Federated存储引擎\n>由于性能不太好，而且通常可以通过复制获得相同的效果，所以默认是禁止的，启用需要在启动时增加federated参数\n\n1. 提供了远程访问Mysql服务器上表的方法\n2. 本地不存储数据，数据全部放在远程服务器上\n3. 本地需要保存表结构和远程服务器的连接信息\n\n\n### Mysql服务器参数介绍\n>Mysql获取文件配置信息路径有两种，命令行参数和配置文件。命令行参数形式如：mysqld_safe --datadir = /data/sql_data,配置文件配置的时候先配置的文件会被后配置的文件覆盖，使用：mysql --help --verbose | grep -A1 'Default options',一般不建议直接修改全局变量\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/mysql/chapter2-4.png\"></div>\n\n#### Mysql参数作用域\n\n1. 全局参数\n    1. set global 参数名=参数值\n    2. set @@global.参数名 := 参数值\n    3. 全局参数配置完了需要登出才能使配置生效\n2. 会话参数\n    1. set[session]参数名 = 参数值\n    2. set @@session.参数名 := 参数值\n\n#### 内存配置相关参数\n1. 确定可以使用的内存的上限，32位系统是单线程的，配置的内存最多3G\n2. 确定 Mysql 的每个连接使用的内存，以下的参数都是为每个线程分配的，如果有100个连接则会分配100倍以下四个参数大小的和，如果参数配置的过大会造成内存的浪费和溢出\n    1. 排序缓冲区的尺寸，sort_buffer_size,Mysql不是建立连接时候就分配内存给排序缓冲区的，而是在排序的时候分配，并且分配参数指定大小的全部内存\n    2. 连接缓冲区的尺寸，join_buffer_size,多张表关联，每个join会分配一个缓冲区，所以这个值不用设置的太大\n    3. 读缓冲池的大小，read_buffer_size 在MYISAM表进行全表扫描的时候，读缓冲区的大小，只有在有查询的时候才会分配，分配设置的值的大小大小必须是4K的倍\n    4. 索引缓冲区的大小，read_rnd_buffer_size ,在有查询需要的时候会分配需要的大小而不是设置的值的全部大小\n\n3. 确定为操作系统保留多少内存\n    1. 测试，开发环境为了节约成本可以和服务器共用内存\n    2. 线上版本需要使用单独的数据库服务器\n4. 为缓冲池分配内存\n    1. Innodb_buffer_pool_size:总内存-（每个线程所需要的内存*连接数）-系统保留内存\n    2. key)buffer_size,MYISAM只缓存索引，数据由操作系统缓存\n\n#### I/O相关配置参数\n\n##### Innodb I/O相关配置\n>Innodb在提交事务的时候会使用预写日志的方式，在事务提交的时候会先写入到事务日志中，而不是每次将修改的数据刷新到文件中。事务修改时数据和索引文件会映射到表的随机的位置，所以刷新数据变更到数据文件会产大量的随机IO,而记录日志所需要的是顺序的IO.\n\n1. Innodb_log_file_size,单个事务日志文件的大小\n2. Innodb_log_files_in_group,事务日志的总个数\n3. 事务日志的总大小 = Innodb_log_files_in_group * Innodb_log_file_size\n4. Innodb_log_buffer_size,事务日志缓冲区的大小，每一秒刷新到磁盘\n5. Innodb_flush_log_at_trx_commit,事务日志刷新的频率。\n    > 0:每秒进行依次log写入cache,并刷新 log到磁盘。这种方式在服务器down机的情况下会丢失1秒钟的数据\n    > 1[默认]: 每次事务提交执行log写入cache,并刷新到磁盘。这种情况下最安全，但是效率也是最低的。\n    > 2[建议]：每次事务提交，执行log数据写到cache,每秒执行一次 刷新log到磁盘中。事务日志文件刷新到磁盘中才是完成了事务的持久化操作,而事务日志先是刷新到操作系统的cache中，再从cache中刷新到磁盘中。\n6. Innodb_flush_method = O_DIRECT，Innodb刷新的方式，影响着Innodb数据文件和日志文件如何和文件系统进行交互。linux系统设置为O_DIRECT，直接从存储系统中读或者写数据，完全屏蔽了操作系统的缓存，从而避免了Innodb和文件系统的双重缓存。\n7. Innodb_file_per_table = 1,Innodb会给每一个表建立独立的表空间，强烈建议开启\n8. Innodb_doublewrite = 1,启动Innodbd的双写缓存，主要用来避免页没有写完整导致的数据的不完整。Innodb的一个页是16k，当系统崩溃或者是程序BUG导致的一个写操作没有能够完成，造成16k的数据不能完整的写入，导致数据文件的损坏，双写缓存就是为了解决这种问题。\n\n##### MyISAM I/O相关配置\n>如果使用延迟写入，服务器出现崩溃，缓冲区中的有些块没有能够写入到磁盘中，这时候会造成MYISAM表中索引文件的损坏，这时候就需要使用repair table对表进行修复了。\n\n1. delay_key_write\n    1. OFF:每次写操作后刷新键缓冲中的脏块到磁盘\n    2. ON:只对在键表时制定了delay_key_wirte选项的表进行延迟刷新\n    3. ALL:对所有MYISAM 表都使用延迟键刷新\n\n##### 安全相关配置参数\n1. expire_logs_days 指定自动清理binlog的天数\n2. max_allowed_packet 控制Mysql可以接受的包的大小，一般为32M，使用主从服务器的情况要保证参数一致\n3. skip_name_resolve 禁用DNS查找。当连接数据库时mysql会试图确定连接的mysql客户端的主机的域名，为了验证域名又会进行DNS的正向和反向查找，如果DNS服务器出现问题就会产生查询的堆积，最终导致连接的超时。\n4. sysdate_is_no:确保sysdate()和now()返回的结果是一样的，在一个sql中多次调用sysdate()函数可能会返回不一样的结果，会造成意想不到的结果，如基于段的主从复制中会造成主从复制的数据的不一致。\n5. read_only;禁止非super权限的用户写权限。建议在主从复制数据库中的从库中启用，它可以禁止没有任何super权限的用户对数据的变更操作，只接受从主库中传输过来的数据变更。\n6. skip_slave_start:禁止Slave自动恢复。使用在从库上，阻止mysql在重启后试图自动的进行主从复制。因为咋i一个不安全的崩溃和其他操作后，自动的复制也是不安全的。\n7. sql_model 设置Mysql所使用的sql模式。默认情况下mysql对sql语法的检查是比较宽松的，比如我们在执行分组查询时，允许查询中使用的非汇聚和函数的列不全部出现在group By从句中，这是不符合sql规范的，但是mysql并不会报错。\n    1. strict_tans_tables:如果给的数据不能插入到给定的存储引擎中，则会中断当前操作，但是对于非事务的存储引擎是没有任何影响的。\n    2. no_engine_subtitution :在建表时如果指定的存储引擎不可用不会使用默认的存储引擎建立表\n    3. no_zero_date:不接受全部为0的日期\n    4. no_zero_in_date：不接受部分日期为0的日期\n    5. only_full_group_b:在分组查询中goup by中要将所有的没有聚合函数的列都列出来，否则sql语句不能执行\n\n##### 其他常用参数配置\n1. sync_binlog 控制Mysql如何控制操作系统cache磁盘刷新binlog,默认为0,由操作系统自己决定，如果大于0则意味着两次刷新到磁盘文件之间，间隔着多少次二进制日志的写操作，通常是一次事务就是一次写操作，设置为1,每次事务都会有binlog写日志的操作，最安全，成本最高。\n2. tmp_table_size 和 max_heap_table_size控制内存临时表大小，不用设置过大\n3. max_connections 控制允许的最大连接数，默认为100，通常为2000\n\n","tags":["MySQL"],"categories":["-DataBase"]},{"title":"第一章：MySQL性能管理及架构设计-实例","url":"/year/08/10/MySQL/","content":"\n## 前言\n> 本篇文章主要讲解一些mysql优化的策略\n\n<!--more-->\n\n## 第一章 实例\n\n### 大表格\n#### 特征\n\n1. 单表超过1千万行、且有较频繁的跟新操作\n2. 单表大小超过10G\n\n#### 对DDL的影响\n1. 建立索引时间过长，MySQL版本<5.5造成死锁、MySQL版本>5.5不会造成死锁，但是会引起主从延迟\n2. 修改表结构需要长时间锁表、造成数据库引起插入操作被阻塞、从而造成主从延迟\n#### 解决方案\n##### 大表的历史数据归档\n###### 优点\n减少对前后端业务的影响\n###### 难点\n    1. 归档时间点的选择\n    2. 如何进行归档操作（大表的大量ddl操作）\n##### 分库分表\n###### 难点\n1. 分表的主键的选择\n2. 分表和跨分区数据的查询和统计\n\n### 事务\n#### 特性\n1. 原子性\n> 一个ddl操作要么全部执行、要么全部失败\n2. 一致性\n> 事务将数据库从一种一致性状态转化到另一种一致性状态，事务开始之前和结束后数据库中数据的完整性没有被破坏，\n> 举个例子，从一张银行卡中转2000￥到另外一张卡，总金额是不变的\n3. 隔离性\n> 其他数据库默认已提交读，而mysql默认可重复读\n\n    1. 未提交读（READ UNCOMMITED）\n    > 一个事务执行是可以读取另一个未提交事务的操作\n\n    2. 已提交读（READ COMMITED）\n    > 一个事务执行是可以读取另一个已提交事务的操作，是不可重复读\n\n    3. 可重复读（READ REPEATED）\n    > 同一个事务的执行结果相同，就算当其他事务提交了改变了数据\n\n    4. 可串行化\n    > 在读取的每一行上都加锁\n\n4. 持久性（durability）\n> 事务提单提交，其所做的修改就会被永久的存到数据库中\n\n#### 大事务\n> 运行时间比较长，操作比较多的事务\n\n#### 风险\n1. 锁定太多的数据，造成大量的阻塞和 锁超时\n2. 回滚时所需时间比较长\n3. 执行时间长，造成主从延时\n\n#### 解决方案\n1. 避免一次处理太多的数据，采用分批处理\n2. 移出不必要在事务中的select操作\n\n## 第二章 影响mysql性能的因素\n### 综述\n1. cpu、内存、磁盘等硬件\n2. 操作系统以及其参数优化\n3. 数据库存储引擎的选择\n\n    > MyISAM：不支持事务、表级锁\n\n    > InnoDB: 支持事务、行级锁、事务的ACID特性\n\n4. 数据库参数配置\n5. 数据库表结构设计、sql语句的编写\n\n### CentOS 系统参数优化\n#### 内核相关参数(/etc/sysctl.conf)\n> 相关参数直接增加到文件末尾\n\n1. 每个端口最大的监听队列长度：net.core.somaxconn =65535（>2048）\n\n2. net.core.netdev_max_backlog = 65535\n\n    > 三次握手后网络端口由监听状态变成可连接状态，每个网络接口接受数据包的速率比内核处理数据包的速率快的时候，允许被发送到队列中的数据包的数目\n\n3. net.ipv4.tcp_max_syn_backlog = 65535\n\n    > 还未获得连接的请求可保存在队列中的最大数目\n\n4. 处理tcp连接等待状态的时间,加快tcp连接的回收:\n\n    > net.ipv4.tcp_fin_timeout = 10\n\n    > net.ipv4.tcp_tw_resuse = 1\n\n    > net.ipv4.tcp_tw_recycle = 1\n\n5. tcp连接接受和发送缓冲区的最小值和最大值\n\n    > net.core.wmem_default = 87380\n\n    > net.core.wmem_max = 16777216\n\n    > net.core.rmem_default = 87380\n\n    > net.core.rmem.max = 16777216\n\n6. 失效的tcp连接占用系统资源的数量\n\n    > net.ipv4.tcp_keepalive_time = 120 :tcp发送keepalive时间的间隔\n\n    > net.ipv4.tcp_keepalive_intvl = 30 : 当探测点消息未获得响应时重发该消息的时间间隔\n\n    > net.ipv4.tcp_keepalive_probes = 3 ：认定tcp连接失效之前，最多发送多少个keepalive参数消息\n\n7. kernel.shnmax = 4294967295\n    >linux中最重要的参数之一，用于定义单个共享的内存段的最大值，这个应该设置的足够大，以便在一个共享内存下容纳下整个的Innodb缓冲池的大小，取物理内存-1byte\n\n8. vm.swappiness =0\n    > 除非虚拟内存完全满了，否则不使用交换区\n\n#### 增加资源限制(/etc/security/limit.conf)\n1. 控制打开文件数量的限制\n其中 *代表对所有用户有效， soft指当前系统生效的设置,hard指当前系统中所能设置的最大值，nofile 表示所限制的资源是打开文件的最大数目，65535就是限制的数目,这个文件的修改需要重启系统才能生效\n\n    > * soft nofile 65535\n\n    > * hard nofile 65535\n\n#### 磁盘调度策略(/sys/block/devname/queue/scheduler)\n> 默认使用cfq完全公平的磁盘调度策略，使用下面的命令查看侧畔的当前调度策略\n```markdown\ncat /sys/block/sda/queue/scheduler\n\nnoop anticipatory deadline [cfq]\n```\n>将磁盘的调度策略改成deadline\n```markdown\necho deadline > /sys/block/sda/queue/scheduler\n```\n\n### 文件系统对性能的影响\n\n| window | linux |\n|--------|-------|\n|FAR 、NTFS|EXT3、EXT4、XFS|\n\n其中windows服务器只有NTFS,linux服务器三个中XFS比其他的性能要高\n\n#### Ext3/4系统的挂载参数(/etc/fstab)\n1. 常用配置\n\n    > /dev/sda1/ext4 noatime,nodiratime,data=writeback 11\n\n    > noatime用于禁止记录文件的访问时间，nodiratime:用于禁止记录读取目录的时间\n\n2. data=writeback | ordered | journal\n    1. writeback：只有元数据写入日志，元数据写入和数据写入不同步,最快的一种配置，InnoDb有自己的事务日志所以，对于InnoDB来说是最好的选择\n    2. ordered：只会记录元数据，但是提供一些一致性的保障，写元数据之前会先写数据，使他们抱持一致，比writeback更加慢些，但是更加安全\n    3. journal：在数据写入到文件之前将先写到例子当中，最慢的一种\n\n### MySQL 体系结构\n\n\n\n\n\n\n\n","tags":["MySQL"],"categories":["-DataBase"]},{"title":"第十章:SpringCloud-hystrix解决雪崩效应","url":"/year/08/06/SrpingCloud-chapter10/","content":"## 前言\n> 本章节主要分析雪崩效应的原因、模拟雪崩效应、解决雪崩效应\n\n<!--more-->\n\n## 什么是雪崩效应?\n> 当所有的请求都调用了一个服务，占满了服务器的最大线程，而导致无法请求其他服务。当A服务调用B服务时超时或者错误，导致调用A服务的C服务也无法返回结果，依次类推，导致Ng个服务无法返回结果。\n\n>假设tomcat服务器的最大线程数为50，现在有50个请求从客户端同时访问/order/getOrder接口，当第51个请求访问/order/addOrder接口的时候就会发生一直等待的情况，导致请求超时。而造成雪崩效应的最主要的原因在于请求的超时。当/order/getOrder接口访问/user/get接口，由于网络延时等原因一直未收到响应，所以getOrder会一直占用着线程。\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/hystrix1.png\"></div>\n\n### 模拟雪崩效应\n\n> 在sevice-member会员服务的controller的getAllUser中将线程休眠3秒\n\n```markdown\n    @GetMapping(value = \"/getAllUser\",produces = \"application/json;charset=UTF-8\")\n    public List<String> getAllUser() {\n\n        try {\n            Thread.sleep(3000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n\n        List<String> users = new ArrayList<>();\n        users.add(\"user1\");\n        users.add(\"user2\");\n        users.add(\"user3\");\n        users.add(\"user4\");\n        users.add(\"port:\" + port);\n        return users;\n    }\n```\n\n> 在service-order-feign 订单服务中设置tomcat的最大线程数，ribbon、hystrix的超时时间\n\n```markdwon\neureka:\n  client:\n    service-url:\n      defaultZone: http://localhost:1234/eureka/\nspring:\n  application:\n    name: service-order-feign\n\nserver:\n  port: 8766\n  #设置tomcat最大线程数\n  tomcat:\n  #默认150\n    max-connections: 10\n\n#设置 ribbon 读取和连接超时时间\nribbon:\n  ReadTimeout: 60000\n  #默认两秒\n  ConnectTimeout: 60000\n\n#设置feign连接超时时间\nhystrix:\n  command:\n      default:\n        execution:\n          isolation:\n            thread:\n              timeoutInMilliseconds: 5000\n```\n\n> 在service-order-feign 的controller中添加接口,用于在发生雪崩效应的时候调用另外一个接口\n\n```markdown\n    @GetMapping(\"/userInfo\")\n    public  String getUserInfo() {\n        return \"userInfo\";\n    }\n```\n\n>使用JMeter测试，新建web测试\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/hystrix2.png\"></div>\n\n>删除下图中的三个文件\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/hystrix31.png\"></div>\n\n>设置线程数等参数\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/hystrix3.png\"></div>\n\n> 点击 Home Page 设置测试的url等信息\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/hystrix4.png\"></div>\n\n\n>启动注册中心、会员服务、订单服务、测试用例，并立刻访问接口http://localhost:8766/userInfo，如出现等待的状态则表示配置成功。\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/hystrix5.png\"></div>\n\n## 解决办法\n1. 使用超时机制\n2. 服务降级： 服务调用接口超时或者发生错误，不让其调用接口而去调用本地的failback接口。\n3. 熔断机制：类似于保险丝，对于高并发的场景，一旦达到某个请求量就会触发熔断，报错，再使用服务降级\n4. 限流机制：可以使用nginx进行限流\n5. 隔离机制：对每个接口分配一个线程池，将每个接口隔离开\n\n>修改service-order-feign中的配置，开启hystrix、设置hystrix超时时间为10秒\n\n```markdwon\neureka:\n  client:\n    service-url:\n      defaultZone: http://localhost:1234/eureka/\nspring:\n  application:\n    name: service-order-feign\n\nserver:\n  port: 8766\n  #设置tomcat最大线程数\n  tomcat:\n  #默认150\n    max-connections: 10\n\n#设置 ribbon 读取和连接超时时间\nribbon:\n  ReadTimeout: 10000\n  #默认两秒\n  ConnectTimeout: 10000\n\nfeign:\n  hystrix:\n    enabled: true\n#设置feign连接超时时间\nhystrix:\n  command:\n      default:\n        execution:\n          isolation:\n            thread:\n              timeoutInMilliseconds: 10000\n```\n\n>配置fallback处理类，新建类FallBack,继承FeignService接口\n\n```markdown\n@Component\npublic class FallBack implements FeignService {\n    @Override\n    public List<String> getAllUser() {\n        List<String> list = new ArrayList<>();\n        list.add(\"fallback\");\n        return list;\n    }\n}\n```\n>在 FeignService 中配置Fallback\n\n```markdwon\n@FeignClient(value = \"service-member\",fallback = FallBack.class)\n@Component\npublic interface FeignService {\n    @GetMapping(\"getAllUser\")\n    List<String> getAllUser();\n}\n```\n>启动项目、运行测试，第一次访问http://localhost:8766/userInfo还是等待，第二次访问不需要等待。其原因请看hystrix熔断机制原理\n\n## hystrix 熔断机制原理\n>关于熔断机制的实现原理网上很多讲的也比较详细，这里不重复阐述，可以参考[传送门](https://segmentfault.com/a/1190000005988895)。\n\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第九章:SpringCloud-feign","url":"/year/08/06/springCloud-chapter9/","content":"\n## 前言\n> 第五章中手痛reset方式调用接口，本章节使用更为常用的feign的方式调用接口,feign自带了负载均衡\n\n<!--more-->\n\n### 项目搭建\n\n> 新建项目,选择web、eureka server、feign、ribbon依赖\n\n```markdown\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.4.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n        <spring-cloud.version>Finchley.SR1</spring-cloud.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-openfeign</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n```\n>配置\n\n```markdown\neureka:\n  client:\n    service-url:\n      defaultZone: http://localhost:1234/eureka/\nspring:\n  application:\n    name: service-order-feign\nserver:\n  port: 8766\n```\n> 在启动类上加上注解\n\n```markdwon\n@SpringBootApplication\n@EnableEurekaClient\n@EnableFeignClients\npublic class ServiceOrderFeignApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(ServiceOrderFeignApplication.class, args);\n    }\n}\n```\n\n> feign调用接口,新建接口\n\n```markdown\n@FeignClient(value = \"service-member\")\n@Component\npublic interface FeignService {\n\n    @GetMapping(\"getAllUser\")\n    List<String> getAllUser();\n}\n```\n\n> controller层调用\n\n```markdown\n    @RestController\npublic class Controller {\n    @Autowired\n    FeignService feignService;\n\n    @GetMapping(value = \"gertAllUserByFeign\",produces = \"application/json;charset=utf-8\")\n    public List<String> gertAllUsers() {\n        return feignService.getAllUser();\n    }\n}\n```\n> 依次启动注册中心、service-member(8762端口)、service-member(8763端口)、service-order-feign，访问http://localhost:8766/gertAllUserByFeign两次，如果出现两个端口号的数据则配置成功。\n\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第八章:SpringCloud-配置中心","url":"/year/08/02/SpringCloud-chapter8/","content":"\n## 前言\n> 本章节主要介绍SpringCloud配置中心的实现原理和项目搭建\n\n<!--more-->\n\n## 实现原理\n\n1. 服务项目发送配置请求给配置中心\n2. 配置中心从缓存中查找配置\n3. 如果缓存中没有则从远程仓库中拉取\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/config1.png\"></div>\n\n## 项目搭建\n\n### 创建git远程配置中心\n> 创建远程仓库，上传文件config-client-dev.yml\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/config2.png\"></div>\n\n### 创建配置中心服务端\n>配置中心服务端用于获取、缓存远程仓库的配置\n\n> 1.引入web、eureka、config server 依赖，项目名称为service-config-server\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/config3.png\"></div>\n\n> 2.编写配置文件\n\n```markdown\nspring:\n  application:\n    name: service-config-server\n  cloud:\n    config:\n      server:\n        git:\n        #远程仓库地址\n          uri: https://github.com/jChanJi/service-config-server.git\n          search-paths: repo\n          #公共仓库不用写\n          username:\n          password:\n      #配置所在分支\n      label: master\n\nserver:\n  port: 8889\n\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://localhost:1234/eureka/\n```\n> 3.在启动类上加上@EnableConfigServer、@EnableEurekaClient注解\n\n> 4.启动项目，在浏览器输入http://localhost:8889/foo/dev，显示如下信息则配置成功\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/config4.png\"></div>\n\n### 创建配置中心客户端\n\n> 配置中心客户端用于获取服务端的配置\n\n> 1.引入web、eureka server、 config client 依赖，项目名称为 service-config-client\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/config5.png\"></div>\n\n> 2.在resource目录下新建并且配置bootstrap.yml文件\n\n```markdown\nspring:\n  application:\n    name: service-config-client\n  cloud:\n    config:\n      label: master\n      #指定了Spring profiles，那么这些profiles将有较高优先级\n      profile: dev\n      uri: http://localhost:8889/\nserver:\n  port: 8888\neureka:\n  client:\n    service-url:\n        defaultZone: http://localhost:1234/eureka\n```\n> 3.在启动了类上加上@EnableEurekaClient注解\n\n> 4.编写controller\n\n```markdown\n@RestController\npublic class Controller {\n\n    @Value(\"${userName}\")\n    private String userName;\n\n    @GetMapping(\"/getUserName\")\n    public String getUserName() {\n        return userName;\n    }\n}\n```\n> 5.运行项目，访问http://localhost:8888/getUserName接口如果返回结果这配置成功\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/config6.png\"></div>\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第七章:SpringCloud-接口网关","url":"/year/07/31/SpringCloud-chapter7/","content":"\n## 前言\n> 本章主要介绍接口网关的概念和搭建\n\n<!--more-->\n\n## 什么是接口网关?\n>接口网关的作用是拦截请求，类似于nginx,下面是简略原理图\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/zuul1.png\"></div>\n\n1. 客户端发送两个请求分别请求会员服务和订单服务\n2. 网官拦截,根据接口名称映射到实际的接口地址。其中接口名称根据项目名称定。\n\n## 搭建接口网关\n### 编写调用接口\n\n> 在会员服务service-member的controller中添加需要调用的接口\n\n```markdown\n   @GetMapping(\"getMemberService\")\n    public String getMemberService() {\n        return \"this is member service\";\n    }\n```\n> 在订单服务service-order的controller中添加需要调用的接口\n\n```markdown\n    @GetMapping(\"getOrderService\")\n    public String getOrderService() {\n        return \"this is order service\";\n\n    }\n```\n### 搭建网关项目\n\n1. 新建service-zuul网关项目，选择web、eureka、zuul依赖,项目名称为serice-zuul\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/zuul2.png\"></div>\n2. 在启动内上加上@EnableEurekaClient、@EnableZuulProxy注解\n3. 配置\n```markdown\neureka:\n  client:\n    serviceUrl:\n      defaultZone: http://localhost:1234/eureka/\nserver:\n  port: 8765\nspring:\n  application:\n    name: service-zuul\nzuul:\n  routes:\n    api-a:\n    #对外提供的接口\n      path: /api/member/**\n      #映射到的服务项目名称\n      service-id: service-member\n    api-b:\n      path: /api/order/**\n      service-id: service-order\n```\n4.依次启动注册中心、service-member、service-order、service-zuul,分别访问http://localhost:8765/api/member/getMemberService、http://localhost:8765/api/order/getOrderService接口，效果如下：\n\n/api/member/getMemberService\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/zuul3.png\"></div>\n\n/api/order/getOrderService\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/zuul4.png\"></div>\n\n### 网关参数拦截\n\n>网关拦截类似于过滤器，但是不同点在于过滤器在每个服务中，而网关拦截在进入服务之前\n\n>新建一个类，Filter，代码\n\n```markdown\n@Component\npublic class Filter extends ZuulFilter {\n    public static  final Logger log = LoggerFactory.getLogger(Filter.class);\n    @Override\n    public String filterType() {\n        return \"pre\";\n    }\n\n    @Override\n    public int filterOrder() {\n        return 0;\n    }\n\n    @Override\n    public boolean shouldFilter() {\n        return true;\n    }\n\n    @Override\n    public Object run() throws ZuulException {\n        RequestContext ctx = RequestContext.getCurrentContext();\n        HttpServletRequest request = ctx.getRequest();\n        log.info(String.format(\"%s >>> %s\", request.getMethod(),request.getRequestURL().toString()));\n        Object accessToken = request.getParameter(\"token\");\n        //如果有token这放行\n        if (accessToken != null) {\n            return null;\n        }\n\n        log.warn(\"token is empty\");\n   //sendZuulResponse(false),就等于对其它过滤器一路开了红灯，都不会被执行了，直到SendResponseFilter，输出Response的内容。\n        ctx.setSendZuulResponse(false);\n        ctx.setResponseStatusCode(401);\n\n        try {\n            ctx.getResponse().getWriter().write(\"token is empty\");\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\n        return null;\n    }\n}\n```\n重新启动service-zuul,访问接口，不加token时返回：\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/zuul5.png\"></div>\n加上token返回\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/zuul6.png\"></div>\n\n\n\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第六章:SpringCloud-负载均衡","url":"/year/07/31/SpringCloud-chapter6/","content":"\n## 前言\n> 本章主要介绍负载均衡的原理和使用Ribbon搭建负载均衡环境\n\n<!--more-->\n\n## 实现原理\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/load_balance.png\"></div>\n\n>1. 客户端发送请求\n2. nginx反向代理、负载均衡等\n3. 所有的服务在项目启动时全部注册到了注册中心\n4. nginx分配求求到了其中的一台提供会员服务的服务器:会员服务1，会员服务1从注册中心获取所有的订单服务的接口。\n5. 注册中心返回所有的订单服务接口给会员服务\n6. 会员服务1再将获取到的所有接口交给Ribbon做负载均衡\n7. Ribbon直接调用某一个订单服务\n\n## 搭建负载均衡\n\n>其实最简单的负载均衡的配置我们已经搭建好了，即在service-order项目中加的Ribbon依赖和在启动类中配置的负载均衡。这里需要注意的是负载均衡配置放在service-order服务中，即放在需要做负载均衡的服务中而不是注册中心。为了能够辨别清晰，我们在会员服务返回的数据中加上端口号\n\n>在service-member中返回数据的中添加端口号\n\n```markdown\n @Value(\"${server.port}\")\n    private String port;\n\n    @GetMapping(value = \"/getAllUser\",produces = \"application/json;charset=UTF-8\")\n    public List<String> getAllUser() {\n        List<String> users = new ArrayList<>();\n        users.add(\"user1\");\n        users.add(\"user2\");\n        users.add(\"user3\");\n        users.add(\"user4\");\n        users.add(\"port:\" + port);\n        return users;\n    }\n```\n> 启动项目注册中心、会员服务端口号为8762的会员服务,然后修改端口号为8763,再启动一个服务，idea启动项目默认的为单例模式，在Edited configurations界面的最右上方，将single instance only 取消勾选\n\n> 打开注册中心，发现有两个service-member服务，端口号不同\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ribbon2.png\"></div>\n\n> 启动订单服务service-order、访问接口，第一次返回的数据总端口号是8762，第二次请求的端口号数据8763，因为Ribbon默认的负载均衡策略为轮询模式\n\n第一次请求\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ribbon3.png\"></div>\n\n第二次请求\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ribbon4.png\"></div>\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第五章:SpringCloud-服务消费","url":"/year/07/27/SpringCloud-chapter5/","content":"## 前言\n> 本章节主要为eureka服务消费的配置过程,可当作是订单服务。\n\n<!--more-->\n\n## 新建项目\n>同上一章一样，新建一个项目,加入web，eureka-server,ribbon依赖，项目名称为service-order\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka9.png\" width=\"600\" height=\"600\"></div>\n\n>依赖如下所示\n```markdown\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.3.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n        <spring-cloud.version>Finchley.RELEASE</spring-cloud.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n```\n## 项目配置\n> 在启动类上配置@EnableEurekaClient,并配置Ribbon\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eruka10.png\"></div>\n\n>配置application.yml\n```markdown\neureka:\n  client:\n    serviceUrl:\n    #eureka注册中心地址\n      defaultZone: http://localhost:1234/eureka/\nserver:\n  port: 8764\nspring:\n  application:\n  #服务名称\n    name: service-order\n```\n\n## 编写测试代码\n>编写service\n\n```markdown\n@Service\npublic class OrderService {\n    @Autowired\n    RestTemplate restTemplate;\n\n    public List<String> getAllUsers() {\n       return restTemplate.getForObject(\"http://service-member/getAllUser\",List.class);\n    }\n}\n```\n>编写controller\n```markdown\n @Autowired\n    OrderService orderService;\n\n    @GetMapping(value = \"/getAllUser\",produces = \"application/json;charset=UTF-8\")\n    public List<String> getAllUser() {\n       return orderService.getAllUsers();\n    }\n```\n\n## 验证\n\n> 依次启动 注册中心、服务提供者、服务消费者，查看注册中心web界面和调用order-serivce接口，如果出现一下结果则配置成功\n\n\n> 注册中心\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka11.png\" ></div>\n\n> 接口数据\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka12.png\"></div>\n\n## 问题\n> 由于EurekaServer使用到了jackson依赖,默认的将controller层返回的数据转化成了xml格式的数据，所以要在mappering中指定返回的数据格式，在服务提供者和消费者的Controller层的mappering上加上produces = \"application/json;charset=UTF-8\"\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第四章:SpringCloud-服务注册","url":"/year/07/27/SpringCloud-chapter4/","content":"## 前言\n> 本章节主要演示eureka服务注册的配置过程,暂且可以当作是会员服务，用于被订单服务调用。\n\n<!--more-->\n\n## 新建项目\n\n>使用spring Initializr 创建一个默认的springboot工程，项目名为service-member\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka1.png\" width=\"600\" height=\"600\"></div>\n\n>点击next,选择web组件和eureka-server组件\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka5.png\" width=\"600\" height=\"600\"></div>\n\n>项目依赖如下所示\n```markdown\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.3.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n        <spring-cloud.version>Finchley.RELEASE</spring-cloud.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n## 项目配置\n\n>在启动类上加@EnableEurekaClient注解\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka6.png\"></div>\n\n>配置application.yml\n```markdown\neureka:\n  client:\n    serviceUrl:\n    #eureka注册中心的地址\n      defaultZone: http://localhost:1234/eureka/\nserver:\n  port: 8762\nspring:\n  application:\n  #工程名称\n    name: service-member\n```\n\n## 编写测试代码\n\n>新建一个controler,返回list类型数据\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka7.png\" ></div>\n\n## 启动注册中心和服务提供者\n\n>如果显示下面界面则表示成功\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka8.png\"></div>\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第三章:SpringCloud-eureka环境搭建","url":"/year/07/25/SpringCloud-chapter3/","content":"\n## 前言\n>本章节将使用idea搭建eureka环境\n<!--more-->\n\n## 创建工程\n>使用spring Initializr 创建一个默认的springboot工程\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka1.png\" width=\"600\" height=\"600\"></div>\n\n>填写工程名称，下面全部点击next,最后点击finish\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka2.png\" width=\"600\" height=\"600\"></div>\n\n## 导入依赖\n```markdown\n<parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.0.3.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n        <spring-cloud.version>Finchley.RELEASE</spring-cloud.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n```\n## 配置\n>在项目如口程序中加上@EnableEurekaServer注解\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka3.png\"></div>\n\n>将application.properties改名为application.yml,加入一下配置\n```markdown\nspring:\n  application:\n    name: eureka-server\n\nserver:\n  #注册中心的端口为1234\n  port: 1234\n\neureka:\n  instance:\n    hostname: localhost\n  client:\n  #应用为注册中心，由于不向注册中心注册自己，所以设置为false\n    register-with-eureka: false\n    #表示注册中心去检索服务，由于本身注册中心的职责是维护服务实例，因此不需要去检索，设置为false；\n    fetch-registry: false\n    serviceUrl:\n          defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/\n\nlogging:\n  file: ${spring.application.name}.log\n```\n## 运行\n>运行程序,输入localhost:1234，如果出现一下界面，界面则配置成功\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/eureka4.png\" width=\"800\" height=\"600\"></div>","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第二章:SpringCloud-SpringCloud简述","url":"/year/07/24/springCloud-chapter2/","content":"\n## 前言\n>本章节主要介绍SpringClou所解决哪些问题,以及SpringCloud有哪些模块构成。\n\n<!--more-->\n## SpringCloud解决什么样的问题\n> 上一节简单的提到过springCloud和dubbo一样是一种RPC远程调用框架，顾名思义，其要解决的问题主要集中在接口调用中。其中包含容错机制、路由策略、高并发的情况、接口限流、断路等问题。\n\n## SpringCloud由哪些服务组成\n### 注册中心\n>SpringCloud中使用eureka作为配置管理的服务注册中心、zookeeper这是dubbo官方认可的注册中心。eureka分为服务端和客户端，服务端也成为服务注册中心，负责服务的注册与发现。客户端包含服务消费者和服务生产者。下面折eureka的注册中心、消费者、生产者关系图。\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/register-center.png\"></div>\n\n### 断路器\n### 路由策略\n### 负载均衡\n### 全局锁\n### 分布式会话\n### 客户端调用\n### 接口网关\n> zuul\n## 关键词\n","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第一章:SpringCloud-网站架构演变","url":"/year/07/19/springCloud-chapter1/","content":"\n## 前言\n>本章节主要简单介绍一下网站架构的演变，从“单点应用”->\"分布式系统面向于服务架构\"->\"微服务架构\"。\n\n<!--more-->\n## 传统的web项目\n>传统的web项目主要采用三层架构，分为控制层、业务逻辑成、数据库访问层。其代码主要使用包名进行区分，其缺点在于：对于像互联网公司几十上百人的协作开发的时候，不利于对代码的管理、代码后期的维护等。一个小的改动便要重新发布整个项目，在项目很大的时候显然是不合理的。\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/three_layedr_architecture.png\" width=\"300\" height=\"400\" /></div>\n\n## 面向服务的架构\n> 面向服务的架构将业务逻辑成和数据库访访问层从三层架构中抽离出来，并提供对外访问的接口。web项目的控制层通过rpc远程调用访问暴露出来的接口。从而形成SOA的项目。\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/soa.png\"></div> \n\n>其中比较容易混淆的是“分布式开发”和“集群”的概念，分布式是指将一个大项目拆分成n个小项目，如会员系统、支付系统、消息系统等。而集群是指将具有相同功能的代码部署到多台服务器上，目的是减小服务器压力，应对高并发的情况。\n\n### 优缺点\n>面向于服务开发的优点在于：根据服务解耦代码、适合大公司多人合作。缺点在于：网络延时、维护复杂、不好整合、编写复杂。当项目较小，参与人员不多时选择传统的项目架构，当项目很大，开发人员较多时选择微服务架构。\n\n## 微服务架构\n>微服务架构也是分布式的，他是对传统的SOA架构领域的的升级，将服务尽可能的细分，使用\"http协议\"+\"json数据格式\"+\"restful接口风格\"的轻量级的架构，并且每个每个服务可以单独运行，相互解耦。\n\n<div align=\"center\"><img src=\"https://raw.githubusercontent.com/jChanJi/static_resource/master/img/micro_service.png\"></div>\n\n>其中的项目接口只能在内网中使用，和外网接口进行对接时使用https加密\n\n### 微服务架构和SOA区别\n>SOA主要针对银行使用XMl格式传输数据，是企业级的ESB服务，重量级框架。微服务架构则会更加的细分服务，使用restful风格的json格式数据传送数据，是轻量级的框架，并且每个服务可以单独部署，充分解耦\n\n## 关键词\n### rpc\n> rpc是一门远程调用的技术，使用rpc的的远程调用框架有：SpringCloud、HttpClient、hessioan、dubbo\n\n### SOA\n> 面向于服务架构（SOA），大多数使用SOAP通讯协议，即\"http协议\"+\"xml序列化和反序列化\",而Web Service 没有进行序列化和反序列化，一般银行用的较多。","tags":["微服务"],"categories":["-SpringCloud"]},{"title":"第一章：MySQL性能管理及架构设计-实例","url":"/year/06/20/Mysql_chapter1/","content":"\n##前言\n>此笔记为听慕课网上的课程《MySQL性能管理及架构设计》时记录的粗略笔记大纲，第一章主要介绍了一些实例。\n\n<!--more-->\n\n### 大表格\n#### 特征\n\n1. 单表超过1千万行、且有较频繁的跟新操作\n2. 单表大小超过10G\n\n#### 对DDL的影响\n1. 建立索引时间过长，MySQL版本<5.5造成死锁、MySQL版本>5.5不会造成死锁，但是会引起主从延迟\n2. 修改表结构需要长时间锁表、造成数据库引起插入操作被阻塞、从而造成主从延迟\n#### 解决方案\n##### 大表的历史数据归档\n###### 优点\n减少对前后端业务的影响\n###### 难点\n1. 归档时间点的选择\n2. 如何进行归档操作（大表的大量ddl操作）\n##### 分库分表\n###### 难点\n1. 分表的主键的选择\n2. 分表和跨分区数据的查询和统计\n\n### 事务\n#### 特性\n1. 原子性\n> 一个ddl操作要么全部执行、要么全部失败\n2. 一致性\n> 事务将数据库从一种一致性状态转化到另一种一致性状态，事务开始之前和结束后数据库中数据的完整性没有被破坏，\n> 举个例子，从一张银行卡中转2000￥到另外一张卡，总金额是不变的\n3. 隔离性\n> 其他数据库默认已提交读，而mysql默认可重复读\n\n    1. 未提交读（READ UNCOMMITED）\n    > 一个事务执行是可以读取另一个未提交事务的操作\n\n    2. 已提交读（READ COMMITED）\n    > 一个事务执行是可以读取另一个已提交事务的操作，是不可重复读\n\n    3. 可重复读（READ REPEATED）\n    > 同一个事务的执行结果相同，就算当其他事务提交了改变了数据\n\n    4. 可串行化\n    > 在读取的每一行上都加锁\n\n4. 持久性（durability）\n> 事务提单提交，其所做的修改就会被永久的存到数据库中\n\n#### 大事务\n> 运行时间比较长，操作比较多的事务\n\n#### 风险\n1. 锁定太多的数据，造成大量的阻塞和 锁超时\n2. 回滚时所需时间比较长\n3. 执行时间长，造成主从延时\n\n#### 解决方案\n1. 避免一次处理太多的数据，采用分批处理\n2. 移出不必要在事务中的select操作\n\n\n\n\n\n\n\n","tags":["MySQL"],"categories":["-DataBase"]},{"title":"浏览器被www.t999.cn劫持","url":"/year/02/23/xiaoma/","content":"\n## 使用小马激活后浏览器被www.t999.cn劫持\n>1、查看文件路径:C:\\Windows\\System32\\DRIVERS\\mssafel.sys\n右键>属性，时间不对，数字签名为上海域联软件技术有限公司\n\n>2、Win+R，输入regedit打开注册表，注册表路径:HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\mssafel\n注册表名称:ImagePath 注册表值:system32\\DRIVERS\\mssafel.sys\n\n>3、删除C:\\Windows\\System32\\DRIVERS\\mssafel.sys\n\n>4、重启\n","tags":["Windows"],"categories":["-others"]},{"title":"spring boot从基础到进阶","url":"/year/01/17/springbootstudy/","content":"\n## 前言\n>springboot作为springmvc的升级版，具有很多优势，下面简单一下springboot的搭建与配置，并运行一个小项目。\n\n## 运行环境\n1. jdk1.8\n2. ideaIU\n3. maven3.3.9\n<!--more-->\n## 环境搭建\n![springboot1](https://raw.githubusercontent.com/jChanJi/static_resource/master/springboot/springbboot1.PNG)\n\n![springtboo2](https://raw.githubusercontent.com/jChanJi/static_resource/master/springboot/springboot2.PNG)\n\n![springtboot3](https://raw.githubusercontent.com/jChanJi/static_resource/master/springboot/springboot3.PNG)\n\n![springtboot4](https://raw.githubusercontent.com/jChanJi/static_resource/master/springboot/spingboot4.PNG)\n\n项目的整体目录结构为：\n\n![projec_structures](https://raw.githubusercontent.com/jChanJi/static_resource/master/springboot/project_structures.PNG)\n\n>其中GirlApplication.java为程序启动的入口,application.properties文件为配置文件\n## 程序代码\n>本程序由慕课网提供教学，本人在此基础上加入了很多注释，以方便理解，项目[地址](https://github.com/jChanJi/springboot_study)\n,下面是搭建springboot应用的关键技术\n![key_technology](https://raw.githubusercontent.com/jChanJi/static_resource/master/springboot/springboot.PNG)\n","tags":["spring"],"categories":["-javaweb"]},{"title":"spark项目基础实战","url":"/year/01/15/spark_sougou_logs/","content":"\n# 前言\n>使用搜狗开源的日志分析数据等实践一些spark操作\n\n# 运行环境\n1. hadoop2.6.0\n2. spark2.2.0\n3. scala2.11.11\n4. ideaIU\n\n>注意:要配置好spark和hadoop的环境变量\n\n<!--more-->\n\n# spark shell 操作\n## 数据准备\n>[下载数据](https://pan.baidu.com/s/1jJshDXk), 密码: 21c2．其中数据格式为：访问时间　用户ID　[查询词]　该URL在返回结果中的排名　用户点击的顺序号　用户点击的URL．使用head -100查看内容:\n\n![sougou1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/sougou1.png)\n\n## 上传数据到hdfs\n```markdown\ncd  /usr/local/hadoop/sbin\n./start-all.sh #启动hadoop\ncd /usr/local/spark2.2.0/sbin\n./start-all.sh　#启动spark\njps #查看是否有datanode,namenode,master,worker\nhadoop fs -mkdir /chanji/sougou/data　#在hdfs上创建目录\nhadoop /home/chanji/trainData/sougou/sogou.500w.utf8　/chanji/sougou/data #将下载的数据上传到hdfs上\n```\n\n## 搜索结果排名第1点击次序排在第2的数据\n```markdown\ncd cd /usr/local/spark2.2.0/bin\n./spark-shell\nval rdd1 = sc.textFile(\"hdfs://localhost:9000/chanji/sougou/data/sogou.500w.utf8\") #读取文件\nval rdd2=rdd1.map(_.split(\"\\t\")).filter(_.length==6)　#将不足６个属性的元素过滤掉\nrdd2.count()　#查看记录条数\nval rdd3=rdd2.filter(_(3).toInt==1).filter(_(4).toInt==2)　＃筛选出结果排名第一，点击次序第二的数据\nrdd3.count()\nrdd3.toDebugString　#查看RDD变换过程\n```\n\n## 生成Session查询次数排行榜shell\n\n```markdown\nval rdd4 = rdd2.map(x=>(x(1),1)).reduceByKey(_+_).map(x=>(x._2,x._1)). sortByKey(false).map(x=>(x._2,x._1))\nrdd4.toDebugString\nrdd4.saveAsTextFile(\"hdfs://localhost:9000/chanji/sougou/out\")\n```\n上述代码比较复杂，我们可以拆开分析\n```markdown\nval rdd5 = rdd2.map(x => (x(1),1)) #将记录的第一个元素(session)转化成(x(1),1)的键值对\nval rdd6 = rdd５．reduceByKey(_+_) #将session相同的元素个数相加\nval rdd７　= rdd6.map(x=＞（x._2,x._1)）　＃session和数量对掉位置\nval rdd8 = rdd7.sortByKey(false) #按照倒序排列（由大到小）\nval rdd9 = rdd8.map(x._2,x._1) #再将数量与session对掉位置\nrdd9.toDebugString\nrdd９．saveAsTextFile(\"hdfs://localhost:9000/chanji/sougou/out\") #将结果存入hdfs\nhdfs dfs -getmerge hdfs://localhost:9000/chanji/sougou/out /home/chanji/trainData/sougou #将文件合并并存入到本地路径\ncd /home/chanji/trainData/sougou\nhead result　＃查看结果\n```\n# idea运行spark程序\n## 生成Session查询次数排行榜idea文件运行方式\n### 代码\n```markdown\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject Sougou_Log {\n  def main(args: Array[String]) {\n       if (args.length == 0) {\n            System.err.println(\"Usage: SogouResult <file1> <file2>\")\n            System.exit(1)\n       }\n       val conf = new SparkConf().setAppName(\"SogouResult\").setMaster(\"local\")\n        val sc = new SparkContext(conf)\n\n        //session查询次数排行榜\n        val rdd1 = sc.textFile(args(0)).map(_.split(\"\\t\")).filter(_.length==6)\n        val rdd2=rdd1.map(x=>(x(1),1)).reduceByKey(_+_).map(x=>(x._2,x._1)).sortByKey(false).map(x=>(x._2,x._1))\n         rdd2.saveAsTextFile(args(1))\n         sc.stop()\n  }\n}\n\n```\n## 运行步骤\n>1. Ctrl + F９编译\n2. 如果没有错误，run->editor configurations,在program argument 中填写：［hdfs输入路径]［hdfs输出路径］如下图:\n\n![Sougou_Log](https://raw.githubusercontent.com/jChanJi/static_resource/master/spark/sougou_log.png)\n\n>3.Shift + F10运行，注意输出路径不能与spark shell中的输出路径重复\n\n## idea打包运行join操作\n### 数据准备\n>[下载数据](https://pan.baidu.com/s/1jJsND9G),密码：cyi3，数据格式:\n\n![join](https://raw.githubusercontent.com/jChanJi/static_resource/master/spark/join.png)\n\n### 代码\n```markdown\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject Sougou_Join {\n  def main(args: Array[String]) {\n    if (args.length == 0) {\n      System.err.println(\"Usage: Join <file1> <file2>\")\n      System.exit(1)\n    }\n\n    val conf = new SparkConf().setAppName(\"Join\").setMaster(\"local\")\n    val sc = new SparkContext(conf)\n\n    val format = new java.text.SimpleDateFormat(\"yyyy-MM-dd\")\n    case class Register (d: java.util.Date, uuid: String, cust_id: String, lat: Float,lng: Float)\n    case class Click (d: java.util.Date, uuid: String, landing_page: Int)\n    val reg = sc.textFile(args(0)).map(_.split(\"\\t\")).map(r => (r(1), Register(format.parse(r(0)), r(1), r(2), r(3).toFloat, r(4).toFloat)))\n    val clk = sc.textFile(args(1)).map(_.split(\"\\t\")).map(c => (c(1), Click(format.parse(c(0)), c(1), c(2).trim.toInt)))\n    reg.join(clk).take(2).foreach(println)\n    sc.stop()\n  }\n\n}\n```\n### 生成打包文件\n>1.project structure->artifacts-> + ->jar->form modules with dependencies,module选择项目，mian class选择文件的类，下一项寻则copy to the out ...,如下图所示：\n\n![join2](https://raw.githubusercontent.com/jChanJi/static_resource/master/spark/join2.png)\n\n>2.菜单栏->build->build Artifacts.\n\n>3.项目路径/out/artifacts/scala_join中名为scala_test.jar的包复制到任意目录．\n\n```markdown\ncd /spark路径/bin\n./spark-submit --master spark://localhost:7077 --class Sougou_Join --executor-memory 1g /ja包路径／scala_test.jar hdfs://localhost:9000/chanji/sougou/data/join/reg.tsv hdfs://localhost:9000//chanji/sougou/data/join/clk.tsv #hdfs上文件存储路径\n```\n看到打印出的信息则成功：\n\n![join3](https://raw.githubusercontent.com/jChanJi/static_resource/master/spark/join3.png)\n","tags":["ideaIU"],"categories":["-bigdata"]},{"title":"scala基础学习","url":"/year/01/15/scalastudy/","content":"\n# scala基础\n## scala类型体系\n<!--more-->\n![scala](https://raw.githubusercontent.com/jChanJi/static_resource/master/scalastudy/scala%E7%B1%BB%E5%9E%8B%E4%BD%93%E7%B3%BB.png)\n\n>新建scala worksheet文件，写入一下代码。\n\n## 代码块:{}\n```markdown\n  def hello(name: String): String = {\n    s\"hello,${name}\"\n  }\n  hello(\"chanji\")\n  def hello2(name: String) = {\n    s\"hello,${name}\"\n  }\n  hello2(\"chanji2\")\n  def add(x: Int, y: Int) = x + y\n  add(2, 3)\n```\n\n## if与for表达式\n```markdown\n  val a1 = 1\n  if (a1 == 1) a1\n  if (a1 != 1) \"not one\" //返回unit()空值\n  if (a1 != 1) \"not one\" else a1\n\n  val l = List(\"alice\", \"bob\", \"cathy\")\n  for {\n    s <- l //generator\n  } println(s)\n\n  for {\n    s <- l\n    if (s.length > 3) //filter\n  } println(s)\n\n  val reslut_for = for {\n    s <- l\n    s1 = s.toUpperCase() //variable binding\n    if (s1 != \" \")\n  } yield (s1)\n```\n## try表达式\n```markdown\n  try {\n    Integer.parseInt(\"dog\")\n  } catch {\n    case _ => 0\n  } finally {\n    println(\"always be printed\")\n  }\n```\n## match\n```markdown\n  val code = 1\n  val result_match = code match {\n    case 1 => \"one\"\n    case 2 => \"two\"\n    case _ => \"others\" //default\n  }\n```\n\n## 求值策略\ncall by value　在函数运行时候会先计算参数值再执行函数，call by name则会先运行函数，参数值用到的时候再求值\n![scala1](https://raw.githubusercontent.com/jChanJi/static_resource/master/scalastudy/scala%E6%B1%82%E5%80%BC%E7%AD%96%E7%95%A5%EF%BC%91.png)\n\n![scala2](https://raw.githubusercontent.com/jChanJi/static_resource/master/scalastudy/%E6%B1%82%E5%80%BC%E7%AD%96%E7%95%A5%E4%BE%8B%E5%AD%90%EF%BC%92.png)\n```markdown\n  def bar(x: Int, y: => Int): Int = 1\n  def loop(): Int = loop()\n  bar(1, loop()) //一开始没有用到loop()参数所以不会运行死循环\n  //bar(loop,1) //没有返回值，因为是死循环\n\n```\n## 高阶函数：以一个函数输入参数或者输出或者两者都有\n![scala3](https://raw.githubusercontent.com/jChanJi/static_resource/master/scalastudy/%E9%AB%98%E9%98%B6%E5%87%BD%E6%95%B0.png)\n```markdown\n  def operter(f: (Int, Int) => Int) = {\n    f(4, 4)\n  }\n```\n![scala4](https://raw.githubusercontent.com/jChanJi/static_resource/master/scalastudy/%E5%8C%BF%E5%90%8D%E5%87%BD%E6%95%B0.png)\n## 匿名函数:一个函数常量，没有函数名字\n```markdown\n  def greeting() = (name: String) => {\n    \"name\" + \" \" + name\n  }\n```\n## 柯里化\n```markdown\n  def curriedAddd(a: Int)(b: Int) = a + b\n  curriedAddd(2)(2) //4\n  val addOne = curriedAddd(1)_ //第一个参数为１，第二个待定\n  addOne(3) //4\n```\n\n## 尾递归　\n![scala3](https://raw.githubusercontent.com/jChanJi/static_resource/master/scalastudy/%E5%B0%BE%E9%80%92%E5%BD%92%E5%87%BD%E6%95%B0.png)\n```markdown\n  @annotation.tailrec\n  def factorial(n: Int, m: Int): Int =\n    if (n <= 0) m\n    else factorial(n - 1, m * n)\n  factorial(5, 1)\n```\n## 综合例子，求函数在(a,b)之间的整数参数的和\n```markdown\n  def sum (f:Int => Int)(a:Int)(b:Int): Int= {\n    @annotation.tailrec\n    def loop(n: Int, acc: Int): Int = {\n      if (n > b) {\n        println(s\"n=${n},acc=${acc}\")\n        acc\n      } else {\n        println(s\"n=${n},acc=${acc}\")\n        loop(n + 1, acc + f(n))\n      }\n    }\n    loop(a,0)\n  }\n\n  sum(x=>x)(1)(5)//y = x\n  sum(x=> x * x)(1)(5)//y = x*x\n  sum(x=> x * x * x)(1)(5)//y = x*x*x\n  val sumsquare = sum(x => x * x)_\n  sumsquare(1)(5)\n```\n## connection\n### list\n```markdown\n  val a = List(1, 2, 3, 4)\n  val b = 0 :: a\n  val c = \"x\" :: \"y\" :: \"z\" :: Nil //从后往前两两依次计算\n  val d = a ::: c\n  //head\n  a.head\n  b.head\n  c.head\n  //tail尾列表\n  a.tail\n  b.tail\n  c.tail\n  //判断是否为空\n  a.isEmpty\n  Nil.isEmpty\n  //filter\n  //求奇数\n  a.filter(x => x % 2 == 1)\n  //字符串转化为数组\n  val f = \"hello1 world2, chanji3\".toList\n  //过滤出数字\n  f.filter(x => Character.isDigit(x))\n  //takewhile,当满条件时一直取\n  f.takeWhile(x => x != 'c')\n```\n## map\n```markdown\n  c.map(x => x.toUpperCase())\n  c.map(_.toUpperCase())\n  a.filter(_ % 2 == 1)\n  a.filter(_ % 2 == 1).map(_ + 10)\n  val q = List(a, List(4, 5, 6))\n  q.map(x => x.filter(_ % 2 == 0))\n  q.map(_.filter(_ % 2 == 0))\n  q.flatMap(_.filter(_ % 2 == 0)) //将list 合并\n  //reduceLeft和flodleft\n  //reduceLeft(op:(T,T) => T)\n  a.reduceLeft((x, y) => x + y)\n  a.reduceLeft(_ + _)\n  //foldLeft(Z:U)(op:(U,T) =>U)\n  a.foldLeft(0)(_ + _)\n  a.foldLeft(0)((x,y)=>x + y)\n  a.foldLeft(1)(_ * _)\n  ```\n## range\n```markdown\n  1 to 10\n  1 to 10 by 2 //步长为二\n  (1 to 10).toList\n  //until [x,y)\n  1 until 10\n  //stream:lazy list\n  1 #::2 #::3#::Stream.empty\n  val stream = (1 to 10000).toStream\n  stream.head\n  stream.tail\n```\n## toupe与map\n### toupe\n```markdown\n  (1,2)\n  1 -> 2\n  val t = (1,\"chanji\",\"alice\",\"Math\",85.6)\n  t._1\n  t._2\n  def sumSq(in: List[Int]):(Int,Int,Int) = {\n    in.foldLeft((0,0,0))((t,v)=>(t._1 + 1,t._2 + v, t._3 + v*v))\n  }\n  sumSq(a)\n```\n  ### map\n```markdown\n  val p = Map(1 -> \"david\",9->\"mike\")\n  p(1)\n  p(9)\n  p.contains(1)\n  p.contains(2)\n  p.keys\n  p.values\n  p + (2 -> \"bob\")\n  p -1\n  p ++ List(3->\"aa\",4->\"bb\")\n  p -- List(1,9,3,4)\n  p ++ List(3->\"aa\",4->\"bb\") -- List(1,9)\n```\n## 实现快速排序\n```markdown\n  def qSort(a:List[Int]):List[Int] = {\n    if(a.length < 2) a\n    else qSort(a.filter(_ < a.head)) ++\n      a.filter(_ == a.head) ++\n      a.filter(_  >a.head )\n  }\n\n  qSort(List(8,5,3,7,83,7))\n```\n","tags":["scala"],"categories":["-bigdata"]},{"title":"digitalocean服务器搭建ss服务器","url":"/year/01/12/vps/","content":"# 前言\n>经朋友介绍digitalocean服务器对于学生而言比较划算，最基础的服务器5$一个月，通过推荐链接注册可以领取10$的优惠或通过其他网上的推荐码获得15$,25$等不等的优惠券，如果使用github学生优惠券的还可以领50$的优惠。我找到的是15$优惠券。这里比较人性的是这里的优惠券是直接充值到账户上的，和腾讯，阿里的的只能使用一次的优惠券有点不同。需要注意的是优惠码只能用一次，如果使用过一次再使用github优惠码的时候会失败。网上的说法是直接联系客服，联系后的回信是。\n\n>Hello, and thank you for contacting DigitalOcean!\nI apologize for the inconvenience, however as you already have a promotional code on this account, we are unable to apply any additional codes at this time.<br>\n\n>很无奈\n>不甘心的我又发了一封工单，结果50$到账！也就是说5$变成了70$.\n\n![ss4](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ss4.PNG)\n<!--more-->\n\n# 前期准备\n## github学生认证(非必需)\n>1、登陆github,settings->emails->add email address,这里需要注意的是邮箱必须是你学校的邮箱，edu结尾的，每个学生都有。<br>\n\n>2、申请学生包：https://education.github.com/  填写信息，如果收到通过邮件则成功。然后在get your pack页面下领取优惠码，如下图:<br>\n\n![img1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/digitalocean1.PNG)\n## 点击邀请链接注册\n\n>点击https://m.do.co/c/efc4e6d6df86 邀请注册用户，可以得到10$优惠,注册邮箱随便填。<br>\n\n## 提交github邀请码使用不了的客服工单\n到support中提交申请工单，点击Go To Tricks，如下图所示：<br>\n\n![img2](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/digitalocean2.PNG),虽然有可能失败，但是尝试一下比较好，或者不通过邀请链接注册然后填写github邀请码。\n## 创建服务器\n>1、创建5$套餐的vps,选择ubuntu16.04版本的服务器，我这里服务器地点选择的是新加坡，感觉网网速还可以\n\n>2、下载[putty](http://rj.baidu.com/soft/detail/15699.html?ald),登陆服务器，登陆用户名为root,密码会发到你邮箱。登录后会要求修改密码。第一次输入原始密码，第二次输入修改后的密码，第三次重复密码。到此为止服务器已经搭建完成。\n\n# 搭建shadowsocks\n>1、教程各种版本的教程见 https://github.com/teddysun/shadowsocks_install 我们选择的是 shadowsocks-libev.sh的安装方式。加密方式为aes-256-cfb,这是没问题的。Your Server IP填写服务器外网地址，Your Server Port改一下,不要用默认的。\n\n>2、搭建完成后下载window的shadowsocks的客户端 https://github.com/shadowsocks/shadowsocks-windows/releases 我安装的是4.0.7版本,解压运行，根据刚才配置的服务器填写选项。在用户栏右击图标，启用系统代理，代理模式选择全局。\n3、到此为止就可以科学上网了。\n\n# 使用google的BBR加速\nBBR谷歌公司提出的一个开源TCP拥塞控制的算法。在最新的linux 4.9及以上的内核版本中已被采用。所以这里需要先更新内核。\n```markdown\nwget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9.9/linux-headers-4.9.9-040909_4.9.9-040909.201702090333_all.deb\n\nwget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9.9/linux-headers-4.9.9-040909-generic_4.9.9-040909.201702090333_amd64.deb\n\nwget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9.9/linux-image-4.9.9-040909-generic_4.9.9-040909.201702090333_amd64.deb\n\ndpkg -i [依次是上面三个文件]\ndpkg -l | grep linux-image #查看当前内核\napt remove linux-image-4.4.0-108-generic #删除旧版本内核\nupdate-grub #跟新grub文件\nls /boot/vmlinuz*  #查看是否有4.9内核\nreboot #重启\nuname -a #查看当前内核\nvim /etc/sysctl.conf #配置文件\n```\n内容为\n```markdown\nnet.core.default_qdisc=fq\nnet.ipv4.tcp_congestion_control=bbr\n```\n保存后执行\n```markdown\nsysctl -p\n```\n出现 sysctl net.ipv4.tcp_congestion_control则已经启动，至此，就可以在youtube上纵享1080p了\n\n# ubuntu配置ss客户端\n\n## 命令行启动\n```markdown\nsudo apt install shadowsocks\n```\n新建配置文件shadowsocks.json\n```markdown\n{\n  \"server\":\"xxx\",\n  \"server_port\":xxx,\n  \"local_port\":1080,\n  \"password\":\"xxx\",\n  \"timeout\":500,\n  \"method\":\"aes-256-cfb\"\n}\n```\n\n然后运行sslocal -c [配置文件路径]\n\n## 安装shadowsocks-qt5客户端启动\n```markdown\nsudo add-apt-repository ppa:hzwhuang/ss-qt5\nsudo apt update\nsudo apt install shadowsocks-qt5\n```\n然后在搜索栏搜shadow,看到图标，点击打开，和windows端一样\n## 配置系统代理\n但仅仅这样是连不上网的,还得配置系统代理\n>打开　**设置－网络－网络代理** 将http端口清空设置为０，然后第四个socks代理设置为　127.0.0.1，端口号写1080，这样就设置好了全局代理\n\n### 开机启动\n>命令行输入gnome-session-properties,点击add,选择路径　/usr/bin/qt-5,保存\n\n## 配置chrome实现自动切换\n\n>1.　在商店中下载SwitchyOmega\n\n>２．新建情景模式，选择代理服务器,这里命名为ss\n\n>３．代理协议为SOCKS5,代理服务器为127.0.0.1，代理端口为1080，点击应用选项保存\n\n![ss1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ss1.png)\n\n>４．点击自动切换,在规则列表规则一栏的情景模式为ss，默认情景模式一栏设置为直接连接\n\n>５．规则列表格式为AUtoProxy，规则列表网址为https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt，　然后更新情景模式，最后应用选项．\n\n![ss2](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ss2.png)\n\n>６．右击右上角的图标，选择自动切换，打开google，能够连接说明配置成功\n\n\n## linux下设置PAC自动切换\nwindow在全局模式下git等可以使用但是linux下设置全局代理后如果使用git,wget等工具时还要设置http代理,比较麻烦，所以最好的是设置PAC自动切换.\n##生成pac文件\n```markdown\nsudo pip install genpac #安装genpac用于生成pac文件\ncd /usr/local/lib/python2.7/dist-packages/shadowsocks　#进入默认的安装路径，或者直接将路径加入/bash/bin中\nsudo genpac --pac-proxy \"SOCKS5 127.0.0.1:1080\" --gfwlist-proxy=\"SOCKS5 127.0.0.1:1080\" --gfwlist-url=https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt --output=\"autoproxy.pac\"　# 生成pac文件\nsudo mv autoproxy.pac  ~／document　#将文件移动到document\ncd ~/document\nsudo chmod 777 autoproxy.pac　#赋予文件读写执行的权限\n```\n接下来设置网络代理，打开 **设置－网络－网络代理** ,方法自动，URL填写刚才的文件位置\n\n![ss3](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ss3.png)\n# 总结\n>在window下的配置比较简单，开启全局模式所有应用都能运行，但是在linux下开启全局模式会出现git,wget等http和https代理的问题，所以要安装genpac实现自动切换。\n","tags":["vps"],"categories":["-others"]},{"title":"spark之combineByKey","url":"/year/01/11/combineByKey/","content":"# 函数\n```markdown\ncombineByKey(creatCombiner,mergeValue,mergeCombiners,partitioner)\n```\n<!--more-->\n\n# 实例讲解\n## 生成数据\n```markdown\nval scores = sc.parallelize(Array((\"jake\",80.0),\n                                  (\"jake\",90.0),\n                                  (\"jake\",85.0),\n                                  (\"mike\",85.0),\n                                  (\"mike\",92.0),\n                                  (\"mike\",90.0)))\n```\n查看数据\n```markdown\nscores.foreach(println)\n(jake,90.0)\n(jake,80.0)\n(jake,85.0)\n(mike,92.0)\n(mike,85.0)\n(mike,90.0)\n```\n## 求三门总分\n```markdown\nval score2 = scores.combineByKey(score=>(1,score),\n                                (c1:(Int,Double),newScore)=>(c1._1+1,c1._2+newScore),\n                                (c1:(Int,Double),c2:(Int,Double))=>(c1._1+c2._1,c1._2+c2._2))\n```\n查看scores内容\n```markdown\nscore2.foreach(println)\n(mike,(3,267.0))\n(jake,(3,255.0))\n```\n注解：<br>\n1、Int表示科目出现的数目，Double表示第一门与第二门课累加之后的值，newScore表示的是遍历的时候出现的新的分数<br>\n2、当遍历到第二个时因为jake已经遍历过所以调用mergeValue这个参数，科目数变为 +1 分数也变为和newScore之和<br>\nmergeCombiner:c1和c2的科目数和分数相加（所有key相同的value相加）<br>\n3、partitioner一般用不到\n\n## 求平均分\n```markdown\nval average = score2.map{case(name,(num,score))=>(name,score/num)}\naverage.foreach(println)\n```\n运行结果\n```markdown\n(mike,89.0)\n(jake,85.0)\n```\n","tags":["big data"],"categories":["-bigdata"]},{"title":"scala 运行spark程序","url":"/year/01/11/idea_scala_spark/","content":"# 环境配置\n## 配置maven镜像地址\n```markdown\n<mirror>\n    <id>alimaven</id>\n    <name>aliyun maven</name>\n    <url>http://maven.aliyun.com/nexus/content/groups/public/</url>\n    <mirrorOf>central</mirrorOf>      \n</mirror>\n```\n<!--more-->\n\n## sbt仓库地址\n编辑~/.sbt/repositories（没有就新建）\n```markdown\n[repositories]\nlocal\ndl bintray: https://dl.bintray.com/typesafe/ivy-releases/, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly\njcenter: https://jcenter.bintray.com/\ntypesafe-ivy-releases: https://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly\nmaven-central\nsonatype-snapshots: https://oss.sonatype.org/content/repositories/snapshot\n```\n# 版本\n>jdk1.8\n\n>spark 2.2  \n\n>scala 2.11.11\n\n>sbt0.13.16\n\n# sbt依赖\nbuild.sbt文件内容为\n```markdown\n    name := \"scala_test\"\n    version := \"1.0\"\n    scalaVersion := \"2.11.11\" #后面不需要配置\n    libraryDependencies ++= Seq(\"org.apache.spark\" %% \"spark-core\" % \"2.2.0\")\n    libraryDependencies += \"org.apache.spark\" % \"spark-streaming_2.11\" % \"2.2.0\"  \n    libraryDependencies += \"org.apache.spark\" % \"spark-mllib_2.11\" % \"2.2.0\"  \n    libraryDependencies += \"org.apache.spark\" % \"spark-sql_2.11\" % \"2.2.0\"  \n    libraryDependencies += \"org.apache.spark\" % \"spark-streaming-kafka-0-8_2.11\" % \"2.2.0\"  \n    libraryDependencies += \"org.apache.spark\" % \"spark-streaming-flume_2.11\" % \"2.2.0\"  \n    libraryDependencies += \"org.apache.spark\" % \"spark-hive_2.11\" % \"2.2.0\" % \"provided\"  \n    libraryDependencies += \"org.scalanlp\" % \"breeze_2.11\" % \"0.11\"  \n    libraryDependencies += \"org.scalanlp\" % \"breeze-natives_2.11\" % \"0.11\"  \n    libraryDependencies += \"org.apache.hadoop\" % \"hadoop-common\" % \"2.6.0\"\n```\n然后使用sbt打包,如下图所示\n\n![图一](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/1.png)\n\n**注意本机实验的时候一定要先登录localhost的ssh不然到提交作业的时候会没有权限写入文件**\n\n# 运行服务\n```markdown\n./sbin/start-master.sh # 启动spark\n./bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost.localdomain:7077 # 启动worker, spark://localhost.localdomain:7077到8080端口查看   \n./bin/spark-submit --master spark://localhost.localdomain:7077  --class WordCount /home/chanji/scala_test.jar # 提交作业\n　\n```\n再到http://localhost:8080/jobs 查看作业\n# 问题\n## 无法解析主机名\n> 无法解析主机：promote.cache-dns.local＜br>\n\n>hostname分为三种类型：<br>\n静态的（static），瞬态的（transient），和灵活的（pret不然ty）<br>\n执行系统命令hostname得到的是瞬态的主机名，扫描文件中得到的是静态主机名。<br>\n将这两者统一起来就可以解决问题了。<br>\n1.自定义hostname，假设myname<br>\n2.执行hostnamectl set-hostname myname<br>\n这个命令能同时修改三种类型的主机名定义。<br>\n重启\n\n# 基础知识\n>Drive program 是程序的入口,包含这main函数\ncontext起着和集群连接的作用\nRDDs　弹性分布式数据集,实质就是一个数据集，指向一个变量，数据集可以被分为若干部分存在与不同的主机上，但操作这些数据只能通过定义的数据集变量。\n","tags":["big data"],"categories":["-bigdata"]},{"title":"ubuntu下Docker的安装和使用","url":"/year/01/10/docker/","content":"\n# 安装环境检查\n```markdown\nuname -a\nls -l /sys/class/misc/device-mapper\n```\n<!--more-->\n# 使用ubuntu apt-get安装\n```makdown\nsudo apt-get update\nsudo apt-get install docker.io\nsource /etc/bash_completion.d/docker.io\n```\n>由于使用ubuntu安装不是最新的版本所以使用docker提供的方法安装，先卸载\n# apt安装的卸载\n```makdown\nsudo apt-get remove docker\nsudo apt-get remove --auto-remove docker  \nsudo apt-get remove --purge docker.io  \nsudo apt-get autoremove --purge\n```\n# docker提供的方式安装\n## 安装crul\n```markdown\nsudo apt-get install -y curl\n```\n## 安装docker\n```markdown\ncurl -fsSL get.docker.com -o get-docker.sh\n```\n如果报错没有source.list文件则新建\n```markdown\n    sudo sh get-docker.sh\n```\ndocker默认只能由root权限运行，所以给当前用户权限\n```markdown\nsudo usermod -aG docker your-user\n```\n创建docker组并将当前用户加入组\n```markdown\nsudo groupadd docker\nsudo usermod-aG docker $USERE\n```\n修改镜像源地址为Daocloud\n\n```markdown\n curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://efd9e468.m.daocloud.io\n```\n#　docker安装版本的卸载\n```markdwown\n sudo apt-get purge docker-ce\n sudo rm -rf /var/lib/docker\n```\n# 常用的命令\n```markdwn\ndocker run nginx #运行容器\ndocker images  #查看镜像\ndocker run -p 8080:80 -d daocloud.io/nginx #在8080：80端口使用Daemon模式运行以daocloud.io/nginx为镜像的容器\ndocker ps\nsudo vim index.html\n```\n在index.html中写\n```html\n<html>\n<h1>Docker is fun</h1>\n</html>\n```\n将文件上传到docker的容器中\n```markdown\ndocker cp index.html 3a90426cbb80://usr/share/nginx/html  #其中‘3a90426cbb80’是容器号，后面是容器中文件地址\ndocker stop 3a90426cbb80　#停止容器\n```\n当我们再次运行容器的时候发现是一开始未改动的结果，因为docker在容器内作的改动都是暂时的，不会保存下来,如果需要保存则要生成一个新的容器\n```markdown\ndocker commit -m \"fun\" 4138300aa8eb nginx-fun # nginx-fun为新容器的名字\n```\n```markdown\ndocker ps #查看当前运行的容器\ndocker ps -aq #相当于列出所有的容器ID，然后docker rm它们\ndocker pull #获取image\ndocker build #创建image\ndocker images #列出images\ndocker run #运行container\ndocker ps #列出container\ndocker rm # 删除container\ndocker rmi # 删除image\ndocker cp # 在host和container之间拷贝文件\ndocker commit 保存改动为新的image\n```\n# 通过dockerfile创建容器\n## 简单的例子\n```markdown\nvim Dockerfile\n```\n在其中写入\n```markdown\nFROM alpine:latest #基础镜像，alpine是专门针对linux的非常小的镜像\nMAINTAINER chanji　#用户声明\nCMD echo \"hello Docker\" #执行命令\n```\nbuild镜像\n```markdown\nbuild -t hello_docker . # -t　标签参数; '.' 意思为文件路径下所有内容都送给docker engine\ndocker run hello_docker #生成镜像后运行\n```\n可以看到结果  \n```markdown\nhello Docker\n```\n## 稍微复杂的例子\n```markdown\nmkdir Dockerfile2\ncd Dockerfile2\nvim Dockerfile\n```\n在其中写入\n```markdown\nFROM ubuntu\nMAINTAINER chanji\nRUN apt-get update\nRUN　apt-get install -y nginx\nCOPY　index.html /var/www/html\nENTRYPOINT　[\"/usr/sbin/nginx\",\"-g\",\"daemon off;\"]  #将nginx在前台执行\nEXPOSE 80\n```\n再编辑index.html文件\n```markdown\nvim index.html\n```\n在其中写入\n```markdown\nhello docker\n```\n```markdown\ndocker build -t chanji/hello-nginx . #构建\ndocker run -d -p 80:80 chanji/hello-nginx #以deamon模式在80:80端口运行\ncurl http://localhost #显示index.html中的信息hello docker\n```\n## dockerfile简单语法\n\n关键字 | 含义\n------|-----\nFROM  |基础镜像\nRUN　　 |执行命令\nADD   |添加文件\nCOPY  |拷贝文件\nCMD　　　|执行命令\nEXPSOE|暴露端口\nWORKDIR|指定路径     \nMAINTAINER|　维护者\nENV   | 设定环境变量\nUSER　　|制定用户\nVOLUME|mount point挂载的卷\n\n# 镜像分层\n> dockerfile中的每一行语句都有id,分层存储，已有的镜像是只读的，运行生成容器的时候容器有读写权限\n\n# Volume\n>提供独立于程序之外的持久化存储，即将容器运行时的数据保存下来，这样就不用重新生成新的容器\n\n## 将本地文件位置挂在到docker容器内的位置,本地路径默认的\n```markdown\ndocker run -d --name nginx -v /usr/share/nginx/html nginx # -d :表示开启Daemon模式,--name: 名称, -v [容器内路径] [基础镜像]: 基础镜像中挂载卷的位置\ndocker aspect nginx # 查看容器信息中mount的host的地址\nsu # 进入root\ncd /var/lib/docker/volumes/6f01d9e35da52f4a8157f9e2099937cb9040a351b6a327fd3a69b53065836bcb/data # 可以看到当中的信息与nginx容器挂载路径下的内容一致\necho \"hello volum!\" > index.html # 将内容写入到本地挂载的文件\ndocker exec -it nginx /bin/bash #进入docker容器\ncd /usr/share/nginx/html # 打开目录\ncat index.html # 发现文件已经改变\n```\n## 将本地目录挂载在容器中的指定位置，本地路径自己定义\n```markdown\ndocker ps\ndocker stop 776eee319d2b #停止占用80端口的容器\nmkdir -p /vol/html\ncd /vol/html\nvim index.html # 在其中添加内\ncd .. # 到上一级目录\ndocker run -p 80:80 -d -v $PWD/html:/usr/share/nginx/html nginx # 将当前目录下的html文件夹挂载到nginx容器的/usr/share/nginx/html目录\nvim /html/index.html #修改其中内容为hello volume\ncurl localhost # 发现和刚才修改的内容一致\n```\n## 创建一个仅有数据的容器，并将此容器挂载到其他容器\n```markdown\nmkdir -p /vol2/data\ndocker create  -v $PWD/vol2/data:/var/mydata --name data_container ubuntu #创建一个容器，名为data_container，将本地目录挂载在其相应目录，基础镜像为ubuntu\ndocker run -it --volumes-form data_container ubuntu /bin/bash #运行一个新的容器，将数据容器挂载在新的容器上，并进入交互模式\nmount #查看挂载，发现有/var/mydata目录\ncd /var/mydata\ntouch hello.txt\nCrtl + d #退出容器\ncd data\nls # 出现了hello.txt文件\n```\n\n# Registry\n```markdown\ndocker search whalesay #搜索镜像\ndocker pull docker/whalesay # 将镜像pull下来,REPOSITORY可以理解为镜像的名字\ndocker run docker/whalesay cowsay Docker很好玩 #运行容器,调用cowsay，可以打印出鲸鱼和文字\ndocker tag docker/whalesay chanji/whalesay # 产生和docker/whalesay镜像相同的dockerID/whalesay镜像\ndocker login # dockerhub的用户名和密码，需要注册，很卡\ndocker push chanji/whalesay #上传到镜像库\n```\n\n# compose\n>多容器app\n\n## 安装docker-compose\n```markdown\nsu #进入root模式\ncurl -L https://github.com/docker/compose/releases/download/1.9.0/docker-compose-$(uname -s)-$(uname -m) > /usr/local/bin/docker-compose # 将二进制文件写入到bin中\nchmod a+x /usr/local/bin/docker-compose　#修改权限为可执行\ndocker-compose --version　#查看版本\n```\n## 创建多容器app\n```markdown\nmkdir ghost\ncd ghost\nmkdir ghost\nmkdir nginx\nmkdir data\ncd ghost\nvim Dockerfile\n```\n其内容为\n```markdown\nFROM ghost\nCOPY ./config.js /var/lib/ghost/config.js\nEXPOSE 2368\nCMD [\"npm\",\"start\",\"--production\"]\n```\n编辑config.js\n```marldown\nvim config.js\n```\n内容为\n```markdown\nvar path = require('path'),\nconfig;\n\nconfig = {\nproduction: {\n    url: 'http://mytestblog.com',   \n    mail:{},\n    database: {\n        client: 'mysql',\n        connection: {\n            host:'db',\n            user:'ghost',\n            password:'ghost',\n            database:'ghost,\n            port:'3306',\n            charset:'utf8',\n       },\n    debug: false\n    },\n    paths:{\n        contentPath: path.join(process.env.GHOST_CONTENT, '/')\n    },\n    server: {\n    host:'0.0.0.0',\n    port:'2368'\n    }\n   }\n};\nmodule.exports = config;\n```\n编辑nginx模块\n```markdown\ncd ../nginx\nvim Dockerfile\n```\n内容为\n```markdown\nFROM nginx\nCOPY nginx.conf /etc/nginx/nginx.conf\nEXPOSE 80      \n```\n配置nginx.conf\n```markdown\nvim nginx.config\n```\n内容为\n```markdown\nworker_processes 4;\nevents {worker_connections 1024;}\nhttp{\n    server{\n        listen 80;\n    location /{\n        proxy_pass http://ghost-app:2368;\n    }\n    }\n}\n```\n准备一个compose文件\n```markdown\ncd .. #进入第一层的ghost目录\nvim docker-compose.yml\n```\n其内容为\n```markdown\nversion: '2'\nnetworks:\n    ghost:\nservices:\n    ghost-app:\n        build: ghost\n        networks:\n            - ghost\n        depends_on:\n            - db\n        ports:\n            - \"2368:2368\"\n    nginx:\n        build: nginx\n        networks:\n            - ghost\n        depends_on:\n            - ghost-app\n        ports:\n            - \"80:80\"\n    db:\n        image: \"mysql:5.7.15\"\n        networks:\n            - ghost\n        environment:\n            MYSQL_ROOT_PASSWORD: mysqlroot\n            MYSQL_USER: ghost\n            MYSQL_PASSWORD: ghost\n        volumes:\n            - $PWD/data:/var/lib/mysql\n        ports:\n            - \"3360:3360\"    \n```\n上面的代码注意缩进和‘－'后的空格。文件下载[docker-compsoe.yml](https://raw.githubusercontent.com/jChanJi/static_resource/master/docker/docker-compose.yml)<br>\n如果运行结果显示80端口被占用，docker stop [id]停止占用端口的容器\n```markdown\ndocker-compose stop #先停掉拉起来的服务\ndocker-compose rm #删除停掉的服务\ndocker-compose build　＃第一次未生成镜像时会自动构建，但是出错后不会再自动构建，需要build\ndocker-compose up -d #再一次的拉起服务\n```\n上述命令必须在ghost的文件中执行<br>\n以上配置基本完成，打开浏览器\n```markdown\nlocalhost #打开浏览器输入lcoalhost，出现ghost页面则成功\nlocalhost/ghost #配置ghost路径\n```\n下面给出几张成功的页面\n![image1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ghost.png)\n\n![mage2](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ghost2.png)\n\n![image3](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ghost3.png)\n# 总结\n> docker作为目前极为流行的环境部署容器,使用起来确实是方便快捷效率高,以上使用了多app容器实现了mysql,ghost和数据容器的整合，只需要一些简单的命令便可以搭建起来一个服务。\n","tags":["docker"],"categories":["-others"]},{"title":"对四维的鸢尾花数据使用PCA进行降维","url":"/year/01/09/pca/","content":"\n## 前言：\n>此篇笔记主要根据南京大学礼欣老师的[《Python机器学习应用》](http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce)整理而成，详细内容请看礼欣老师的mooc课程。\n\n## 数据介绍：\n对四维的鸢尾花数据使用PCA进行降维并且可视化，数据格式如下：\n![图1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/iris.PNG)\n<!--more-->\n## 代码\n```python\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.datasets import load_iris\n\ndata = load_iris()\ny = data.target\nX = data.data\npca = PCA(n_components=2)\nreduced_X = pca.fit_transform(X)\n\nred_x, red_y = [], []\nblue_x, blue_y = [], []\ngreen_x, green_y = [], []\n\nfor i in range(len(reduced_X)):\n    if y[i] == 0:\n        red_x.append(reduced_X[i][0])\n        red_y.append(reduced_X[i][1])\n    elif y[i] == 1:\n        blue_x.append(reduced_X[i][0])\n        blue_y.append(reduced_X[i][1])\n    else:\n        green_x.append(reduced_X[i][0])\n        green_y.append(reduced_X[i][1])\n\nplt.scatter(red_x, red_y, c='r', marker='x')\nplt.scatter(blue_x, blue_y, c='b', marker='D')\nplt.scatter(green_x, green_y, c='g', marker='.')\nplt.show()\n\n```\n\n## 结果\n![图1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/iris_res.PNG)\n","tags":["PCA"],"categories":["-machinelearning"]},{"title":"NMF和PCA算法对人脸进行特征提取并且进行对比","url":"/year/01/09/NMF_PCA/","content":"\n## 前言：\n>此篇笔记主要根据南京大学礼欣老师的[《Python机器学习应用》](http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce)整理而成，详细内容请看礼欣老师的mooc课程。\n\n## 数据介绍：\n分别使用NMF和PCA算法对人脸进行特征提取并且进行对比\n<!--more-->\n\n## 代码\n```python\nfrom sklearn import decomposition\n\n\nn_row, n_col = 2, 3\nn_components = n_row * n_col\nimage_shape = (64, 64)\n\n\n###############################################################################\n# Load faces data\ndataset = fetch_olivetti_faces(shuffle=True, random_state=RandomState(0))\nfaces = dataset.data\n\n###############################################################################\ndef plot_gallery(title, images, n_col=n_col, n_row=n_row):\n    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n    plt.suptitle(title, size=16)\n\n    for i, comp in enumerate(images):\n        plt.subplot(n_row, n_col, i + 1)\n        vmax = max(comp.max(), -comp.min())\n\n        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n                   interpolation='nearest', vmin=-vmax, vmax=vmax)\n        plt.xticks(())\n        plt.yticks(())\n    plt.subplots_adjust(0.01, 0.05, 0.99, 0.94, 0.04, 0.)\n\n\nplot_gallery(\"First centered Olivetti faces\", faces[:n_components])\n###############################################################################\n\nestimators = [\n    ('Eigenfaces - PCA using randomized SVD',\n         decomposition.PCA(n_components=6,whiten=True)),\n\n    ('Non-negative components - NMF',\n         decomposition.NMF(n_components=6, init='nndsvda', tol=5e-3))\n]\n\n###############################################################################\n\nfor name, estimator in estimators:\n    print(\"Extracting the top %d %s...\" % (n_components, name))\n    print(faces.shape)\n    estimator.fit(faces)\n    components_ = estimator.components_\n    plot_gallery(name, components_[:n_components])\n\nplt.show()\n\n```\n\n## 结果\n```markdown\ndownloading Olivetti faces from http://cs.nyu.edu/~roweis/data/olivettifaces.mat to C:\\Users\\ChanJi\\scikit_learn_data\nExtracting the top 6 Eigenfaces - PCA using randomized SVD...\n(400, 4096)\nExtracting the top 6 Non-negative components - NMF...\n(400, 4096)\n```\n![图1](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/face_ori.PNG)\n![图2](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/face_PCA.PNG)\n![图3](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/face_NMF.PNG)\n","tags":["NMF"],"categories":["-machinelearning"]},{"title":"聚类算法DBSCAN实现大学生上网时长分类","url":"/year/01/09/studentOnline_timeslot/","content":"\n## 前言：\n>此篇笔记主要根据南京大学礼欣老师的[《Python机器学习应用》](http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce)整理而成，详细内容请看礼欣老师的mooc课程。\n\n<!--more-->\n\n## 数据介绍：\n现有大学校园网的日志数据，290条大学生的校园网使用情况数据，数据包\n括用户ID，设备的MAC地址，IP地址，开始上网时间，停止上网时间，上\n网时长，校园网套餐等。利用已有数据，分析学生上网的模式。数据下载[点击我](https://github.com/jChanJi/jchanji.github.com/tree/master/meterial/data/clustering)\n\n\n\n## 主要参数\n### eps: 两个样本被看作邻居节点的最大距离\n### min_samples: 簇的样本数\n### metric：距离计算方式\n\n## 上网时间段\n### 代码\n\n```python\nimport numpy as np\nimport sklearn.cluster as skc\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nmac2id=dict()\nonlinetimes=[]\nf=open('F:\\data\\clustering\\TestData.txt',encoding='utf-8')\nfor line in f:\n    mac=line.split(',')[2]\n    onlinetime=int(line.split(',')[6])\n    starttime=int(line.split(',')[4].split(' ')[1].split(':')[0])\n    if mac not in mac2id:\n        mac2id[mac]=len(onlinetimes)\n        onlinetimes.append((starttime,onlinetime))\n    else:\n        onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]\nreal_X=np.array(onlinetimes).reshape((-1,2))\n\nX=real_X[:,0:1]\n\ndb=skc.DBSCAN(eps=0.01,min_samples=20).fit(X)\nlabels = db.labels_\n\nprint('Labels:')\nprint(labels)\nraito=len(labels[labels[:] == -1]) / len(labels)\nprint('Noise raito:',format(raito, '.2%'))\n\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Silhouette Coefficient: %0.3f\"% metrics.silhouette_score(X, labels))\n\nfor i in range(n_clusters_):\n    print('Cluster ',i,':')\n    print(list(X[labels == i].flatten()))\n\n\nplt.hist(X,24)\nplt.show()\n```\n\n### 结果\n\n```markdown\nLabels:\n[ 0 -1  0  1 -1  1  0  1  2 -1  1  0  1  1  3 -1 -1  3 -1  1  1 -1  1  3  4\n -1  1  1  2  0  2  2 -1  0  1  0  0  0  1  3 -1  0  1  1  0  0  2 -1  1  3\n  1 -1  3 -1  3  0  1  1  2  3  3 -1 -1 -1  0  1  2  1 -1  3  1  1  2  3  0\n  1 -1  2  0  0  3  2  0  1 -1  1  3 -1  4  2 -1 -1  0 -1  3 -1  0  2  1 -1\n -1  2  1  1  2  0  2  1  1  3  3  0  1  2  0  1  0 -1  1  1  3 -1  2  1  3\n  1  1  1  2 -1  5 -1  1  3 -1  0  1  0  0  1 -1 -1 -1  2  2  0  1  1  3  0\n  0  0  1  4  4 -1 -1 -1 -1  4 -1  4  4 -1  4 -1  1  2  2  3  0  1  0 -1  1\n  0  0  1 -1 -1  0  2  1  0  2 -1  1  1 -1 -1  0  1  1 -1  3  1  1 -1  1  1\n  0  0 -1  0 -1  0  0  2 -1  1 -1  1  0 -1  2  1  3  1  1 -1  1  0  0 -1  0\n  0  3  2  0  0  5 -1  3  2 -1  5  4  4  4 -1  5  5 -1  4  0  4  4  4  5  4\n  4  5  5  0  5  4 -1  4  5  5  5  1  5  5  0  5  4  4 -1  4  4  5  4  0  5\n  4 -1  0  5  5  5 -1  4  5  5  5  5  4  4]\nNoise raito: 22.15%\nEstimated number of clusters: 6\nSilhouette Coefficient: 0.710\nCluster  0 :\n[22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]\nCluster  1 :\n[23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23]\nCluster  2 :\n[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\nCluster  3 :\n[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]\nCluster  4 :\n[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\nCluster  5 :\n[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n\n```\n\n![图一](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/stuonline1.PNG)\n## 上网时长\n### 代码\n```python\nimport numpy as np\nimport sklearn.cluster as skc\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\n\nmac2id=dict()\nonlinetimes=[]\nf=open('F:\\data\\clustering\\TestData.txt',encoding='utf-8')\nfor line in f:\n    mac=line.split(',')[2]\n    onlinetime=int(line.split(',')[6])\n    starttime=int(line.split(',')[4].split(' ')[1].split(':')[0])\n    if mac not in mac2id:\n        mac2id[mac]=len(onlinetimes)\n        onlinetimes.append((starttime,onlinetime))\n    else:\n        onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]\nreal_X=np.array(onlinetimes).reshape((-1,2))\n\nX=np.log(1+real_X[:,1:])\ndb = skc.DBSCAN(eps=0.14,min_samples=10).fit(X)\nlabels = db.labels_\n\nprint('Labels:')\nprint(labels)\nratio=len(labels[labels[:] == -1])/len(labels)\nprint('Noise raito:',format(ratio,'.2%'))\n\nn_clusters_ = len(set(labels))-(1 if -1 in labels else 0)\n\nprint('Estimated number of clusters: %d' % n_clusters_)\nprint(\"Silhouette Coefficient :%0.3f\"% metrics.silhouette_score(X, labels))\n\nfor i in range(n_clusters_):\n    print('Cluster',i,':')\n    count = len(X[labels ==i])\n    mean = np.mean(real_X[labels == i][:,1])\n    std=np.std(real_X[labels ==i][:,1])\n    print('\\t number of sample : ',count)\n    print('\\t mean of sample: ',format(mean,'.1f'))\n    print('\\t std of sample: ',format(std,'.1f'))\n\nplt.hist(X,24)\nplt.show()\n```\n### 结果\n```markdown\nLabels:\n[ 0  1  0  4  1  2  0  2  0  3 -1  0 -1 -1  0  3  1  0  3  2  2  1  2  0  1\n  1 -1 -1  0  0  0  0  1  0 -1  0  0  0  2  0  1  0 -1 -1  0  0  0  3  2  0\n -1  1  0  1  0  0 -1  2  0  0  0  1  3  3  0  2  0 -1  3  0  0  2  0  0  0\n  2  1 -1  0  0  0  0  0  0  1 -1  0  3  1  0  1  1  0  1  0  1  0  0 -1  1\n  1  0  0  2  0  0  0  2  2  0  0  0 -1  0  0  4  0  1  2 -1  0  1  0  2  0\n -1 -1 -1  0  1  1  3 -1  0  1  0  2  0  0  2  1  1  0  0  0  0  4 -1  0  0\n  0  0  2  0  0  0  0 -1  2  0  0  0  0  4  0  0 -1  0  2  0  0 -1  0  1  4\n  0  0 -1  1  1  0  0  2  0  0  3 -1 -1 -1  1  0  0  2  1  0 -1 -1  3  2  2\n  0  0  3  0  1  0  0  0  3  2  0 -1  0  1 -1 -1  0  2  2  1  4  0  0  1  0\n  2  0  0  0  0  1  1  0  0  1  0  4 -1 -1  0  0  0 -1 -1  1 -1  4 -1  0  2\n  2 -1  2  1  2 -1  0 -1  0  2  2  1 -1  0  1  2 -1 -1  1 -1  2 -1 -1  1  4\n  2  3  1  0  4  0  0  4  2  4  0  0  2 -1]\nNoise raito: 16.96%\nEstimated number of clusters: 5\nSilhouette Coefficient :0.227\nCluster 0 :\n     number of sample :  128\n     mean of sample:  5864.3\n     std of sample:  3498.1\nCluster 1 :\n     number of sample :  46\n     mean of sample:  36835.1\n     std of sample:  11314.1\nCluster 2 :\n     number of sample :  40\n     mean of sample:  843.2\n     std of sample:  242.9\nCluster 3 :\n     number of sample :  14\n     mean of sample:  16581.6\n     std of sample:  1186.7\nCluster 4 :\n     number of sample :  12\n     mean of sample:  338.4\n     std of sample:  31.9\n```\n\n![图二](https://raw.githubusercontent.com/jChanJi/static_resource/master/img/stuonline2.PNG)\n","tags":["DBSCAN"],"categories":["-machinelearning"]},{"title":"Single Shot MultiBox Detector翻译","url":"/year/12/29/SSD-tensorflow/","content":"## 前言\n> 毕设的课题为基于SSD的深度学习目标检测研究，首先要对SSD框架的思想做到非常的了解，借此机会准备详细的翻译理解一下SSD的内容，如有错误指出，欢迎指出。原文链接：[https://arxiv.org/abs/1512.02325](点我)\n\n## 翻译\n### Abstract\n> 我们展示了使用单层神经网络来检测图片中的物体的方法，称之为SSD,它将输出空间的bounding boxes (边界框)离散化为每一个特征图上的一系列不同纵横比和尺寸的（default boxes）默认框。在进行预测的时候，神经网络会对每个默认框中的每一个物体种类的存在形成一个分数，并且对默认框进行调整以更好的匹配目标的形状 。另外，这个网络还用了不同的方法来联合不同特征图上的预测结果以自然的解决不同尺寸的物体。SSD是一个相对简单的方法，它需要候选对象，因为它完全排除了生成候选对象和接下来对像素和特征重新采样的阶段并且将所有的计算都压缩在了一个单层网络里。这让SSD很容易的训练并且直接的应用到需要检测组件的系统中去。在PASACAL、VOC、COCO和ILASVRC数据集上的实验证明了SSD在精确度上和那些利用额外的候选目标步骤的方法相比很有竞争力并且更快，于此同时SSD还为训练和接口提供了一个统一的框架。，对PASCAL VOC 2007的数据集，在300×300像素的尺寸输入，SSD在Nvidia Titan X上59FPS时达到72.1％的mAP，500×500像素尺寸输入SSD达到75.1％的mAP，超过了相比较的state of art 模型Faster R-CNN 。和其他的单个步骤的方法相比，SSD对于更小尺寸图片的输入有着更好的检出率。\n<!--more-->\n### The Single Shot Detector (SSD)\n\n> 在模型的训练期间中只需要输入原图和ground truth boxes（真实边框图）（如：图四 (a)）。卷积处理中，在许多不同尺寸的特征图（例如8x8 feature map和4x4 feature map）中评估出一系列不同宽比的边界框值，其中默认的为四个，对于每一个默认框评估出所有目标的形状偏移和置信度。在训练时，我们首先将这些默认框匹配到真实标签框。这些框为正，其余视为负。模型损失是定位损失和置信损失之间的加权和。\n\n![feature map](https://raw.githubusercontent.com/jChanJi/static_resource/master/SSD/feature_map.PNG)\n\n### model\n> SSD基于前馈卷积网络，其产生固定大小的边界框集合和框中对象类别的分数，接着是非最大化抑制步骤以产生最终检测。早期的网络基于使用高质量图片分类的标准结构，我们称之为基础网络。后来在基础网络的上进行了改进加上了辅助结构(图四 SSD的Extra Feature Layers部分)。其中关键的特征如下：\n\n> ⑴在检测的时候使用不同尺寸的特征图<br>\n在被删去的基础网络后面加上了卷积特征层，这些特征层减少了逐渐增多的\n尺寸并且能够对不同尺寸的对象进行预测。这个预测检测目标的卷积模型每一个特征层都和YOLO不相同。\n\n> ⑵检测的卷积预测器<br>\n每一个添加了特征层或者从基础网络中选择了一个特征层的网络能够使用一个卷积过滤器的集合产生一个预测目标的确定集合。在图五SSD模型的顶部表明了这一特点。对一个m x n并且有p个通道的特征层来说，用来预测潜在目标的参数的基础元素有3 x 3 x p个小核心，它用来产生分数或者种类或者相对于默认框的相对形状偏移量。在每个核心应用到的m x n区域内它产生一个输出值。\n边界框偏移量的输出值是相对与默认框的位置和特征图的位置测量的，而YOLO在这一步中使用中间全连接层而不是卷积过滤器。\n与YOLO相比，他有如下的特征：首先它在基础网络的后面加上了一些特征层，从而能够对默认框的不同尺寸和方面的比例和他们置信联系的偏移进行预测。在VOC2007测试数据集上，SSD对于300x300尺寸的输入图片的训练结果远远的超过了YOLO对于424x425尺寸图片的训结果，并且在速度上也有很大的改进。\n\n> ⑶默认框和纵横比<br>\n针对网络顶部的多特征图，我们将默认的边界框集合和每一个特征图单元联系起来。默认框使用卷积的方式拼接特征图所以每一个默认框的位置相对于他对应的单元是固定的。在每一个特征图单元中我们能够预测每个单元中相对于默认框的形状偏移量和在每一个边框中表明一个类实体存在的分数。特别的，对于给定k个边框的每个边框，我们计算c类的分数和4个相对于原始默认框形状的偏移。在特征图的每个特征单元上需要有（c+4）x k个过滤器，对于m x n的特征图就会产生（c + 4） x k x m x n个输出。这里的默认框很类似于Faster R-CNN中的锚框，但是这里将他应用到不同解决方案的一些特征图中。在一些特征图中允许不同形状的默认框能偶有效的离散化可能的输出框形状的空间。\n\n![model](https://raw.githubusercontent.com/jChanJi/static_resource/master/SSD/model.PNG)\n","tags":["object detection"],"categories":["-articals"]},{"title":"知识图谱","url":"/year/11/19/knowledgegraph/","content":"\n# 知识图谱\n## 起源\n> &nbsp; &nbsp; &nbsp; &nbsp;知识图谱于2012年5月17日被Google正式提出， 其初衷是为了提高搜索引擎的能力，增强用户的搜索质量以及搜索体验。，RDF (resource description framework)<sup>[1]</sup>模式(RDF schema) （应用）和万维网本体语言(Web ontology language，OWL) 的形式化模型就是基于上述目的产生的。\n\n\n<!--more-->\n\n> [1]RDF: RDF是一个处理元数据的XML,RDF使用XML语法和RDF Schema（RDFS）来将元数据描述成为数据模型。是描述语义层面的本体关系的语言。\n\n>[2]数据模型: 数据模型（Data Model）是数据特征的抽象。数据（Data）是描述事物的符号记录，模型（Model)是现实世界的抽象。数据模型所描述的内容有三部分：数据结构、数据操作和数据约束。\n\n## 定义\n> &nbsp; &nbsp; &nbsp; &nbsp;知识图谱是Google用于增强其搜索引擎功能的知识库。本质上,知识图谱是一种揭示实体之间关系的 **语义网络（semantic network）** ,即具 有有向图结构的一个知识库，其中图的结点代 表实体（entity）或者概念（concept），而图的 边代表实体/ 概念之间的各种语义关系，可以对现实世界的事物及其相互关系进行形式化地描述。现在的知识图谱已被用来泛指各种大规模的知识库。\n\n![知识图谱](https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/knowledgegraph2.png)\n\n## 应用领域\n> &nbsp; &nbsp; &nbsp; &nbsp;广泛应用于智能搜索、 智能问答、个性化推荐等领域。尤其是在智能搜索中，用户的搜索请求不再局限于简单的关键词匹配，搜索将根据用户查询的情境与意图进行推理，实现 概念检索。与此同时，用户的搜索结果将具有层次 化、结构化等重要特征。\n\n## 分类\n> &nbsp; &nbsp; &nbsp; &nbsp;**知识图谱也可分为通用知识图谱(开放链接知识库)和行业知识图谱(垂直行业知识库)**。\n\n### 通用知识图谱\n> &nbsp; &nbsp; &nbsp; &nbsp;通用知识图谱注重广度，强调融合更多的实体，较行业知识图谱而言,其准确度不够高,并且受概念范围的影响,很难借助本体库对公理、规则以及约束条件的支持能力规范其实 体、属性、实体间的关系等。通用知识图谱主要应 用于智能搜索等领域。行业知识图谱通常需要依靠 特定行业的数据来构建，具有特定的行业意义。\n\n### 行业知识图谱\n> &nbsp; &nbsp; &nbsp; &nbsp;行业知识图谱中，实体的属性与数据模式往往比较丰富，需要考虑到不同的业务场景与使用人员。\n\n## 架构\n### 逻辑架构\n>  &nbsp; &nbsp; &nbsp; &nbsp;知识图谱在逻辑上可分为 **模式层** 与 **数据层** 两个层次.\n\n#### 1、数据层\n> &nbsp; &nbsp; &nbsp; &nbsp;数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用(实体1，关系， 实体2)、(实体、属性，属性值)这样的三元组来表达事实,可选择 **图数据库** 作为存储介质。\n\n#### 2、模式层\n> &nbsp; &nbsp; &nbsp; &nbsp;模式层构建在数据层之上，主要是通过 **本体库** 来规范数据层的一系列事实表达。 **本体是结构化知识库的概念模板** ，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。\n\n### 体系结构（构建模式）\n> &nbsp; &nbsp; &nbsp; &nbsp;知识图谱的体系架构知识图谱的体系架构是其指构建模式结构，如图1所示。其中虚线框内的部分为知识图谱的构建过 程，该过程需要随人的认知能力不断更新迭代。知识图谱主要有 **自顶向下(top-down)** 与 **自底向上(bottom-up)** 两种构建方式。\n\n#### 自顶向上\n> 自顶向下指的是: **先为知识图谱定义好本体与数据模式，再将实体加入到知识库**。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如Freebase项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。\n\n#### 自底向上\n> &nbsp; &nbsp; &nbsp; &nbsp;自底向上指的是 **从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式** 。目前，大多数知识图谱都采用自底向上的方式进行构建，其中典型就是 Google的Knowledge Vault。\n\n![图1](https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/1.PNG)\n\n\n## 关键技术\n###  知识抽取\n#### 知识抽取\n##### 基于规则与词典的实体抽取方法\n> &nbsp; &nbsp; &nbsp; &nbsp;早期的实体抽取是在限定文本领域、限定语义 单元类型的条件下进行的，主要采用的是基于规则与词典的方法， **例如使用已定义的规则，抽取出文本中的人名、地名、组织机构名、特定时间等实体。**\n\n##### 基于统计机器学习的实体抽取方法\n> &nbsp; &nbsp; &nbsp; &nbsp;随后，研究者尝试将机器学习中的 **监督学习** 算法用于命名实体的抽取问题上。单纯的监督学习算法在性能上 不仅受到训练集合的限制，并且算法的准确率与召回率都不够理想。相关研究者认识到监督学习算法的制约性后，尝试将监督学习算法与规则相互结合。\n\n##### 面向开放域的实体抽取方法\n> &nbsp; &nbsp; &nbsp; &nbsp;其基本思想是通过 **少量的实体实例建立特征模型，再通过该模型应用于新的数据集得到新的命名实体**。基于 **无监督学习** 的开放域聚类算法，其基本思想是基于已知实体的语义特征去搜索日志中识别出命名的实体，然后进行聚类。\n\n#### 关系抽取\n> &nbsp; &nbsp; &nbsp; &nbsp;关系抽取的目标是解决实体间语义链接的问题。主要有效的方法是基于马尔可夫逻辑网和基于本体推理的深层隐含关系抽取方法，主要有一下俩个分类。\n\n##### 开放式实体关系抽取\n> &nbsp; &nbsp; &nbsp; &nbsp;开放式实体关系抽取可分为二元开放式关系抽 取和n元开放式关系抽取。\n\n##### 基于联合推理的实体关系抽取\n> &nbsp; &nbsp; &nbsp; &nbsp;联合推理的关系抽取中的典型方法是马尔可夫逻辑网MLN(Markov logic network)<sup>[1]</sup>。\n\n> [1] 马尔可夫逻辑网:\n\n###  属性抽取\n> &nbsp; &nbsp; &nbsp; &nbsp;属性抽取主要是针对实体而言的，通过属性可形成对实体的完整勾画。由于实体的属性可以看成是 **实体与属性值之间的一种名称性关系** ，因此可以将实体属性的抽取问题转换为关系抽取问题。\n\n> &nbsp; &nbsp; &nbsp; &nbsp;大量的属性数据主要存在于半结构化、非结构化的大规模开放域数据集中。抽取这些属性的方法，一种是将上述从百科网站上抽取的 **结构化数据作为可用于属性抽取的训练集，然后再将该模型应用于开放域中的实体属性抽取** 。另一种是 **根据实体属性与属性值之间的关系模式，直接从开放域数据集上抽取属性。**\n\n# 本体\n## 定义\n>  &nbsp; &nbsp; &nbsp; &nbsp;Gruber给出了Ontology的一个最为流行的定义,即“Ontology是概念模型<sup>[1]</sup> 的明确的规范说明”。\n\n> [1] 概念模型: “概念模型” 指通过抽象出客观世界中一些现象的相关概念而得到的模型。首先把现实世界中的客观对象抽象为某一种信息结构，这种信息结构并不依赖于具体的计算机系统，不是某一个数据库管理系统（DBMS）支持的 **数据模型** ，而是概念级的模型，称为概念模型。<br>\n\n> [2] 数据模型: 数据模型（Data Model）是数据特征的抽象。数据是描述事物的符号记录，模型是现实世界的抽象。数据模型为数据库系统的信息表示与操作提供了一个抽象的框架。数据模型所描述的内容有三部分：数据结构、数据操作和数据约束。\n\n## 举例解释\n> &nbsp; &nbsp; &nbsp; &nbsp;上面的概念很抽象，不是很好理解，其实本体的概念有两层意思，一层是哲学层面的意思，一层是引申到信息科学中的语义层面的意思。\n\n### 哲学上的本体\n> “鼠标”，“mouse”,\n\n![符号](https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/entity1.PNG)\n\n> &nbsp; &nbsp; &nbsp; &nbsp;等都是表示\"鼠标\"这个本体的的\"符号\"。由此可见“本体”是只可意会不可言传的，因为所有的描述都成为了“本体”的外在符号，我们世界上的所有图像、语言、我们看到的、听到的、感受到的，都成为符号到本体的某种映射。\n\n### 信息科学中的本体\n> &nbsp; &nbsp; &nbsp; &nbsp;Ontology是一种 **描述术语** （包含哪些词汇）及 **术语间关系** （描述苹果、香蕉、水果之间的关系）的概念模型。Ontology的形式可简单可复杂。最简单的词汇表（只定义术语集合，不定义术语之间的关系）也可以看成是一种“本体”；但严格意义上的本体，是既定义了术语、也定义了术语之间关系的。生活中，最常见、最成熟的本体，就属图书馆里的图书分类法。本体，以图书分类法为例，一方面限定了术语集合（即规定大家必须采用共同承认的一套词汇，禁止私自发明新词），另一方面定义术语之间的上下位关系（如：计算机技术隶属于工业技术，软件技术隶属于计算机技术，等等）。只要大家都认同该本体，并在实践中长期遵守该本体，依照它来编排和索引书目，那么日后寻找一本书就会非常方便。\n\n## 本体与知识图谱(语义网)的区别\n> 1、Ontology 是对共享概念模型的规范说明 ,这里所说的“共享概念模型” 指该模型中的 **概念是公认** 的 ,至少在某个特定的领域是公认的。一般情况下 ,Ontology 是 **面向特定领域**  , 用于描述特定领域的概念模型。<br>\n\n> 2、语义网络从数学上说 ,是一种带有标记的有向图。它最初用 于表示命题信息 ,现广泛应用于专家系统表示知识。语义网络中节点表示物理实体、概念或状态 ,连接节点的边用于表示关系。语义网络中对节点和边没有其他特殊的规定 ,因此 **语义网络描述的对象或范围比Ontology广。**\n\n\n引用<br>\n[1] 徐增林, 盛泳潘, 贺丽荣,等. 知识图谱技术综述[J]. 电子科技大学学报, 2016, 45(4):589-606.<br>\n[2] 漆桂林, 高桓, 吴天星. 知识图谱研究进展[J]. 情报工程, 2017, 3(1):4-25.<br>\n[3] 邓志鸿, 唐世渭, 张铭,等. Ontology研究综述[J]. 北京大学学报(自然科学版), 2002, 38(5):730-738.<br>\n[4]李国洪, 梁保城, 赵毅,等. Ontology研究的知识图谱演化[J]. 情报杂志, 2013(3):101-105.<br>\n[5] Gruber T R. A Translation Approach to Portable Ontology Specifications. Knowledge Acquisition ,1993 ,5 :199～220\n","categories":["-articals"]},{"title":"Arabesque:A System for Distributed Graph Mining","url":"/year/10/30/artical_1/","content":"\n## Arabesque: A System for Distributed Graph Mining\n> ### Arabesque:分布式的图挖掘系统\n\n\n#### 原文链接：[点我跳转](https://github.com/jChanJi/jchanji.github.com/blob/master/meterial/093-teixeira.pdf)\n\n## 目录\n\n* [Abstract](#Abstract)\n\n* 1.[Introduction](#introduction)\n\n<!--more-->\n\n* 2.Graph Mining Problems\n\n* 3.The Filter-Process Model\n\n    * 3.1.Computational Model\n\n    * 3.2.Alternative Paradigms: Think Like a Vertex and Think Like a Pattern\n\n* 4.Arabesque: API, Programming, and Implementation\n    * 4.1 Arabesque API\n\n    * 4.2 Programming with Arabesque\n\n    * 4.3 Arabesque implementation\n\n* 5.Graph Exploration Techniques\n    * 5.1 Coordination-Free Exploration Strategy\n\n    * 5.2 Storing Embeddings Compactly\n\n    * 5.3 Partitioning Embeddings for Load Balancing\n\n    * 5.4 Two-Level Pattern Aggregation for Fast Pattern Canonicality Checking\n* 6.Evaluation\n    * 6.1 Experimental Setup\n\n    * 6.2 Alternative Paradigms: TLV and TLP\n\n    * 6.3 Arabesque: The TLE Paradigm\n\n    * 6.4 Large Graphs with Arabesque\n\n* 7.Related Work\n\n* 8.Conclusions\n\n* 9.Acknowledgments\n\n* 10.References\n\n### <span id=\"Abstract\">Abstract</span>\n\n\n\n#### 原文\n\n#### Abstract\n  Distributed data processing platforms such as MapReduce\nand Pregel have substantially simplified the design and deployment\nof certain classes of distributed graph analytics algorithms.\nHowever, these platforms do not represent a good\nmatch for distributed graph mining problems, as for example\nfinding frequent subgraphs in a graph. Given an input\ngraph, these problems require exploring a very large number\nof subgraphs and finding patterns that match some “interestingness”\ncriteria desired by the user. These algorithms are\nvery important for areas such as social networks, semantic\nweb, and bioinformatics.\n\n  In this paper, we present Arabesque, the first distributed\ndata processing platform for implementing graph mining\nalgorithms. Arabesque automates the process of exploring\na very large number of subgraphs. It defines a high-level\nfilter-process computational model that simplifies the development\nof scalable graph mining algorithms: Arabesque explores\nsubgraphs and passes them to the application, which\nmust simply compute outputs and decide whether the subgraph\nshould be further extended. We use Arabesque’s API\nto produce distributed solutions to three fundamental graph\nmining problems: frequent subgraph mining, counting motifs,\nand finding cliques. Our implementations require a\nhandful of lines of code, scale to trillions of subgraphs, and\nrepresent in some cases the first available distributed solutions.\n\n#### 翻译\n  Distributed data processing platforms such as MapReduce\nand Pregel have substantially simpliﬁed the design and deployment of certain classes of distributed graph analyticsal gorithms.\n\n>分布式数据处理平台例如mapreduce和pregel实质上是简化了某些类的分布式图形化分析算法的设计和调度\n\n However, these platforms do not represent a good\nmatch for distributed graph mining problems, as for example\nfinding frequent subgraphs in a graph.\n\n>但是，这些平台没有表现出对分布式图形挖掘问题的匹配。就以在图表中频繁的寻找子图作为例子\n\nGiven an input graph, these problems require exploring a very large number of subgraphs and finding patterns that match some “interestingness” criteria desired by the user.\n\n>给出一个输入图表，这些问题需要扫描（探索）一个数量很多的子图并且寻找和用户期望的一些\"兴趣性\"准则相匹配的模式（图案,样品）。\n\nThese algorithms are very important for areas such as social networks, semantic web, and bioinformatics.\n\n\n>这些算法对例如社交网络，语义网，和分析复杂生物的学科的领域非常重要。\n\n\nIn this paper, we present Arabesque, the first distributed\ndata processing platform for implementing graph mining\nalgorithms.\nArabesque automates the process of exploring a very large number of subgraphs.\n\n>在这篇文献当中，我们介绍Arabesque,第一个实现图挖掘算法的分布式数据处理平台。Arabesque自动化了探索一个很大数量的子图的流程。\n\nIt defines a high-level filter-process computational model that simplifies the development of scalable graph mining algorithms: Arabesque explores subgraphs and passes them to the application, which must simply compute outputs and decide whether the subgraph should be further extended.\n\n>他定义了一个高级的过滤过程的计算模型，它简化了可升级的图挖掘算法的开发:Arabesque 探索子图并且将他们传递给应用程序，这个应用程序必须简单的计算输出和决定是否子图应该被进一步的被扩展。\n\nWe use Arabesque’s API to produce distributed solutions to three fundamental graph mining problems: frequent subgraph mining, counting motifs,and finding cliques.\n\n>我们用Arabesque的API去产生三个基础的图挖掘问题的分布式解决方案:频繁的子图挖掘，计数的图案，寻找派系。\n\nOur implementations require a handful of lines of code, scale to trillions of subgraphs, and represent in some cases the first available distributed solutions.\n\n>我们的实现需要很少行的代码，规模数万亿的子图，和在某些情况下第一个可获得的分布式解决方案的示范\n\n\n#### 段落翻译\n#### 摘要\n分布式数据处理平台例如mapreduce和pregel实质上是简化了某些类的分布式图形化分析算法的设计和调度.但是，这些平台没有表现出对分布式图形挖掘问题的匹配。就以在图表中频繁的寻找子图作为例子.给出一个输入图表，这些问题需要扫描（探索）一个数量很多的子图并且寻找和用户期望的一些\"兴趣性\"准则相匹配的模式（图案,样品）。这些算法对例如社交网络，语义网，和分析复杂生物的学科的领域非常重要。\n\n在这篇文献当中，我们介绍Arabesque,第一个实现图挖掘算法的分布式数据处理平台。Arabesque自动化了探索一个很大数量的子图的流程。他定义了一个高级的过滤过程的计算模型，它简化了可升级的图挖掘算法的开发:Arabesque 探索子图并且将他们传递给应用程序，这个应用程序必须简单的计算输出和决定是否子图应该被进一步的被扩展.我们用Arabesque的API去产生三个基础的图挖掘问题的分布式解决方案:频繁的子图挖掘，计数的图案，寻找派系。我们的实现需要很少行的代码，规模数万亿的子图，和在某些情况下第一个可获得的分布式解决方案的示范\n\n\n\n### <span id=\"Introduction\">Introduction</span>\n\n### 原文\n\nGraph data is ubiquitous in many fields, from the Web to advertising\nand biology, and the analysis of graphs is becoming\nincreasingly important. The development of algorithms\nfor graph analytics has spawned a large amount of research,\nespecially in recent years. However, graph analytics has traditionally\nbeen a challenging problem tackled by expert researchers,\nwho can either design new specialized algorithms\nfor the problem at hand, or pick an appropriate and sound\nsolution from a very vast literature. When the input graph or\nthe intermediate state or computation complexity becomes\nvery large, scalability is an additional challenge.\n\nThe development of graph processing systems such as\nPregel [25] has changed this scenario and made it simpler to\ndesign scalable graph analytics algorithms. Pregel offers a\nsimple “think like a vertex” (TLV) programming paradigm,\nwhere each vertex of the input graph is a processing element\nholding local state and communicating with its neighbors\nin the graph. TLV is a perfect match for problems that\ncan be represented through linear algebra, where the graph\nis modeled as an adjacency matrix (or some other variant\nlike the Laplacian matrix) and the current state of each vertex\nis represented as a vector. We call this class of methods\ngraph computation problems. A good example is computing\nPageRank [6], which is based on iterative sparse matrix\nand vector multiplication operations. TLV covers several\nother algorithms that require a similar computational architecture,\nfor example, shortest path algorithms, and over the\nyears many optimizations of this paradigm have been proposed\n[17, 26, 36, 42].\n\nDespite this progress, there remains an important class\nof algorithms that cannot be readily formulated using the\nTLV paradigm. These are graph mining algorithms used\nto discover relevant patterns that comprise both structurebased\nand label-based properties of the graph. Graph mining\nis widely used for several applications, for example, discovering\n3D motifs in protein structures or chemical compounds,\nextracting network motifs or significant subgraphs\nfrom protein-protein or gene interaction networks, mining\nattributed patterns over semantic data (e.g., in Resource\nDescription Framework or RDF format), finding structurecontent\nrelationships in social media data, dense subgraph mining for community and link spam detection in web data,among others. Graph mining algorithms typically take a labeled and immutable graph as input, and mine patterns\nthat have some algorithm-specific property (e.g., frequency\nabove some threshold) by finding all instances of these patterns\nin the input graph. Some algorithms also compute aggregated\nmetrics based on these subgraphs\n\n\n![图1](./img/1.PNG)\n\nFigure 1: Exponential growth of the intermediate state in\ngraph mining problems (motifs counting, clique finding,\nFSM: Frequent subgraph mining) on different datasets.\n\n\nDesigning graph mining algorithms is a challenging and\nactive area of research. In particular, scaling graph mining\nalgorithms to even moderately large graphs is hard. The set\nof possible patterns and their subgraphs in a graph can be\nexponential in the size of the original graph, resulting in an\nexplosion of the computation and intermediate state. Figure\n1 shows the exponential growth of the number of “interesting”\nsubgraphs of different sizes in some of the graph\nmining problems and datasets we will evaluate in this paper.\nEven graphs with few thousands of edges can quickly generate\nhundreds of millions of interesting subgraphs. The need\nfor enumerating a large number of subgraphs characterizes\ngraph mining problems and distinguishes them from graph\ncomputation problems. Despite this state explosion problem,\nmost graph mining algorithms are centralized because of the\ncomplexity of distributed solutions.\n\n\nIn this paper, we propose automatic subgraph exploration\nas a generic building block for solving graph mining\nproblems, and introduce Arabesque, the first embedding exploration\nsystem specifically designed for distributed graph\nmining. Conceptually, we move from TLV to “think like an\nembedding” (TLE), where by embedding we denote a subgraph\nrepresenting a particular instance of a more general\ntemplate subgraph called a pattern (see Figure 2).\n\n\nArabesque defines a high-level filter-process computational\nmodel. Given an input graph, the system takes care\nof automatically and systematically visiting all the embeddings\nthat need to be explored by the user-defined algorithm,\nperforming this exploration in a distributed manner. The system\npasses all the embeddings it explores to the application,\nwhich consists primarily of two functions: filter, which indicates whether an embedding should be processed, and process,\nwhich examines an embedding and may produce some\noutput. For example, in the case of finding cliques the filter\nfunction prunes embeddings that are not cliques, since none\nof their extensions can be cliques, and the process function\noutputs all explored embeddings, which are cliques by construction.\nArabesque also supports the pruning of the exploration\nspace based on user-defined metrics aggregated across\nmultiple embeddings.\n\n![图2](./img/2.PNG)\n\n\nFigure 2: Graph mining concepts: an input graph, an example\npattern, and the embeddings of the pattern. Colors represent\nlabels. Numbers denote vertex ids. Patterns and embeddings\nare two types of subgraphs. However, a pattern is\na template, whereas an embedding is an instance. In this example,\nthe two embeddings are automorphic.\n\nThe Arabesque API simplifies and thus democratizes the\ndesign of graph mining algorithms, and automates their execution\nin a distributed setting. We used Arabesque to implement\nand evaluate scalable solutions to three fundamental\nand diverse graph mining problems: frequent subgraph mining,\ncounting motifs, and finding cliques. These problems\nare defined precisely in Section 2. Some of these algorithms\nare the first distributed solutions available in the literature,\nwhich shows the simplicity and generality of Arabesque.\n\nArabesque’s embedding-centered API facilitates a highly\nscalable implementation. The system scales by spreading\nembeddings uniformly across workers, thus avoiding\nhotspots. By making it explicit that embeddings are the fundamental\nunit of exploration, Arabesque is able to use fast\ncoordination-free techniques, based on the notion of embedding\ncanonicality, to avoid redundant work and minimize\ncommunication costs. It also enables us to store embeddings\nefficiently using a new data structure called Overapproximating\nDirected Acyclic Graph (ODAG), and to devise a\nnew two-level optimization for pattern-based aggregation,\nwhich is a common operation in graph mining algorithms.\n\n\nArabesque is implemented as a layer on top of Apache\nGiraph [3], a Pregel-inspired graph computation system,\nthus allowing both graph computation and graph mining\nalgorithms to run on top of the same infrastructure. The\nimplementation does not use a TLV approach: it considers\nGiraph just as a regular data parallel system implementing\nthe Bulk Synchronous Processing model.\nTo summarize, we make the following contributions:\n\n• We propose embedding exploration, or “think like an embedding”,\nas an effective basic building block for graph\nmining. We introduce the filter-process computational\nmodel (Section 3), design an API that enables embedding\nexploration to be expressed effectively and succinctly,\nand present three example graph mining applications\nthat can be elegantly expressed using the Arabesque\nAPI (Section 4).\n\n• We introduce techniques to make distributed embedding\nexploration scalable: coordination-free work sharing, ef-\nficient storage of embeddings, and an important optimization\nfor pattern-based aggregation (Section 5).\n\n• We demonstrate the scalability of Arabesque on various\ngraphs. We show that Arabesque scales to hundreds of\ncores over a cluster, obtaining orders of magnitude reduction\nof running time over the centralized baselines (Section\n6), and can analyze trillions of embeddings on large\ngraphs.\n\nThe Arabesque system, together with all applications\nused for this paper, is publicly available at the project’s website:\nwww.arabesque.io.\n\n#### 翻译\n\nGraph data is ubiquitous in many fields, from the Web to advertising and biology, and the analysis of graphs is becoming increasingly important.\n\n>图形数据在许多领域普遍存在，从网站到广告业和生物学，并且分析图形正在变得越来越重要。\n\nThe development of algorithms for graph analytics has spawned a large amount of research, especially in recent years.\n\n>图形分析算法的发展催生了大量的研究，尤其是在近些年来。\n\nHowever, graph analytics has traditionally been a challenging problem tackled by expert researchers, who can either design new specialized algorithms for the problem at hand, or pick an appropriate and sound solution from a very vast literature.\n\n>但是，图形分析历年来是具有挑战性的，由那些能够为了手上的问题设计新的专门的算法或者从非常庞大的文献中选择一个适当并且健全的解决方案的专家去解决。\n\nWhen the input graph or the intermediate state or computation complexity becomes very large, scalability is an additional challenge.\n\n>当输入图形或者中间状态或者计算复杂度非常大的时候，可测量性是一额外的挑战。\n\nThe development of graph processing systems such as Pregel [25] has changed this scenario and made it simpler to design scalable graph analytics algorithms.\n\n>图形处理系统的发展例如pregel改变了这种方案，并且使设计可升级的图形分析算法更加的简单。\n\nPregel offers a simple “think like a vertex” (TLV) programming paradigm, where each vertex of the input graph is a processing element holding local state and communicating with its neighbors in the graph.\n\n>Pregel 提供了一个简单的\"像顶点一样思考\"的编程范例，每一个输入图的顶点是一个保持局部状态的处理单元并且在图形中和它的邻点进行通讯。\n\nTLV is a perfect match for problems that can be represented through linear algebra, where the graph is modeled as an adjacency matrix (or some other variant like the Laplacian matrix) and the current state of each vertex is represented as a vector.\n\n>T L V 对那些通过线性代数表示的问题能够完美的匹配，在那些图形建模为邻接矩阵（或者一些其他变形像路普拉斯矩阵）和每一个顶点的当前状态被表示为一个向量的问题中。\n\nWe call this class of methods graph computation problems.\n\n>我们称这一类的方法叫做图计算问题\n\nA good example is computing PageRank [6], which is based on iterative sparse matrix and vector multiplication operations.\n\n>一个好的例子就是计算PageRank, 它是基于迭代稀疏矩阵和向量乘法运算。\n\nTLV covers several other algorithms that require a similar computational architecture, for example, shortest path algorithms, and over the years many optimizations of this paradigm have been proposed [17, 26, 36, 42].\n\n>TLV 涉及了一些其他的算法，它需要相似的计算结构，例如，最短路径算法，多年来，这种模式的许多优化已被提出来。\n\nDespite this progress, there remains an important class of algorithms that cannot be readily formulated using the TLV paradigm.\n\n>尽管这些进展，这里依然有一类重要的算法不可以使用TLV范例制定。\n\nThese are graph mining algorithms used to discover relevant patterns that comprise both structurebased and label-based properties of the graph.\n\n>这些就是用于发现相关模式的基于结构和基于表的图的性质的图挖掘算法。\n\n\n Graph mining is widely used for several applications, for example, discovering 3D motifs in protein structures or chemical compounds, extracting network motifs or significant subgraphs from protein-protein or gene interaction networks, mining attributed patterns over semantic data (e.g., in Resource Description Framework or RDF format), finding structure content relationships in social media data, dense subgraph mining for community and link spam detection in web data,among others.\n\n>图挖掘广泛的用于一些应用，例如发现蛋白质结构中或者化学物质中的3D图案，从蛋白质或者基因交互网络中提取网络图案或者重要的子图，(例如在资源描述框架或者R D F 格式中)，正在使用发音在社会媒体数据，密集的子图挖掘社区和链接的垃圾邮件检测在Web数据中发现结构内容的关系，等等。\n\n Graph mining algorithms typically take a labeled and immutable graph as input, and mine patterns that have some algorithm-specific property (e.g., frequency above some threshold) by finding all instances of these patterns\nin the input graph.\n\n\n>图形挖掘算法通常采用一个标记和不可变的图形作为输入,和具有一些算法特性的挖掘模式（例如频率高于某个阈值），通过在输入图中的样式的所有实例。\n\nSome algorithms also compute aggregated metrics based on these subgraphs。\n一些算法也计算基于这些子图的综合指标。\n\n\nFigure 1: Exponential growth of the intermediate state in\ngraph mining problems (motifs counting, clique finding,\nFSM: Frequent subgraph mining) on different datasets.\n\n>图一：在不同的数据集中图挖掘问题中的中间状态的指数增长（图案计数，派系的发现，频繁子图挖掘）\n\nDesigning graph mining algorithms is a challenging and\nactive area of research.\n\n>设计图挖掘算法在研究中是一个具有挑战性和活跃的领域\n\n\nIn particular, scaling graph mining\nalgorithms to even moderately large graphs is hard.\n\n>尤其是，将图挖掘算法应用于中等大小的图是困难的\n\nThe set of possible patterns and their subgraphs in a graph can be\nexponential in the size of the original graph, resulting in an\nexplosion of the computation and intermediate state.\n\n>图表中的可能的模式集和他们的子图有可能是原始图大小的指数倍，导致了爆炸性的计算和中间状态。\n\nFigure 1 shows the exponential growth of the number of “interesting”\nsubgraphs of different sizes in some of the graph\nmining problems and datasets we will evaluate in this paper.\n\n>图一展示在一些图挖掘问题中不同大小的“intersting”子图的数量的爆炸性增长并且我们将评估文本的数据集。\n\nEven graphs with few thousands of edges can quickly generate\nhundreds of millions of interesting subgraphs.\n\n>即使是数千个边的图也能生成数亿的\"intersting\"子图。\n\nThe need for enumerating a large number of subgraphs characterizes\ngraph mining problems and distinguishes them from graph\ncomputation problems.\n\n>图挖掘问题以需要枚举很大数量的子图为特征并且将其于图计算问题区别开来。\n\n Despite this state explosion problem,\nmost graph mining algorithms are centralized because of the\ncomplexity of distributed solutions.\n\n>尽管这个状态是爆炸性的问题，但是大多数图形挖掘算法是集中式的，因为\n分布式解决方案的复杂性。\n\n\nIn this paper, we propose automatic subgraph exploration\nas a generic building block for solving graph mining\nproblems, and introduce Arabesque, the first embedding exploration\nsystem specifically designed for distributed graph\nmining.\n\n>在这篇文章中，我们把自动子图搜索看作是一个解决图挖掘问题的通用构建，并且介绍阿拉伯图案\n，它是第一个为分布式图挖掘设计的嵌入式的探索系统\n\nConceptually, we move from TLV to “think like an\nembedding” (TLE), where by embedding we denote a subgraph\nrepresenting a particular instance of a more general\ntemplate subgraph called a pattern (see Figure 2).\n\n>从概念上讲，我们从TLV移动到了“像嵌入一样思考”（TLE）,通过嵌入，我们表示一个子图\n通过表示一个称之为模式的更一般的模板的特别的实例（看图二）\n\nArabesque defines a high-level filter-process computational\nmodel.\n\n>阿拉伯图案定义了一个高层次的过滤过程计算模型。\n\nGiven an input graph, the system takes care\nof automatically and systematically visiting all the embeddings\nthat need to be explored by the user-defined algorithm,\nperforming this exploration in a distributed manner.\n\n>给出一个输入图，系统会自动的，系统性的关注访问所有的那些需要通过通过分布式的方式进行的自定义算法探索的嵌入部分。\n\nThe system passes all the embeddings it explores to the application,\nwhich consists primarily of two functions: filter, which indicates whether an embedding should be processed, and process,which examines an embedding and may produce some output.\n\n>系统通过所有的嵌入部分并暴露给应用，应用主要由两个函数组成：过滤器，指示是否嵌入部分应该被处理。处理，审查一个嵌入部分和有可能处理一些输出。\n\nFor example, in the case of finding cliques the filter function prunes embeddings that are not cliques, since none of their extensions can be cliques, and the process function outputs all explored embeddings, which are cliques by construction.\n\n>例如，在发现子图派系的案例中过滤器的功能用于修剪不是派系的嵌入部分，因为他们的拓展没有可能是派系，并且处理函数输出所有探索的嵌入部分，那些嵌入的部分通过建设而形成派系。\n\nArabesque also supports the pruning of the exploration\nspace based on user-defined metrics aggregated across multiple embeddings.\n\n>Arabesque还支持修剪基于用户定义的度量标准聚合的空间多次嵌入的探测。\n\nFigure 2: Graph mining concepts: an input graph, an example\npattern, and the embeddings of the pattern. Colors represent\nlabels. Numbers denote vertex ids. Patterns and embeddings\nare two types of subgraphs. However, a pattern is\na template, whereas an embedding is an instance. In this example,\nthe two embeddings are automorphic.\n\n>图二：\n\nThe Arabesque API simplifies and thus democratizes the\ndesign of graph mining algorithms, and automates their execution\nin a distributed setting. We used Arabesque to implement\nand evaluate scalable solutions to three fundamental\nand diverse graph mining problems: frequent subgraph mining,\ncounting motifs, and finding cliques. These problems\nare defined precisely in Section 2. Some of these algorithms\nare the first distributed solutions available in the literature,\nwhich shows the simplicity and generality of Arabesque.\n\nArabesque’s embedding-centered API facilitates a highly\nscalable implementation. The system scales by spreading\nembeddings uniformly across workers, thus avoiding\nhotspots. By making it explicit that embeddings are the fundamental\nunit of exploration, Arabesque is able to use fast\ncoordination-free techniques, based on the notion of embedding\ncanonicality, to avoid redundant work and minimize\ncommunication costs. It also enables us to store embeddings\nefficiently using a new data structure called Overapproximating\nDirected Acyclic Graph (ODAG), and to devise a\nnew two-level optimization for pattern-based aggregation,\nwhich is a common operation in graph mining algorithms.\n\n\nArabesque is implemented as a layer on top of Apache\nGiraph [3], a Pregel-inspired graph computation system,\nthus allowing both graph computation and graph mining\nalgorithms to run on top of the same infrastructure. The\nimplementation does not use a TLV approach: it considers\nGiraph just as a regular data parallel system implementing\nthe Bulk Synchronous Processing model.\nTo summarize, we make the following contributions:\n\n• We propose embedding exploration, or “think like an embedding”,\nas an effective basic building block for graph\nmining. We introduce the filter-process computational\nmodel (Section 3), design an API that enables embedding\nexploration to be expressed effectively and succinctly,\nand present three example graph mining applications\nthat can be elegantly expressed using the Arabesque\nAPI (Section 4).\n\n• We introduce techniques to make distributed embedding\nexploration scalable: coordination-free work sharing, ef-\nficient storage of embeddings, and an important optimization\nfor pattern-based aggregation (Section 5).\n\n• We demonstrate the scalability of Arabesque on various\ngraphs. We show that Arabesque scales to hundreds of\ncores over a cluster, obtaining orders of magnitude reduction\nof running time over the centralized baselines (Section\n6), and can analyze trillions of embeddings on large\ngraphs.\n\nThe Arabesque system, together with all applications\nused for this paper, is publicly available at the project’s website:\nwww.arabesque.io.\n\n\n#### 段落翻译\n\n图形数据在许多领域普遍存在，从网站到广告业和生物学，并且分析图形正在变得越来越重要。\n图形分析算法的发展催生了大量的研究，尤其是在近些年来。但是，图形分析历年来是具有挑战性的，由那些能够为了手上的问题设计新的专门的算法或者从非常庞大的文献中选择一个适当并且健全的解决方案的专家去解决。当输入图形或者中间状态或者计算复杂度非常大的时候，可测量性是一额外的挑战。\n\n\n图形处理系统的发展例如pregel 改变了这种方案，并且使设计可升级的图形分析算法更加的简单。\nPregel 提供了一个简单的\"像顶点一样思考\"的编程范例，每一个输入图的顶点是一个保持局部状态的处理单元并且在图形中和它的邻点进行通讯。T L V 对那些通过线性代数表示的问题能够完美的匹配，在那些图形建模为邻接矩阵（或者一些其他变形像路普拉斯矩阵）和每一个顶点的当前状态被表示为一个向量的问题中。我们称这一类的方法叫做图计算问题一个好的例子就是计算PageRank, 它是基于迭代稀疏矩阵和向量乘法运算。TLV 涉及了一些其他的算法，它需要相似的计算结构，例如，最短路径算法，多年来，这种模式的许多优化已被提出来。\n\n尽管这些进展，这里依然有一类重要的算法不可以使用TLV范例制定。这些就是用于发现相关模式的基于结构和基于表的图的性质的图挖掘算法.图挖掘广泛的用于一些应用，例如发现蛋白质结构中或者化学物质中的3D图案，从蛋白质或者基因交互网络中提取网络图案或者重要的子图，(例如在资源描述框架或者R D F 格式中)，正在使用发音在社会媒体数据，密集的子图挖掘社区和链接的垃圾邮件检测在Web数据中发现结构内容的关系，等等。图形挖掘算法通常采用一个标记和不可变的图形作为输入,和具有一些算法特性的挖掘模式（例如频率高于某个阈值），通过在输入图中的样式的所有实例。一些算法也计算基于这些子图的综合指标。\n\n![图1](./img/1.PNG)\n\n图一：在不同的数据集中图挖掘问题中的中间状态的指数增长（图案计数，派系的发现，频繁子图挖掘）\n\n设计图挖掘算法在研究中是一个具有挑战性和活跃的领域。尤其是，将图挖掘算法应用于中等大小的图是很困难的。图表中的可能的模式集和他们的子图有可能是原始图大小的指数倍，导致了爆炸性的计算和中间状态。图一展示在一些图挖掘问题中不同大小的“intersting”子图的数量的爆炸性增长并且我们将评估文本的数据集。即使是数千个边的图也能生成数亿的\"intersting\"子图。图挖掘问题以需要枚举很大数量的子图为特征并且将其于图计算问题区别开来。尽管这个状态是爆炸性的问题，但是大多数图形挖掘算法是集中式的，因为分布式解决方案的复杂性。\n\n在这篇文章中，我们把自动子图搜索看作是一个解决图挖掘问题的通用构建，并且介绍阿拉伯图案\n，它是第一个为分布式图挖掘设计的嵌入式的探索系统。从概念上讲，我们从TLV移动到了“像嵌入一样思考”（TLE）,通过嵌入，我们表示一个子图通过表示一个称之为模式的更一般的模板的特别的实例（看图二）。阿拉伯图案定义了一个高层次的过滤过程计算模型。给出一个输入图，系统会自动的，系统性的关注访问所有的那些需要通过通过分布式的方式进行的自定义算法探索的嵌入部分。系统通过所有的嵌入部分并暴露给应用，应用主要由两个函数组成：过滤器，指示是否嵌入部分应该被处理。处理，审查一个嵌入部分和有可能处理一些输出。例如，在发现子图派系的案例中过滤器的功能用于修剪不是派系的嵌入部分，因为他们的拓展没有可能是派系，并且处理函数输出所有探索的嵌入部分，那些嵌入的部分通过建设而形成派系。\n\n![图2](./img/2.PNG)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1]: https://jchanji.github.io \"主页\"\n","tags":["distribute system"],"categories":["-articals"]},{"title":"ubuntu只显示桌面，没有菜单栏","url":"/year/09/03/ubuntu_only_background/","content":"## 前言\n>在裝輸入法的時候好像刪了什麼東西導致電腦重啓的時候只能顯示桌面背景和文件，導航等都沒了，頓時嚇壞我了，找了好多教程終於成功了。\n\n<!--more-->\n\n##安裝unity\n```markdown\nsudo apt-get install unity\n```\n##刪除配置文件\n```markdown\nsudo rm -rf .conf\nsudo rm -rf .gconfg\nsudo rm -rf ~/.Xauthority\nreboot\n```\n本教程不一定對其他情況也適合\n","tags":["-ubuntu"],"categories":["-others"]},{"title":"伪分布式spark安装配置","url":"/year/09/03/spark_step/","content":"\n\n## 前言\n>本教程为spark的伪分布式教程，学生党条件有限所以伪分布式应该是比较好的选择。其中要注意的是版本匹配的问题。\n\n<!--more-->\n\n## 一、版本\n1. CentOS7\n2. jdk:jdk1.8.0_131\n2. hadoop：2.6.0\n3. scala:2.11.11\n4. spark:2.1.1\n\n## 二、下载\n1. [spark2.1.1](http://spark.apache.org/downloads.html)\n2. [scala2.11.11](http://www.scala-lang.org/download/all.html)\n\n## 三、安装配置\n\n### 1、解压,修改权限\n```markdown\nsudo tar -zxvf spark-2.1.1-bin-hadoop2.6.tgz -C /usr/local\ncd /usr/local\nsudo chown -R hadoop:hadoop ./spark\n```\n### 2、在解压的spark目录下新建文件/test/hellospark,写上内容\n\n### 3、进入scala模式\n\n```markdown\n   cd /spark/bin\n   ./spark-shell\n```\n\n### 4、运行代码\n\n```markdown\n5、val lines = sc.textFile(“../test/hellospark”)\n   lines.count()\n   lines.first()\n```\n\n### 5、修日志级别\n```markdown\n   cd /usr/local/spark/conf\n   cp  log4j.properties.template  log4j.properties   \n   sudo vim log4j.properties\n```\n   将其中的rootCategory=INFO 改为 WARN\n","tags":["big data"],"categories":["-bigdata"]},{"title":"sumlime text3 配置Markdown","url":"/year/09/03/markdown/","content":"## 一、sumlime text3 配置Markdown和常用快捷键\n### 1、sumlime text3 配置Markdown\n>#### 1、安装package Control<br>\n\n>#### 2、安装Markdown Preview<br>\n>&ensp; 2.1、 按Shif + Alt + P打开<br>\n>&ensp; 2.2、输入pcip,回车（进入install package）<br>\n\n<!--more-->\n\n>#### 3、安装Markdown  Editing<br>\n>&ensp;3.1、进入 install package<br>\n>&ensp;3.2、输入 Markdown Editing // Markdown编辑和语法高亮支持<br>\n\n>#### 4、安装Markdown  Previewer<br>\n>&ensp; 4.1、进入 install package<br>\n>&ensp; 4.2、Markdown  Previewer  //Markdown导出html预览支持<br>\n\n>#### 5、安装OmniMarkup Previewer<br>\n>&ensp; 5.1、进入 install package<br>\n>&ensp; 5.2、OmniMarkup Previewer //在浏览器中实时预览\n\n### 2、常用快捷键\n> 1. Ctrl + Alt + O //在浏览器中打开\n> 2. Alt + M  //生成html文件\n> 3. Ctrl+Alt+O: Preview Markup in Browser.\n> 4. Ctrl+Alt+X: Export Markup as HTML.\n> 5. Ctrl+Alt+C: Copy Markup as HTML.\n\n## 二、使用Cmd markdown在线编辑\n> 1. 在线[编辑] [1] 网址\n> 2. 也可以下载客户端离线编辑[客户端] [2]\n> 3. 如果想转成html等其他功能需要付费，不过基础功能已经差不多够用了\n\n\n## 遇到的问题\n\n### 1、Ctrl+Alt+O后打开了浏览器但是不能够预览ｍａｒｋｄｏｗｎ页面\n\n```markdown\n    在 Preferences > Package Settings > OmniMarkupPreviewer > Settings - User 中粘贴以下代码即可\n\n    {\n    \"renderer_options-MarkdownRenderer\": {\n        \"extensions\": [\"tables\", \"fenced_code\", \"codehilite\"]\n    }\n    }\n```\n\n### 2、在sublime text3 中切换不了中文\n```markdown\nsudo apt-get update && sudo apt-get upgrade\ngit clone https://github.com/lyfeyaj/sublime-text-imfix.git\ncd ~/sublime-text-imfix\nsudo ./ sublime-imfix\n然后重启sublime text3\n```\n\n\n[1]: https://www.zybuluo.com/mdeditor \"Cmd Markdown\"\n[2]: https://www.zybuluo.com/cmd/ \"下载\"\n\n","tags":["markdown"],"categories":["-others"]},{"title":"从互联到新工业革命-读后感","url":"/year/09/03/internet_of_everthing_for_new_industrial_revolution/","content":"\n## 目录\n### [一、工业4.0\"网红\"的养成之路](#one)\n### [二、\"工业互联网\" VS \"工业4.0\"](#two)\n### [三、中国制造2025](#three)\n### [四、工业革命升级技能点](#forth)\n### [五、人工智能](#firth)\n### [六、工业互联网的智能网络](#six)\n### [七、结束语](#end)\n## 前言\n>&nbsp;&nbsp;&nbsp;&nbsp;有幸能够参加东华大学计算机科学与技术学院举办的“大数据与智能制造”暑期夏令营。虽然只有短短的两天时间，但是收获颇多，尤其是听了燕山大学机械学院院长张立杰教授“智能制造和传统制造”的演讲和美国佛罗里达大学教授李晓林教授“Creating Intellignece via big learning”的演讲，对智能制造和人工智能领域了更加深刻的印象。由于我比较愚钝，具体的演讲内容不能详细的复述出来。再次由衷的感谢东华大学的常珊老师免费提供给我们《从互联到新工业革命》这本书，今天在火车上读完了这本通俗易懂而又见解深刻的书,无意发现,张立杰教授，李晓林教授，刘云浩教授，在智能制造方面的见解英雄所见略同，而为了更加体系的介绍和便于自己思路的清理，下面更多介绍清华大学软件学院院长刘云浩教授对互联网时代和新工业革命大潮的理解和体会并且再加上一点我个人的浅陋的见解。 由于本人文采有限，不能够很全面的写出刘老师书中的方方面面也无法用诙谐的语句吸引读者兴趣，所以我极力推荐大家读一读刘云浩老师的作品《从互联到新工业革命》（清华大学出版社）。由于我也只是泛读了一遍,所以写博客的时候也是第二次更加粗略的阅读，当中有什么见解不到的地方欢迎联系我（邮箱见文章底部）。\n\n<!--more-->\n\n## 导言\n>&nbsp;&nbsp;&nbsp;&nbsp;由于我也是刚接触大数据，人工智能不久，虽然对其中的技术细节还不是非常的了解，但引用刘国华教授\"纸上谈兵\"的观点，如果不去\"纸上谈兵\"而直接去埋头编程那么有可能永远都完成不了项目，或者当中算法效率是很低的。同样我觉得，学习一个完全陌生的专业，如果连它的发展趋势和技术线路都没有搞清楚的话，那么也只是一头雾水的填鸭式的学习,不利于以后潜力的挖掘和能力的提升。下面我们就\"空谈\"一些对技术能力提高\"无用\"的，空泛的，所谓\"夸夸而谈\"的观点。\n\n## <span id=\"one\"> 一、工业4.0\"网红\"的养成之路</span>\n>&nbsp;&nbsp;&nbsp;&nbsp;目前为止确切的有三次工业革命：1769年瓦特发明蒸汽机，标志着机械化的时代到来,机器代替了人类一部分的体力活动，人类向机械化迈进;1869年，德国西门子公司发明了第一台交流发动机，电器取代了机器，电器动力取代了蒸汽动力，从此促进了大规模，批量化的生产。也是从此时开始，东方开始落后西方；1969年，第一块可编程逻辑控制器Modicon 084问世，这标志这电子信息技术的发明并且直接导致了产品生产的高度自动化。此外还有一件划时代的事发生了，便是阿帕网的形成，也就是互联网的雏形。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;近两百年之前的工业革命，在之后的150年里使西方人均收入增长了13倍，而1800年以前，西方人均经济翻一倍则需要800年。这足以见得这几次工业革命对人类社会生产力的解放是多么的恐怖，这也预示着，人类社会的发展速度将会越来越快。那么第四次工业革命会是在2069年吗？显然，就目前的形式来看完全等不到2069年。新工业革命已经隐约到来，虽然我们不能够从未来的角度来看现在来，判定是否是第四次工业革命，就像前三次工业革命一样，发生之后才意识具有多么伟大的意义，但是，我们已经确切的感受到新工业革命了。\"人类第一次成功的在事前预测了一次革命，而不是像以前一样事后才认识到是一场革命\",正如刘老师所言。\"工业4.0\"\n由孔翰宁(Henning Kagermann),沃夫冈.瓦尔斯特(\"Wolfgang Wahlster\"),沃尔夫迪特尔.卢卡斯(Wolf-Dieter Lukas)三位博士提出,由于\"产官学\"（ 产业界，政府，学术界）属性的与生俱来，很快便由德国工程院，弗劳恩获夫协会，西门子公司等接手，组成了工业4.0小组,于是工业4.0迅速的冲出了德国，走向了世界。所以\"工业4.0\"正是天时地利人和的结果。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;这里插入一些我的人生观，不想听大道理的可以跳过了。其实很多事情都是水到而渠成的,尤其是知识积累更是如此。面对飞速发展的软件行业，作为初级程序员,很多人都想一口吃个胖子,想要快速的掌握开发技能,喜欢看速成的视频,教程，包括我也是如此,但很多时候却走了很多的弯路。因为基础知识不扎实而找东找西,就是一根筋的想要找和自己的问题一摸一样的解答，却不知道或者懒得去变通一下代码，或者花点时间去专研一下代码中的逻辑思维，从中受到启发。与其花半天时间去研读代码。却更愿意去花一天时间尝遍百度上的所有教程。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;官方对于\"工业4.0\"的解释是,\"工业4.0包括将信息物理系统(Cyber physical System，CPS)技术一体化应用与制造业和物流行业,以及在工业生产过程中采用物联网和服务技术\"。从这段定义中个我们可以看到很熟悉的一个词\"物联网\",所以从此也可以看出以后的IT热门方向。物联网从前几年开始变得炙手可热，但发展一直没有想象中的那么迅猛，这和很多方面因素有关，包括硬件支持，传输技术，等等。但这丝毫不影响其发展趋势，因为物联网还是在不断发展的，而且越来越快,其模型成熟的时间决定着万物互联时代到来的时间。\"工业4.0\"产生的\"智能工厂\"和\"智能生产\"将改变传统的批量统一化的生产模式,实现高度灵活的个性化和数字化生产及服务，最终使生产更智能，更高效，跟快速，更经济。\n\n## ps1\n>由于我的手速有限，时间紧迫，今天只能谈到这。作为有点学术性质而又不深入具体细节的博客，希望大家能够当成故事看，了解当今的IT界的发展方向。我每天晚上会抽出11点之后断网的时间续写，第二点早上9点跟新,时间有限，我会尽快的完结。\n\n## <span id=\"two\"> 二、\"工业互联网\" VS \"工业4.0\"</span>\n>&nbsp;&nbsp;&nbsp;&nbsp;面对的德国的高歌猛进，世界第一大国美国怎能无动于衷了？毕竟在这个星球上主导权决定着发言权，就算是没有足够的主导权也不能牵制于人。由沙利文（Frost & Sullivan）这家咨询公司在一份报告中创造性的提出了“工业互联网”这个概念。也因此给沙利文公司在工业制造领域带来了话语权。公司还顺带的设立了“制造领袖奖”,2016年通用公司就很高兴的领了这个奖。2014年3月由通用电气（提供综合技术与服务）联合AT&T(M2M的解决方案)、Cisco(提供网络解决方案)、Intel(半导体、芯片和处理器)、IBM(智慧地球)成立了“工业互联网联盟(Industrial Internet Consortium,IIC)”。很显然工业互联网这块大蛋糕美国怎么会任由他人分割了,到2015年初，该联盟成员已经达到130家，西门子，华为等号称要自己做工业互联网平台的企业也没能抵制住诱惑。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;很失望的是中国还是一如既往的落后与西方国家，作为发展中国家，不得不承认在先进技术和理念方面中国目前只能去模仿，距离成为“中国制造2025”目标中的世界一流的工业水准还是有很大的差距的。毕竟，不要说“工业4.0”，中国大部分企业还停留在“工业2.0”的水准，“工业3.0”水平也是很弱，这和很多因素有关，但我们还是对未来充满希望的，毕竟科技的快速发展网络的普及化，信息的透明化，以及人才的全球流通，给发展中国家带来的好处是可以快速的跟上队伍。我国并不缺少运行产业联盟的企业，但是成功的却非常少，其中企业自身创新能力弱，国际视野的局限性大是一方面，缺乏一个良好的利益共享机制，无法发挥每个企业的特长也是国内产业联盟难以落地的重要原因。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;\"工业互联网\"和\"工业4.0\"中国到底应该站在哪一边了？这就要进一步的了解这两个热门词汇了。“工业互联网”可以说是自顶向下，侧重于利用互联网的技术来改善生产设备和产品服务。从物联网、云计算、大数据分析等信息技术的角度出发，将之应用于工业领域，改造工业生产的产品服务和管理过程等。“工业4.0”则是自下而上，侧重于在生产与制造过程的智能化、数字化。以生产设备为核心的CPS为出发点，推进数据融合和服务共享，从而推及工业生产过程以及产品服务等。虽然由于两国的的产业优势不同导致的工业互联网的结构正好颠倒，但其中的核心思想还是十分相似的。2016年3月，\"工业4.0平台\"和\"工业互联网联盟\"在瑞士苏黎世初步达成合作意向，开始了强强联合。这也是应了\"马太效应\"，\"凡有的，还要加给他，叫他有余；凡没有的，连他所有的也要夺去\"。\n\n## ps2\n>不知不觉已经12点了，为了不打扰舍友休息，今天就到这里。\n\n## <span id=\"three\"> 三、中国制造2025</span>\n>&nbsp;&nbsp;&nbsp;&nbsp;世界上很多国家都指定了符合本国国情的工业互联网规划，但基本上都是依据“工业4.0”或者“工业互联网”进行改编。同样，中国也不例外。“中国制造2025”以促进制造业创新发展为主题，以提质增效为中心，以加快新一代信息技术与制造业深度融合为主线，以推进智能制造为主攻方向，以满足经济社会发展和国防建设对重大技术装备的需求为目标，强化工业基础能力，提高综合集成水平，完善多层次多类型的培养体系，促进产业转型升级，培育有中国特色的制造文化，实现制造业由大变强的历史跨越。坚持“创新驱动、质量为先、绿色发展、结构优化、人才为本”的基本方针，坚持“市场主导、政府引导，立足当前、着眼长远，整体推进、重点突破，自主发展、开放合作”的基本原则，通过“三步走”实现制造强国的战略目标：第一步，到2025年迈入制造强国行列；第二步，到2035年中国制造业整体达到世界制造强国阵营中等水平；第三步，到新中国成立一百年时，综合实力进入世界制造强国前列。看到这一大推的雄伟措辞，不得不说中国最强的专家真的不是盖的，由50多位院士100多位领域专家共同指定的规划，将中国制造转变为中国智造的伟大目标高调的向全世界展示。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;中国制造业的现状其实和国足相差无几，所以注定不能自上顶而下的对中国的工业基础进行改革，薄弱的工业基础实在是堪忧。从中国的物流业占GDP比重是美国德国的发达国家的两倍左右就可以看出其中的差距。所以“中国制造2025”更倾向于“工业4.0”的自下而上的进行改革。\n\n## <span id=\"forth\">四、工业革命升级技能点</span>\n>&nbsp;&nbsp;&nbsp;&nbsp;从技术角度上来说，第四次工业革命是一场从嵌入式系统到信息物理融合系统的技术变革，通过物联网，云计算，大数据在工业中的运用，促成基于网络化的变革。其关键的技术难点和重点在于实现智能化设备自知而治，泛在化网络（无处不在的网络）互联互通，中心化数据实时实效，开放化服务相辅相成，建立能够在联网对象彼此之间，网络对象和网络环境之间，联网对象和人之间共享的工业互联网，形成物联网，数据联网，服务联网以及人员联网的网络化开放平台。\n\n## PS3\n>第二遍浏览时发现想要阐述出书中的核心思想还是很难的，越来越发现写不下去了。所以还是得放下键盘，再仔细思考一番，理清思路。\n\n## <span id=\"firth\">五、人工智能</span>\n>&nbsp;&nbsp;&nbsp;&nbsp;谈到智能工业，智能电网，使用的往往是Smart这个词，而人工智能则使用Aritifical Intelligence（AI）。其原因是人工智能突出的是机器的反映方式能够类似人的智能。而半个世纪以来，人工智能的发展历程很坎坷，机器是否智能一直是一个很有争议的话题。之前之所以认为机器不具有智能是因为机器所完成的任务都是人类所定义好的，并没有超出人类的认知范围或者能力限制。而现在有些深度学习训练出来模型很多已经超出了人的认知范围，因为人能通过参数，阈值的对结果值进行調优，但是算法内部到底是怎样实现的却很难被人所知。随之而来的问题就是，不能够确保其训练出来的模型能够永远的正常使用，所以在金融行业，医疗行业等安全系数要求很高的行业中使用起来是需要对其进行风险评估的。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;人工智能发展的阻碍主要有三个方面。第一、计算机的计算能力。随着硬件的不断升级计算计的运算能力显著提升，但是这不一定代表就可以解决全部的问题。仍然有很多无法优化的算法是需要大量的计算资源的，深度学习出来之后CPU就已经不适合用作为机器学习开发的硬件了，GPU（图形处理单元）将逐步的取代CPU在机器学习中的地位。第二、计算机对真实世界的感知能力。道现在为止人类研究的人工智能在“智力“上已经很高了，但是还是无法像人一样感知世界。对计算机而言实现逻辑推理等人类高级智慧只需要相对较少的计算能力，而现实感知、运动等人类低级智慧却需要巨大的计算资源。第三、推理和逻辑框架。人工智能也无法像人类一样在没有老师的情况下还能够自行的推理并且联想学习，也就是不具备迁移学习的能力。所以人工智能要模拟人的智能其难点不在于人脑进行的各种必然性推理，而是最能体现人的能动性和创造性的不确定推理。\n\n## <span id=\"six\">六、工业互联网的智能网络</span>\n>&nbsp;&nbsp;&nbsp;&nbsp;一个个孤立的点所包含的信息是很少的，但是将这些点之间相互连接起来，其中包含的信息量就极其巨大了。工业互联网的核心就是将原本割裂的工业数据实现流通，从而变成一个智能网络。我们可以概括为四个环节，即”感，联，知，控“。感，即感知层，机器，机组，物料，人员等物体之间能够相互感知，交互协作，从而实现不同生产实体之间的深度协同。联，即网络互联层，旨在将多元对象组成的异构复杂网络之间形成彼此互联互通的泛在化网络（可以简单理解为无处不在的网络）。知，即数据分析层，网络化的数据有些在传输过程中被即时处理，更多的是汇聚到中心节点后被集中处理。数据分析层负责工业大数据的存储、处理、建模、挖掘、和优化等方面。控，即开放服务层，基于工业大数据形成的决策依据，通过多种面向工业生产应用的开放式，共享型的标准化服务，被工业生产部门调用和实施，反馈到工业生产的各个环节，从而实现对工业生产的控制和调节。从网络角度出发，形成了实体联网，数据联网，服务联网的三重联网。\n\n### <span id=\"end\">七、结束语</span>\n\n>&nbsp;&nbsp;&nbsp;&nbsp;按照书本上的章节的话应该还有几章的内容没有讲到，主要内容是作者结合了中国工业的现状，对前几年发生的事情进行分析，对未来的展望。所以，到此为止就算是完结散花了。但总是感觉自己写的还很不到位，估计是因为心境的问题吧，有时候心境达不到，确实不能够写出什么深层次的东西，总是感觉很是词穷。下面又是我个人的主观瞎想了，没有兴趣的可以拜拜了。\n\n>&nbsp;&nbsp;&nbsp;&nbsp;有一句话叫做“软件定义世界”，而以后可能是“人工智能定义软件“。现在人工智能已经逐渐的深入到人们的生活当中来，语音识别，图像识别，等传统的机器学习任务现在使用深度学习对其进行训练，其在人脸识别，语音识别，游戏等很多方面已经超过了人类。所以深度学习的出现，可以说是又一次的焕发了机器学习的活力。毕竟人类从上个世界五六十年代一直到到现在在人工智能方面，能够显著的看出成果并取得重大突破的就是现在，几乎每天的新闻都是某某某使用深度学习神经网络实现了啥啥啥历史性的突破。\n而人工智能，大数据，云计算近几年来的火爆都不是”横空出世“的。个人觉得其最根本的原因在于互联。正是因为互联，所以数据呈现爆炸式的增长，所以单机的性能远远的不能满足大数据的需求。所以便出现了分布式集群，hadoop框架的诞生更是刺激了大数据的飞速发展。然而个人公司想要购置维护一个机器集群其话费可想而知，估计初创公司在第一步的购置硬件支持上就已经阻力重重了。这时候云计算平台的出现无异于雪中送炭，你只需要根据你所需要的服务，按照资源分配的多少，租用的时常，支付相应的费用便可，这样就大大的降低了业务快速上线的难度。云计算不仅仅给大数据提供了可扩展的平台，也是给人工智能提供了便捷，使得个人的算法实践能够方便，快捷，低成本的运行起来，而不必担心购买昂贵的GPU,显卡等硬件配置并且考虑使用完后的处置。我发像我已经犯困了，舍友以后也能好好睡觉了，完结。\n\n[1]: https://jchanji.github.io \"主页\"\n","tags":["Made in China 2025"],"categories":["-articals"]},{"title":"win10 安装ubuntu 16.04","url":"/year/09/03/install_ubuntu/","content":"\n## 前言\n>在win10下安装ubuntu双系统。在笔记本安装ubuntu的时候遇到了很多的挫折，曾经也放弃过，但很不幸的是，在未来的今天又碰上了。有时候问题的答案很简单，但是需要大量的时间去得到它，并不是你的能力不行，而是网上的干扰答案实在是太多，无法分辨谁对谁假的时候，往往会一个个的试过去。我不认为这是个很笨的方法，因为多花点时间总能体会更多的东西。貌似废话有点多，下面直接上干货。\n\n<!--more-->\n\n## 一、版本\n1. win10 企业版\n2. ubuntu 16.04\n3. UltraSO 9.6.6.3300\n4. 显卡：GTX 965M\n5. cup:i7-6700HQ\n\n\n## 二、下载\n1. [ubuntu 16.04](http://releases.ubuntu.com/16.04.2/ubuntu-16.04.2-desktop-amd64.iso?_ga=2.92867550.254780022.1497589112-1524410519.1497589112)\n2. [UltraSO](http://172.19.251.251/files/510300000015EB65/dl.softmgr.qq.com/original/Compression/uiso9_cn_9.6.6.3300.exe)\n\n## 三、安装配置\n\n### 1.将镜像刻录到u盘（至少4g）\n```markdown\n打开UltraSO，点击试用。\n1、文件->打开->镜像位置\n2、启动->写如硬盘映像->写入\n```\n### 2、分配空闲分区\n```markdown\n1、右击我的电脑->管理->磁盘管理\n2、选择非系统盘的主分区，右击->压缩卷，选择压缩大小，一般为50G,我的是100G,\n根据自己磁盘情况分配。\n```\n\n### 3、将电脑设置为U盘启动\n```markdown\n插入U盘，进入BIOS将U盘设置为启动项\n注：不同主板的BIOS大多都不会相同，所以根据自己电脑型号到网上查找。\n```\n### 4、安装系统\n#### 1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。\n#### 2、卡在logo\n```markdown\n重新启动电脑，在选择系统安装的界面按e,进入grup界面，让后在splash后面加上：空格nomodeset空格，按F10执行。后面重启出了最后一次也是一样操作\n```\n#### 3、创建分区\n```markdwon\n1、选择自己分配的空闲的磁盘，进行分盘\n2、分盘的注意点是：\n    2.1、 /：存储系统文件，建议10GB ~ 15GB,我分配16G；\n    2.2、 swap：交换分区，即Linux系统的虚拟内存，建议是物理内存的2倍,我分配16G；\n    2.3、 /home：建议最后分配所有剩下的空间；\n    2.4、 boot：包含系统内核和系统启动所需的文件，实现双系统的关键所在，建议200M,我分配400M。\n3、其他的默认或者根据自己需求设置，点击安装\n```\n\n### 5、分辨率问题\n一般比较新的N（英伟达）卡会出现没有安装驱动的问题，所以屏幕的分率很低，这时候就需要安装N卡驱动。直接安装会导致开机的时候卡在登陆界面进不去，所以必须借助于bumblebee(大黄蜂)，至于原因有兴趣的可以查一查，这里不多阐述。\n```markdwon\nsudo apt-get install bumblebee bumblebee-nvidia primus linux-headers-generic\nReboot\n```\n重新启动后：\n```markdown\nsudo apt-get purge nvidia-* #删除所有的N卡驱动\nsudo add-apt-repository ppa:graphics-drivers/ppa  #添加第三方驱动源\nsudo apt-get update #更新源\nsudo  apt-cache search nvidia-*  #查询nvidia驱动可用版本，这里推荐到英伟达官网查看自己显卡驱动的版本，我的是375\nsudo apt-get install nvidia-375 # 安装驱动\n\n```\n打开软件更新器，然后将附加驱动－>未知换成显卡的驱动<br>\n最后重新启动，什么都不用做，等开机！\n\n\n[1]: https://jchanji.github.io \"主页\"\n\n","tags":["win 10"],"categories":["-others"]},{"title":"CentOS7 安装vnc","url":"/year/09/03/vnc/","content":"\n## 1.首先执行这一句防止系统文件被修改\n```markdown\nchattr +i /etc/resolv.conf\n```\n## 2、然后安装tigervnc\n```markdown\nsudo yum install -y tigervnc tigervnc-server\n```\n\n<!--more-->\n\n## 3.查看自己的服务器支持安装哪些包\n```markdown\nsudo yum grouplist\n```\n#### 查看自己的服务器里中Available Environment Groups下面有哪些可以安装的Desktop,我这里的是GNOME Desktop\n## 4. 安装GNOME Desktop\n```markdown\nsudo yum groupinstall GNOME Desktop\n```\n\n## 5. 启动服务\n```markdown\nvncserver\n```\n## 6.连接vnc \n>第一次执行会提示输入密码，然后再验证输入一次回车，vnc服务端就算搭建好了！\n接下来在手机或者电脑上下载vnc客户端，输入你的IP:5901连接\n然后输入密码就可以看到你的服务器界面了！\n\n","tags":["vnc"],"categories":["-others"]},{"title":"CentOS 7 安装eclipse mars 2","url":"/year/09/03/install_eclipse/","content":"\n#### 操作系统：CentOS 7\n#### eclispe版本：Eclipse Mars 2\n\n<!--more-->\n\n## 下载安装\n> ### 1.下载安装[eclipse][2]\n\n> ### 2.解压 \n\n```markdown\n sudo tar -zxvf [下载的安装包名称] -C [安装的目录]  \n```\n\n> ### 3.创建软链接\n\n```markdown\n sudo ln -s /安装的目录/eclipse/eclipse  /usr/bin/eclipse\n```\n\n> ### 4.添加图标\n\n```markdown\ngedit /usr/share/applications/eclipse.desktop\n```\n将下面内容添加到文件中\n```mrkdown\n[Desktop Entry]\nEncoding=UTF-8\nName=Eclipse\nComment=Eclipse Mar2\nExec=/usr/bin/eclipse\nIcon=/[解压的目录]/eclipse/icon.xpm\nCategories=Application;Development;Java;IDE\nVersion=1.0\nType=Application\nTerminal=0\n```\n\n\n[2]: http://mirrors.ustc.edu.cn/eclipse/technology/epp/downloads/release/mars/2/eclipse-jee-mars-2-linux-gtk-x86_64.tar.gz \"eclise下载\"","tags":["CentOS 7"],"categories":["-others"]},{"title":"伪分布式hbase安装配置","url":"/year/08/30/hbase_step/","content":"\n## 前言\n>网上有很多的教程，大体流程都差不多，但是在很多细节配置方面有点区别，本教程适用于伪分布式环境下（一般自己电脑上练习伪分布式够了）的hbase的基本安装配置。hadoop伪分布式环境已经搭建好,如果没有搭建好，推荐教程 [hadoop伪分布式教程](http://www.powerxing.com/install-hadoop-in-centos/),hbase官方[中文文档](http://abloz.com/hbase/book.html)\n\n<!--more-->\n\n## 一、版本\n\n1. CentOS7\n2. jdk:openjdk1.7.0_141\n2. hadoop：2.6.0\n3. hbase:0.98.13\n4. 一定要注意jdk,hadoop和hbase的版本匹配问题,可到官网查看！\n\n## 二、下载\n1.[hbase-0.98.13-hadoop2-bin.tar.gz](http://archive.apache.org/dist/hbase/0.98.13/hbase-0.98.13-hadoop2-bin.tar.gz)\n\n## 三、安装配置\n\n### 1、解压文件到指定目录\n```markdown\ntar -zxvf hbase-0.98.13-hadoop2-bin.tar.gz -C /usr/local\n```\n### 2、重命名\n```markdown\ncd /usr/local\nsudo mv [解压后的文件名] [hbase]\n```\n### 3、修改hbase-site.xml\n```markdown\ncd /hbase/conf\nsudo vim hbase-site.xml\n```\n将内容改为\n```markdown\n\n<configuration>\n<property>\n    <name>hbase.rootdir</name>\n    <value>hdfs://localhost:9000/hbase</value>\n  </property>\n  <property>\n    <name>hbase.zookeeper.property.dataDir</name>\n    <value>/usr/local/hbase/data/zkData</value>\n  </property>\n<property>\n    <name>hbase.cluster.distributed</name>\n    <value>true</value>\n  </property>\n</configuration>\n```\n说明：<br>\n1、很多教程的hbase.rootdir的hdfs的端口都和官网配置一样是8020，这里根据你自己的实际端口号配置，我的默认的为9000（一般都是），如果端口配置错误的话，之后的进程都能启动，但是在hdfs中没有创建hbase文件，也不能通过60010端口访问web UI.<br>\n2、dataDir的目录可以自己定义，不需要预先创建，hbase会根据配置自动生成。\n\n### 3、修改hbase-env.sh\n```markdown\nsudo vim hbase-env.sh\n```\n添加自己的JAVA_HOME路径\n```markdown\nexport JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk/\n```\n### 4、修改regionservers\n在/etc/hosts文件中添加主机名映射，再regionservers中默认的localhost改为主机名\n```mardown\nsudo vim etc/hosts\n```\n在最后一行添加 127.0.0.1 master\n```mardown\nsudo vim regionservers\n```\n将localhost改为mater</br>\n说明：如果ip映射出现问题后面的regionserver会启动不了\n\n### 5、启动服务\n首先先启动hadoop\n```markdown\nstart-all.sh\n```\n再启动hbase\n```markdown\ncd /usr/local/hbase/bin\n./hbase-daemon.sh start zookeeper\n./hbase-daemon.sh start regionserver\n./hbase-daemon.sh start master\n```\n### 6、查看web UI\n在浏览器中输入localhost:60010<br>\n如果能正常显示页面说明配置成功<br>\n说明：刚开启服务后由于hadoop处于安全模式导致不能访问，可以等几十秒再次访问或者通过命令\n```markdown\nhadoop dfsadmin -safemode leave \n```\n解除保护\n## 四、常用的一些命令\n\n### 1、从hdfs导入导出表\n```markdown\n1）导入\n./hbase org.apache.hadoop.hbase.mapreduce.Driver import 表名    数据文件位置\n\n2)导出\n./hbase org.apache.hadoop.hbase.mapreduce.Driver export 表名    数据文件位置\n```\n注意：直接操作会报没有jar包的错误，根据提示将hbase的jar包put进提示的hdfs路径中即可\n## 五、遇到的错误和解决办法\n\n### 1、无法启动HRegionServer和HMaster\n报错日志\n```markdown\n2017-06-13 19:10:12,458 ERROR [main] master.HMasterCommandLine: Master exiting\njava.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster\n  at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:3033)\n  at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:193)\n  at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:135)\n  at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n  at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)\n  at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:3047)\nCaused by: java.net.BindException: 无法指定被请求的地址\n  at sun.nio.ch.Net.bind0(Native Method)\n  at sun.nio.ch.Net.bind(Net.java:463)\n  at sun.nio.ch.Net.bind(Net.java:455)\n  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)\n  at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)\n  at org.apache.hadoop.hbase.ipc.RpcServer.bind(RpcServer.java:2488)\n  at org.apache.hadoop.hbase.ipc.RpcServer$Listener.<init>(RpcServer.java:590)\n  at org.apache.hadoop.hbase.ipc.RpcServer.<init>(RpcServer.java:1956)\n  at org.apache.hadoop.hbase.master.HMaster.<init>(HMaster.java:507)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)\n  at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:3028)\n  ... 5 more\n\n```\n解决办法\n```markdown\n我们可以看到Caused by: java.net.BindException: 无法指定被请求的地址，所以有可能是外网的的影响，所以先关闭网络连接，再启动服务，发现成功了，然后再开启网络。\n```\n### 2、启动hbase服务时找不到pid文件\n问题原因\n```markdown\n1. hbase进行大量的插入时region server 所分配的内存堆过小\n2. pid文件保存在tmp目录下容易丢失。\n```\n解决办法\n```markdown\n1. 在hb的hbase-env.sh中\n# The maximum amount of heap to use, in MB. Default is 1000.\n# export HBASE_HEAPSIZE=1000\n将1000改成30720\n\n2. 在hbase-env.sh中修改pid文件的存放路径：\n在hbase-env.sh中下面的文字默认是注释掉的，放开即可，也可以自己指定存放位置：\n# The directory where pid files are stored. /tmp by default.  \n export HBASE_PID_DIR=/var/hadoop/pids  \n```\n","tags":["hbase"],"categories":["-bigdata"]},{"title":"使用java将文件夹下的文件批量的从gbk编码转化成utf-8编码","url":"/year/08/30/codeparse_gbk2utf/","content":"## 前言\n\n>使用java,对文件遍历，修改文件编码\n\n<!--more-->\n\n### 一、建立java项目，导入[commons-io-*.jar][2]\n### 二、新建class，文件名随便起，我的是Codeparse,包名为exchangecode\n```markdown\npackage exchangecode;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.util.Collection;\n\nimport org.apache.commons.io.FileUtils;\n\npublic class Codeparse {\n\n    public static void main(String[] args) throws IOException {\n        //GBK编码格式源码路径,根据自己的文件路径写 \n        String srcDirPath = \"F:\\\\test\"; \n        //转为UTF-8编码格式源码路径，根据自己的文件路径写 \n        String utf8DirPath =\"F:\\\\out\"; \n                \n        //获取所有txt文件,如果是其他类型的文件，将{“txt”}中的txt换为其他文件的后缀名\n        @SuppressWarnings(\"unchecked\")\n        Collection<File> javaGbkFileCol =  FileUtils.listFiles(new File(srcDirPath), new String[]{\"txt\"}, true); \n                \n        for (File javaGbkFile : javaGbkFileCol) { \n              //UTF8格式文件路径 \n              String utf8FilePath = utf8DirPath+javaGbkFile.getAbsolutePath().substring(srcDirPath.length()); \n              \n              //使用GBK读取数据，然后用UTF-8写入数据 \n              FileUtils.writeLines(new File(utf8FilePath), \"UTF-8\", FileUtils.readLines(javaGbkFile, \"GBK\"));        \n        }\n        System.out.println(\"success!\");\n    }\n\n}\n```\n### 三、运行\n\n[2]: http://mirror.bit.edu.cn/apache//commons/io/binaries/commons-io-2.5-bin.zip \"commons-io-*.jar\"\n","tags":["-java"],"categories":["-others"]},{"title":"ubuntu16.04 anaconda环境下安装tensoflow(GPU)","url":"/year/08/29/tensorflow_step/","content":"## 前言\n>目前深度学习炙手可热的框架毫无疑问是tensorflow,在本教程主要介绍tensorflow在anaconda中的安装，在火车上实在是无聊，电脑又没有网络，只能打发一下时间。\n\n<!--more-->\n\n## 一、版本\n1. anaconda 4.3.21\n2. python 3.5\n3. tensorflow 1.2.0(github上目前最新版本)\n4. ubuntu 16.04\n## 二、下载\n1. [Anaconda3-4.4.0-Linux-x86_64.sh]()\n2. [tensorflow_gpu-1.2.1-cp35-cp35m-linux_x86_64.whl]()\n##三、注意事项\n```markdown\n1、电脑上已经安装了cadu8.0和cudnn5.1环境\n2、tensorflow1.2.0版本支持cadu8.0,其他低版本的tensorflow会发生找不到依赖的错误。\n３、安装后运行会出现CPU computations,cpu指令集优化的警告，目前没有很好的解决办法，不过影响不大，因为我们主要使用的是GPU.\n```\n## 四、安装配置\n\n### 1、安装anaconda\n```markdown\nsudo bash Anaconda3-4.4.0-Linux-x86_64.sh\n```\n\n### 2、　安装python3.5环境\n```markdown\nconda create -n tensorflow python = 3.5\n```\n### 3、安装tensorflow\n```markdown\nsource activate tensorflow #进入刚才安装好的环境\ncd ~/下载　＃进入tensorflow　的pip安装文件的目录\npip install tensorflow_gpu-1.2.1-cp35-cp35m-linux_x86_64.whl #安装tensorflow\n```\n\n## 五、测试\n进入python环境\n```markdown\npython\n```\n运行代码\n```python\nimport tensorflow as tf\nsess = tf.Session()\n\na = tf.constant(10)\nb = tf.constant(20)\n\nprint(sess.run(a+b))\n```\n输出结果\n```markdown\n3\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["tensorflow"],"categories":["-machinelearning"]},{"title":"K-means算法-对31省消费水平分类","url":"/year/08/29/citycosumption/","content":"## 前言：\n>此篇笔记主要根据南京大学礼欣老师的[《Python机器学习应用》](http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce)整理而成，详细内容请看礼欣老师的mooc课程。\n\n<!--more-->\n\n## 数据介绍：\n现有1999年全国31个省份城镇居民家庭平均每人全年消费性支出的八个主\n要变量数据，这八个变量分别是：食品、衣着、家庭设备用品及服务、医疗\n保健、交通和通讯、娱乐教育文化服务、居住以及杂项商品和服务。利用已\n有数据，对31个省份进行聚类。。数据下载[点击我](https://github.com/jChanJi/static_resource/blob/master/clustering/TestData.txt)\n\n## 主要参数\n1. n_clusters：用于指定聚类中心的个数\n2. init：初始聚类中心的初始化方法\n3. max_iter：最大的迭代次数\n4. 一般调用时只用给出n_clusters即可，init\n默认是k-means++，max_iter默认是300\n5. data：加载的数据\n6. label：聚类后各数据所属的标签\n7. axis: 按行求和\n8. fit_predict()：计算簇中心以及为簇分配序号\n\n## 代码\n```python\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\n\ndef loadData(filePath):\n    fr = open(filePath,'r+')\n    lines = fr.readlines()\n    retData = []\n    retCityName = []\n    for line in lines:\n        items = line.strip().split(\",\")\n        retCityName.append(items[0])\n        retData.append([float(items[i]) for i in range(1,len(items))])\n    return retData,retCityName\n\n\nif __name__ == '__main__':\n    data,cityName = loadData('F:/data/clustering/city.txt')\n    km = KMeans(n_clusters=4)\n    label = km.fit_predict(data)\n    expenses = np.sum(km.cluster_centers_,axis=1)\n    #print(expenses)\n    CityCluster = [[],[],[],[]]\n    for i in range(len(cityName)):\n        CityCluster[label[i]].append(cityName[i])\n    for i in range(len(CityCluster)):\n        print(\"Expenses:%.2f\" % expenses[i])\n        print(CityCluster[i])\n```\n\n## 结果\n```markdown\nExpenses:4441.04\n['安徽', '湖南', '湖北', '广西', '海南', '四川', '云南']\nExpenses:7754.66\n['北京', '上海', '广东']\nExpenses:5567.33\n['天津', '江苏', '浙江', '福建', '重庆', '西藏']\nExpenses:3788.76\n['河北', '山西', '内蒙古', '辽宁', '吉林', '黑龙江', '江西', '山东', '河南', '贵州', '陕西', '甘肃', '青海', '宁夏', '新疆']\n\n```\n\n## 注：当改变簇n_clusters为8(CityCluster长度也设置为8)时结果\n```python\nExpenses:3497.85\n['山西', '内蒙古', '黑龙江', '河南', '宁夏']\nExpenses:5311.98\n['天津', '江苏', '重庆', '云南']\nExpenses:7010.02\n['北京', '浙江']\nExpenses:7517.80\n['广东']\nExpenses:4357.67\n['安徽', '湖南', '湖北', '广西', '海南', '四川']\nExpenses:5287.90\n['福建', '西藏']\nExpenses:8247.69\n['上海']\nExpenses:3934.21\n['河北', '辽宁', '吉林', '江西', '山东', '贵州', '陕西', '甘肃', '青海', '新疆']\n\n```\n我们发现簇多所分的层次就越多\n","tags":["K-means"],"categories":["-machinelearning"]},{"title":"CentOS7安装配置hadoop集群","url":"/year/08/29/hadoop/","content":"\n## 基本操作\n### [简单的的vim命令](http://www.cnblogs.com/jeakon/archive/2012/08/13/2816802.html)\n### [linux常用命令](http://www.weixuehao.com/archives/25)\n### [linux命令查找网站](http://man.linuxde.net/)\n\n# CentOS 下安装hadoop<br>\n\n<!--more-->\n\n## 一、安装Vmware 12\n> 1. 官网下载[VMware-Workstation-Full-*.bundle][2]\n> 2. sudo ssh./VMware-Workstation-Full-*.bundle\n> 3. 破解：破解工具[VMware12.Keymaker][3] \n> 4. 根据提示安装\n\n## 二、安装CentOS 7\n> 1. 下载[镜像][4]\n> 2. 新建虚拟机，根据提示操作（注意选择安装GNOME桌面），设置主机名为CentOSMaster点击安装\n> 3. 设置root密码和添加hadoop用户（设置为管理员）\n> 4. 等待安装，完成后重启，连接网络，完成配置 \n> 5. 语言选择汉语（pinyin）\n\n## 三、 安装hadoop集群\n#### 参考教程：\n#### 单机/伪分布式：<http://www.powerxing.com/install-hadoop-in-centos/>\n#### 分布式集群：<http://www.powerxing.com/install-hadoop-cluster/>\n\n>### 1. 创建hadoop用户(如果没有)\n\n```markdown\n1. su                               # 上述提到的以 root 用户登录\n2. useradd -m hadoop -s /bin/bash   # 创建新用户hadoop\n3. passwd hadoop                    #设置密码\n4. visudo                           #增加管理员权限\n```\n找到 root  ALL=(ALL)  ALL 这行,下一行增加:hadoop ensp; ensp; ALL=(ALL)  ensp;ensp; ALL<br>\n\n>### 2. 安装Java环境(在hadoop用户下)\n\n1. 安装openjdk<br>\n```markdown\n    sudo yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel\n```\n如果遇到yum进程被占用，删除yum.pid\n```markdown\n    sudo rm -rf /var/run/yum.pid\n```\n2. 配置JAVA_HOME<br>\n```markdown\n    vim ~/.bashrc\n```\n    在文件最后面添加如下单独一行（指向 JDK 的安装位置)<br>\n```markdown\n    export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk\n```\n3. 使配置生效<br>\n```markdown\n    source ~/.bashrc\n```\n4. 检验是否配置成功<br>\n```markdown\n   echo $JAVA_HOME  #检验变量值\n   java -version \n   %JAVA_HOME/bin/java -version\n```\n如果java -version 和 %JAVA_HOME/bin/java -version一样表示成功,否则看5<br><br>\n5. 如果和以前的jdk版本冲突的:<br>\n    查找当前的安装的jdk版本<br>\n```markdown\n    rpm -q |grep java\n```  \n    删除openjdk版本意外的版本<br>\n```markdown\n    rpm -e --nodeps java版本的名称\n```  \n\n>### 3.安装配置hadoop2集群\n\n1. 下载hadoop压缩包，选择[hadoop-2.x.y.tar.gz][5]文件,这里我选择的是2.6.1版本<br><br>\n2. 解压<br>\n```markdown\n    sudo tar -zxf ~/下载/hadoop-2.6.1.tar.gz -C /usr/local    # 解压到/usr/local中\n    cd /usr/local/  #打开/usr/local目录\n    sudo mv ./hadoop-2.6.1/ ./hadoop  # 将文件夹名改为hadoop\n    sudo chown -R hadoop:hadoop ./hadoop  # 修改文件权限，冒号后没有空格\n```\n4. 显示版本<br>\n```markdown\n    cd  /usr/local/hadoop\n    ./bin/hadoop version\n```\n5. 配置环境变量<br>\n```markdown\n    gedit ~/.bashrc (vim ~/.bashrc)\n```\n    在文件中添加：<br>\n```markdown\n    #Hadoop Environment Variables\n    export HADOOP_HOME=/usr/local/hadoop\n    export HADOOP_INSTALL=$HADOOP_HOME\n    export HADOOP_MAPRED_HOME=$HADOOP_HOME\n    export HADOOP_COMMON_HOME=$HADOOP_HOME\n    export HADOOP_HDFS_HOME=$HADOOP_HOME\n    export YARN_HOME=$HADOOP_HOME\n    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native\n    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\n```\n6. 使配置生效<br>\n```markdown\n    source ~/.bashrc\n    hadoop version #验证\n```\n7. 关闭虚拟机，克隆两个虚拟机，命名为CentOSSlave1,CentOSSlave2,注意要选择完整克隆<br><br>\n8. 依次打开CentOSMaster,CentOSSlave1,CentOSSlave2,查看各自的ip,(ens**下的inet内容)<br>\n```markdown\n    ifconfig\n```   \n9. 修改各自的主机名(Matser，Slave1，Slave2)：<br>\n在master节点上：<br>\n```markdown\n    sudo hostnamectl  set-hostname Master\n```\n在Slave1节点上：<br>\n```markdown\n    sudo hostnamectl  set-hostname Slave1\n```\n在Slave2节点上：<br>\n```markdown\n    sudo hostnamectl  set-hostname Slave2\n```\n10. 修改ip映射（三个节点都要修改）：<br>\n```markdown\n    sudo vim /etc/hosts\n```\n    在末尾加上:<br>\n```markdown\n   ip1   Master\n   ip2   Slave1\n   ip3   Slave2\n```\n11. 设置开机启动网络<br>\n    修改 /etc/sysconfig/network-scripts/ifcfg-ens*（具体文件名每个人有可能不同）,将最后一行的ONBOOT 改为yes    \n```markdown\n    vim  /etc/sysconfig/network-scripts/ifcfg-ens33  #我的文件名称为ifcfg-ens33\n``` \n12. 通过在终端分别执行ping Master，ping Slave1，ping Slave2,看是否能通，ctrl+C停止<br><br>\n13. 主节点Master使用ssh无密钥登陆节点（注意ssh登陆的用户名）<br><br>\n    a. 首先生成 Master 节点的公匙，在 Master 节点的终端中执行：<br>\n```markdown\n       su hadoop               #登陆到hadoop用户,所有操作都使hadoop用户的行为\n       cd ~/.ssh               # 如果没有该目录，先执行一次ssh Master\n       rm ./id_rsa*            # 删除之前生成的公匙（如果有）\n       ssh-keygen -t rsa       # 一直按回车就可以\n```\n    b. 让Master节点需能无密码ssh本机，在 Master 节点上执行：<br>\n```markdown\n       cat ./id_rsa.pub >> ./authorized_keys\n       chmod 600 ./authorized_keys    # 修改文件权限\n```\n      完成后可执行 ssh Master 验证一下（可能需要输入 yes，成功后执行 exit 返回原来的终端）。<br><br>\n    c. 将上公匙传输到 Slave1 节点(Slave2也是一样操作将Slave1改成Slave2):<br>\n```markdown\n       scp ~/.ssh/id_rsa.pub hadoop@Slave1:/home/hadoop/<br><br>\n``` \n    d. 在Slave1和Slave2节点上 操作：<br>\n```markdown\n       mkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略\n       cat ~/id_rsa.pub >> ~/.ssh/authorized_keys\n       rm ~/id_rsa.pub    # 用完就可以删掉了\n````\n    e. 在Master节点上ssh Slave1和Slave2，验证是否能连接上<br><br>\n```markdown\n    ssh Slave1\n    \n    ssh Slave2\n```\n14. 在Master节点上操作，cd /usr/local/hadoop/etc/hadoop,进入root模式<br>\n    a. 修改slaves文件,将localhost注释，添加Slave1,换行，Slave2<br><br>\n```markdown\n    vim slaves\n```\n    b. 修改core-site.xml<br>\n    ```markdown\n    <configuration>\n           <property>\n                <name>fs.defaultFS</name>\n                <value>hdfs://Master:9000</value>\n            </property>\n            <property>\n                <name>hadoop.tmp.dir</name>\n                <value>file:/usr/local/hadoop/tmp</value>\n                <description>tmp directories</description>\n            </property>\n    </configuration>\n    ```\n    c. 修改hdfs-site.xml,其中的dfs.replication的value根据Slave的个数填写<br>\n    ```html\n     <configuration>\n            <property>\n                    <name>dfs.namenode.secondary.http-address</name>\n                    <value>Master:50090</value>\n            </property>\n            <property>\n                    <name>dfs.replication</name>\n                    <value>2</value>\n            </property>\n            <property>\n                    <name>dfs.namenode.name.dir</name>\n                    <value>file:/usr/local/hadoop/tmp/dfs/name</value>\n            </property>\n            <property>\n                    <name>dfs.datanode.data.dir</name>\n                    <value>file:/usr/local/hadoop/tmp/dfs/data</value>\n            </property>\n      </configuration>\n    ````\n    d. 重命名 mapred-site.xml.template为mapred-site.xml,并修改mapred-site.xml为：<br>\n    ```html\n    <configuration>\n            <property>\n                    <name>mapreduce.framework.name</name>\n                    <value>yarn</value>\n            </property>\n            <property>\n                    <name>mapreduce.jobhistory.address</name>\n                    <value>Master:10020</value>\n            </property>\n            <property>\n                    <name>mapreduce.jobhistory.webapp.address</name>\n                    <value>Master:19888</value>\n            </property>\n    </configuration>\n    ````\n    e. 修改yarn.site.xml为：<br>\n    ```html\n    <configuration>\n            <property>\n                    <name>yarn.resourcemanager.hostname</name>\n                    <value>Master</value>\n            </property>\n            <property>\n                    <name>yarn.nodemanager.aux-services</name>\n                    <value>mapreduce_shuffle</value>\n            </property>\n    </configuration>\n    ```\n15. 配置好后,将Master上的/usr/local/hadoop文件夹复制到各个节点上。如果有临时文件和日志文件先删除,在Master节点上执行:<br>\n```markdown\n    cd /usr/local\n    sudo rm -r ./hadoop/tmp                    # 删除 Hadoop 临时文件\n    sudo rm -r ./hadoop/logs/*                 # 删除日志文件\n    tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制\n    cd ~\n    scp ./hadoop.master.tar.gz Slave1:/home/hadoop\n```\n    如果有其他节点再执行：scp ./hadoop.master.tar.gz Slave(n):/home/hadoop<br><br>\n16. 分别在slave节点上执行<br>\n```markdown\n    sudo rm -r /usr/local/hadoop    # 删掉旧的（如果存在）\n    sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/local\n    sudo chown -R hadoop /usr/local/hadoop   #给hadoop用户读写/usr/local/hadoop的权限\n```\n17. 首次启动需要先在 Master 节点执行 NameNode 的格式化：<br>\n```markdown\n    hdfs namenode -format       # 首次运行需要执行初始化，之后不需要，status=0，表示成功\n```\n18. 关闭防火墙(所有机器)：<br>\n```markdown\n    systemctl stop firewalld.service    # 关闭firewall<\n    systemctl disable firewalld.service # 禁止firewall开机启动\n```\n19. 启动服务<br>\n```markdown\n   start-yarn.sh\n   start-dfs.sh\n   mr-jobhistory-daemon.sh start historyserver\n```\n20. 在master节点上查看java进程<br>\n```markdown\n    jps\n```\n    如果有JobHistoryServer,SecondaryNameNode,Jsp,ResourceManager,NameNode四个进程代表Master上没问题<br><br>\n21. 在slave节点上执行<br>\n```markdown\n    jps\n```\n    如果有Jps，DataNode,NodeManager,三个节点表示配置成功<br><br>\n22. 关闭服务<br>\n```markdown\n    stop-yarn.sh\n    stop-dfs.sh\n    mr-jobhistory-daemon.sh stop historyserver\n```\n\n\n[2]: http://www.vmware.com/cn/products/workstation/workstation-evaluation.html \"Vmware 下载地址\"\n[3]: http://chanji-1252400803.costj.myqcloud.com/VMware12.Keymaker.exe \"VMware12.Keymaker\"\n[4]: http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso \"CentOS 7\"\n[5]: http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.1/hadoop-2.6.1.tar.gz \"hadoop\"","tags":["hadoop"],"categories":["-bigdata"]},{"title":"Hello World","url":"/year/08/29/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n<!--more-->\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/deployment.html)\n"}]