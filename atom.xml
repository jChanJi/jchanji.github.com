<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chanji</title>
  <subtitle>Stay Hungry,Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jchanji.github.io/"/>
  <updated>2017-11-23T01:04:58.987Z</updated>
  <id>http://jchanji.github.io/</id>
  
  <author>
    <name>Chanji</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>知识图谱</title>
    <link href="http://jchanji.github.io/year/11/19/knowledgegraph/"/>
    <id>http://jchanji.github.io/year/11/19/knowledgegraph/</id>
    <published>2017-11-19T14:52:15.921Z</published>
    <updated>2017-11-23T01:04:58.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;知识图谱于2012年5月17日被Google正式提出， 其初衷是为了提高搜索引擎的能力，增强用户的搜索质量以及搜索体验。，RDF (resource description framework)<sup>[1]</sup>模式(RDF schema) （应用）和万维网本体语言(Web ontology language，OWL) 的形式化模型就是基于上述目的产生的。</p>
</blockquote>
<a id="more"></a>
<blockquote>
<p>[1]RDF: RDF是一个处理元数据的XML,RDF使用XML语法和RDF Schema（RDFS）来将元数据描述成为数据模型。是描述语义层面的本体关系的语言。</p>
<p>[2]数据模型: 数据模型（Data Model）是数据特征的抽象。数据（Data）是描述事物的符号记录，模型（Model)是现实世界的抽象。数据模型所描述的内容有三部分：数据结构、数据操作和数据约束。</p>
</blockquote>
<h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;知识图谱是Google用于增强其搜索引擎功能的知识库。本质上,知识图谱是一种揭示实体之间关系的 <strong>语义网络（semantic network）</strong> ,即具 有有向图结构的一个知识库，其中图的结点代 表实体（entity）或者概念（concept），而图的 边代表实体/ 概念之间的各种语义关系，可以对现实世界的事物及其相互关系进行形式化地描述。现在的知识图谱已被用来泛指各种大规模的知识库。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/knowledgegraph2.png" alt="知识图谱"></p>
<h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;广泛应用于智能搜索、 智能问答、个性化推荐等领域。尤其是在智能搜索中，用户的搜索请求不再局限于简单的关键词匹配，搜索将根据用户查询的情境与意图进行推理，实现 概念检索。与此同时，用户的搜索结果将具有层次 化、结构化等重要特征。</p>
</blockquote>
<h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;<strong>知识图谱也可分为通用知识图谱(开放链接知识库)和行业知识图谱(垂直行业知识库)</strong>。</p>
</blockquote>
<h3 id="通用知识图谱"><a href="#通用知识图谱" class="headerlink" title="通用知识图谱"></a>通用知识图谱</h3><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;通用知识图谱注重广度，强调融合更多的实体，较行业知识图谱而言,其准确度不够高,并且受概念范围的影响,很难借助本体库对公理、规则以及约束条件的支持能力规范其实 体、属性、实体间的关系等。通用知识图谱主要应 用于智能搜索等领域。行业知识图谱通常需要依靠 特定行业的数据来构建，具有特定的行业意义。</p>
</blockquote>
<h3 id="行业知识图谱"><a href="#行业知识图谱" class="headerlink" title="行业知识图谱"></a>行业知识图谱</h3><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;行业知识图谱中，实体的属性与数据模式往往比较丰富，需要考虑到不同的业务场景与使用人员。</p>
</blockquote>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h3><blockquote>
<p> &nbsp; &nbsp; &nbsp; &nbsp;知识图谱在逻辑上可分为 <strong>模式层</strong> 与 <strong>数据层</strong> 两个层次.</p>
</blockquote>
<h4 id="1、数据层"><a href="#1、数据层" class="headerlink" title="1、数据层"></a>1、数据层</h4><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用(实体1，关系， 实体2)、(实体、属性，属性值)这样的三元组来表达事实,可选择 <strong>图数据库</strong> 作为存储介质。</p>
</blockquote>
<h4 id="2、模式层"><a href="#2、模式层" class="headerlink" title="2、模式层"></a>2、模式层</h4><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;模式层构建在数据层之上，主要是通过 <strong>本体库</strong> 来规范数据层的一系列事实表达。 <strong>本体是结构化知识库的概念模板</strong> ，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</p>
</blockquote>
<h3 id="体系结构（构建模式）"><a href="#体系结构（构建模式）" class="headerlink" title="体系结构（构建模式）"></a>体系结构（构建模式）</h3><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;知识图谱的体系架构知识图谱的体系架构是其指构建模式结构，如图1所示。其中虚线框内的部分为知识图谱的构建过 程，该过程需要随人的认知能力不断更新迭代。知识图谱主要有 <strong>自顶向下(top-down)</strong> 与 <strong>自底向上(bottom-up)</strong> 两种构建方式。</p>
</blockquote>
<h4 id="自顶向上"><a href="#自顶向上" class="headerlink" title="自顶向上"></a>自顶向上</h4><blockquote>
<p>自顶向下指的是: <strong>先为知识图谱定义好本体与数据模式，再将实体加入到知识库</strong>。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如Freebase项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。</p>
</blockquote>
<h4 id="自底向上"><a href="#自底向上" class="headerlink" title="自底向上"></a>自底向上</h4><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;自底向上指的是 <strong>从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式</strong> 。目前，大多数知识图谱都采用自底向上的方式进行构建，其中典型就是 Google的Knowledge Vault。</p>
</blockquote>
<p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/entity1.PNG" alt="图1"></p>
<h2 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h2><h3 id="知识抽取"><a href="#知识抽取" class="headerlink" title="知识抽取"></a>知识抽取</h3><h4 id="知识抽取-1"><a href="#知识抽取-1" class="headerlink" title="知识抽取"></a>知识抽取</h4><h5 id="基于规则与词典的实体抽取方法"><a href="#基于规则与词典的实体抽取方法" class="headerlink" title="基于规则与词典的实体抽取方法"></a>基于规则与词典的实体抽取方法</h5><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;早期的实体抽取是在限定文本领域、限定语义 单元类型的条件下进行的，主要采用的是基于规则与词典的方法， <strong>例如使用已定义的规则，抽取出文本中的人名、地名、组织机构名、特定时间等实体。</strong></p>
</blockquote>
<h5 id="基于统计机器学习的实体抽取方法"><a href="#基于统计机器学习的实体抽取方法" class="headerlink" title="基于统计机器学习的实体抽取方法"></a>基于统计机器学习的实体抽取方法</h5><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;随后，研究者尝试将机器学习中的 <strong>监督学习</strong> 算法用于命名实体的抽取问题上。单纯的监督学习算法在性能上 不仅受到训练集合的限制，并且算法的准确率与召回率都不够理想。相关研究者认识到监督学习算法的制约性后，尝试将监督学习算法与规则相互结合。</p>
</blockquote>
<h5 id="面向开放域的实体抽取方法"><a href="#面向开放域的实体抽取方法" class="headerlink" title="面向开放域的实体抽取方法"></a>面向开放域的实体抽取方法</h5><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;其基本思想是通过 <strong>少量的实体实例建立特征模型，再通过该模型应用于新的数据集得到新的命名实体</strong>。基于 <strong>无监督学习</strong> 的开放域聚类算法，其基本思想是基于已知实体的语义特征去搜索日志中识别出命名的实体，然后进行聚类。</p>
</blockquote>
<h4 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h4><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;关系抽取的目标是解决实体间语义链接的问题。主要有效的方法是基于马尔可夫逻辑网和基于本体推理的深层隐含关系抽取方法，主要有一下俩个分类。</p>
</blockquote>
<h5 id="开放式实体关系抽取"><a href="#开放式实体关系抽取" class="headerlink" title="开放式实体关系抽取"></a>开放式实体关系抽取</h5><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;开放式实体关系抽取可分为二元开放式关系抽 取和n元开放式关系抽取。</p>
</blockquote>
<h5 id="基于联合推理的实体关系抽取"><a href="#基于联合推理的实体关系抽取" class="headerlink" title="基于联合推理的实体关系抽取"></a>基于联合推理的实体关系抽取</h5><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;联合推理的关系抽取中的典型方法是马尔可夫逻辑网MLN(Markov logic network)<sup>[1]</sup>。</p>
<p>[1] 马尔可夫逻辑网:</p>
</blockquote>
<h3 id="属性抽取"><a href="#属性抽取" class="headerlink" title="属性抽取"></a>属性抽取</h3><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;属性抽取主要是针对实体而言的，通过属性可形成对实体的完整勾画。由于实体的属性可以看成是 <strong>实体与属性值之间的一种名称性关系</strong> ，因此可以将实体属性的抽取问题转换为关系抽取问题。</p>
<p>&nbsp; &nbsp; &nbsp; &nbsp;大量的属性数据主要存在于半结构化、非结构化的大规模开放域数据集中。抽取这些属性的方法，一种是将上述从百科网站上抽取的 <strong>结构化数据作为可用于属性抽取的训练集，然后再将该模型应用于开放域中的实体属性抽取</strong> 。另一种是 <strong>根据实体属性与属性值之间的关系模式，直接从开放域数据集上抽取属性。</strong></p>
</blockquote>
<h1 id="本体"><a href="#本体" class="headerlink" title="本体"></a>本体</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><blockquote>
<p> &nbsp; &nbsp; &nbsp; &nbsp;Gruber给出了Ontology的一个最为流行的定义,即“Ontology是概念模型<sup>[1]</sup> 的明确的规范说明”。</p>
<p>[1] 概念模型: “概念模型” 指通过抽象出客观世界中一些现象的相关概念而得到的模型。首先把现实世界中的客观对象抽象为某一种信息结构，这种信息结构并不依赖于具体的计算机系统，不是某一个数据库管理系统（DBMS）支持的 <strong>数据模型</strong> ，而是概念级的模型，称为概念模型。<br></p>
<p>[2] 数据模型: 数据模型（Data Model）是数据特征的抽象。数据是描述事物的符号记录，模型是现实世界的抽象。数据模型为数据库系统的信息表示与操作提供了一个抽象的框架。数据模型所描述的内容有三部分：数据结构、数据操作和数据约束。</p>
</blockquote>
<h2 id="举例解释"><a href="#举例解释" class="headerlink" title="举例解释"></a>举例解释</h2><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;上面的概念很抽象，不是很好理解，其实本体的概念有两层意思，一层是哲学层面的意思，一层是引申到信息科学中的语义层面的意思。</p>
</blockquote>
<h3 id="哲学上的本体"><a href="#哲学上的本体" class="headerlink" title="哲学上的本体"></a>哲学上的本体</h3><blockquote>
<p>“鼠标”，“mouse”,</p>
</blockquote>
<p><img src="./img/entity1.PNG" alt="符号"></p>
<blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;等都是表示”鼠标”这个本体的的”符号”。由此可见“本体”是只可意会不可言传的，因为所有的描述都成为了“本体”的外在符号，我们世界上的所有图像、语言、我们看到的、听到的、感受到的，都成为符号到本体的某种映射。</p>
</blockquote>
<h3 id="信息科学中的本体"><a href="#信息科学中的本体" class="headerlink" title="信息科学中的本体"></a>信息科学中的本体</h3><blockquote>
<p>&nbsp; &nbsp; &nbsp; &nbsp;Ontology是一种 <strong>描述术语</strong> （包含哪些词汇）及 <strong>术语间关系</strong> （描述苹果、香蕉、水果之间的关系）的概念模型。Ontology的形式可简单可复杂。最简单的词汇表（只定义术语集合，不定义术语之间的关系）也可以看成是一种“本体”；但严格意义上的本体，是既定义了术语、也定义了术语之间关系的。生活中，最常见、最成熟的本体，就属图书馆里的图书分类法。本体，以图书分类法为例，一方面限定了术语集合（即规定大家必须采用共同承认的一套词汇，禁止私自发明新词），另一方面定义术语之间的上下位关系（如：计算机技术隶属于工业技术，软件技术隶属于计算机技术，等等）。只要大家都认同该本体，并在实践中长期遵守该本体，依照它来编排和索引书目，那么日后寻找一本书就会非常方便。</p>
</blockquote>
<p>引用<br><br>[1] 徐增林, 盛泳潘, 贺丽荣,等. 知识图谱技术综述[J]. 电子科技大学学报, 2016, 45(4):589-606.<br><br>[2] 漆桂林, 高桓, 吴天星. 知识图谱研究进展[J]. 情报工程, 2017, 3(1):4-25.<br><br>[3] 邓志鸿, 唐世渭, 张铭,等. Ontology研究综述[J]. 北京大学学报(自然科学版), 2002, 38(5):730-738.<br><br>[4]李国洪, 梁保城, 赵毅,等. Ontology研究的知识图谱演化[J]. 情报杂志, 2013(3):101-105.<br><br>[5] Gruber T R. A Translation Approach to Portable Ontology Specifications. Knowledge Acquisition ,1993 ,5 :199～220</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;知识图谱&quot;&gt;&lt;a href=&quot;#知识图谱&quot; class=&quot;headerlink&quot; title=&quot;知识图谱&quot;&gt;&lt;/a&gt;知识图谱&lt;/h1&gt;&lt;h2 id=&quot;起源&quot;&gt;&lt;a href=&quot;#起源&quot; class=&quot;headerlink&quot; title=&quot;起源&quot;&gt;&lt;/a&gt;起源&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;知识图谱于2012年5月17日被Google正式提出， 其初衷是为了提高搜索引擎的能力，增强用户的搜索质量以及搜索体验。，RDF (resource description framework)&lt;sup&gt;[1]&lt;/sup&gt;模式(RDF schema) （应用）和万维网本体语言(Web ontology language，OWL) 的形式化模型就是基于上述目的产生的。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-arcticals" scheme="http://jchanji.github.io/categories/arcticals/"/>
    
    
  </entry>
  
  <entry>
    <title>Arabesque:A System for Distributed Graph Mining</title>
    <link href="http://jchanji.github.io/year/09/03/artical_1/"/>
    <id>http://jchanji.github.io/year/09/03/artical_1/</id>
    <published>2017-09-03T07:15:57.580Z</published>
    <updated>2017-09-03T07:23:04.953Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Arabesque-A-System-for-Distributed-Graph-Mining"><a href="#Arabesque-A-System-for-Distributed-Graph-Mining" class="headerlink" title="Arabesque: A System for Distributed Graph Mining"></a>Arabesque: A System for Distributed Graph Mining</h2><blockquote>
<h3 id="Arabesque-分布式的图挖掘系统"><a href="#Arabesque-分布式的图挖掘系统" class="headerlink" title="Arabesque:分布式的图挖掘系统"></a>Arabesque:分布式的图挖掘系统</h3></blockquote>
<h4 id="原文链接：点我跳转"><a href="#原文链接：点我跳转" class="headerlink" title="原文链接：点我跳转"></a>原文链接：<a href="https://github.com/jChanJi/jchanji.github.com/blob/master/meterial/093-teixeira.pdf" target="_blank" rel="external">点我跳转</a></h4><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><p><a href="#Abstract">Abstract</a></p>
</li>
<li><p>1.<a href="#introduction">Introduction</a></p>
</li>
</ul>
<a id="more"></a>
<ul>
<li><p>2.Graph Mining Problems</p>
</li>
<li><p>3.The Filter-Process Model</p>
<ul>
<li><p>3.1.Computational Model</p>
</li>
<li><p>3.2.Alternative Paradigms: Think Like a Vertex and Think Like a Pattern</p>
</li>
</ul>
</li>
<li><p>4.Arabesque: API, Programming, and Implementation</p>
<ul>
<li><p>4.1 Arabesque API</p>
</li>
<li><p>4.2 Programming with Arabesque</p>
</li>
<li><p>4.3 Arabesque implementation</p>
</li>
</ul>
</li>
<li><p>5.Graph Exploration Techniques</p>
<ul>
<li><p>5.1 Coordination-Free Exploration Strategy</p>
</li>
<li><p>5.2 Storing Embeddings Compactly</p>
</li>
<li><p>5.3 Partitioning Embeddings for Load Balancing</p>
</li>
<li><p>5.4 Two-Level Pattern Aggregation for Fast Pattern Canonicality Checking</p>
</li>
</ul>
</li>
<li><p>6.Evaluation</p>
<ul>
<li><p>6.1 Experimental Setup</p>
</li>
<li><p>6.2 Alternative Paradigms: TLV and TLP</p>
</li>
<li><p>6.3 Arabesque: The TLE Paradigm</p>
</li>
<li><p>6.4 Large Graphs with Arabesque</p>
</li>
</ul>
</li>
<li><p>7.Related Work</p>
</li>
<li><p>8.Conclusions</p>
</li>
<li><p>9.Acknowledgments</p>
</li>
<li><p>10.References</p>
</li>
</ul>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><span id="Abstract">Abstract</span></h3><h4 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h4><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><p>  Distributed data processing platforms such as MapReduce<br>and Pregel have substantially simplified the design and deployment<br>of certain classes of distributed graph analytics algorithms.<br>However, these platforms do not represent a good<br>match for distributed graph mining problems, as for example<br>finding frequent subgraphs in a graph. Given an input<br>graph, these problems require exploring a very large number<br>of subgraphs and finding patterns that match some “interestingness”<br>criteria desired by the user. These algorithms are<br>very important for areas such as social networks, semantic<br>web, and bioinformatics.</p>
<p>  In this paper, we present Arabesque, the first distributed<br>data processing platform for implementing graph mining<br>algorithms. Arabesque automates the process of exploring<br>a very large number of subgraphs. It defines a high-level<br>filter-process computational model that simplifies the development<br>of scalable graph mining algorithms: Arabesque explores<br>subgraphs and passes them to the application, which<br>must simply compute outputs and decide whether the subgraph<br>should be further extended. We use Arabesque’s API<br>to produce distributed solutions to three fundamental graph<br>mining problems: frequent subgraph mining, counting motifs,<br>and finding cliques. Our implementations require a<br>handful of lines of code, scale to trillions of subgraphs, and<br>represent in some cases the first available distributed solutions.</p>
<h4 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h4><p>  Distributed data processing platforms such as MapReduce<br>and Pregel have substantially simpliﬁed the design and deployment of certain classes of distributed graph analyticsal gorithms.</p>
<blockquote>
<p>分布式数据处理平台例如mapreduce和pregel实质上是简化了某些类的分布式图形化分析算法的设计和调度</p>
</blockquote>
<p> However, these platforms do not represent a good<br>match for distributed graph mining problems, as for example<br>finding frequent subgraphs in a graph.</p>
<blockquote>
<p>但是，这些平台没有表现出对分布式图形挖掘问题的匹配。就以在图表中频繁的寻找子图作为例子</p>
</blockquote>
<p>Given an input graph, these problems require exploring a very large number of subgraphs and finding patterns that match some “interestingness” criteria desired by the user. </p>
<blockquote>
<p>给出一个输入图表，这些问题需要扫描（探索）一个数量很多的子图并且寻找和用户期望的一些”兴趣性”准则相匹配的模式（图案,样品）。</p>
</blockquote>
<p>These algorithms are very important for areas such as social networks, semantic web, and bioinformatics.</p>
<blockquote>
<p>这些算法对例如社交网络，语义网，和分析复杂生物的学科的领域非常重要。</p>
</blockquote>
<p>In this paper, we present Arabesque, the first distributed<br>data processing platform for implementing graph mining<br>algorithms.<br>Arabesque automates the process of exploring a very large number of subgraphs. </p>
<blockquote>
<p>在这篇文献当中，我们介绍Arabesque,第一个实现图挖掘算法的分布式数据处理平台。Arabesque自动化了探索一个很大数量的子图的流程。</p>
</blockquote>
<p>It defines a high-level filter-process computational model that simplifies the development of scalable graph mining algorithms: Arabesque explores subgraphs and passes them to the application, which must simply compute outputs and decide whether the subgraph should be further extended. </p>
<blockquote>
<p>他定义了一个高级的过滤过程的计算模型，它简化了可升级的图挖掘算法的开发:Arabesque 探索子图并且将他们传递给应用程序，这个应用程序必须简单的计算输出和决定是否子图应该被进一步的被扩展。</p>
</blockquote>
<p>We use Arabesque’s API to produce distributed solutions to three fundamental graph mining problems: frequent subgraph mining, counting motifs,and finding cliques. </p>
<blockquote>
<p>我们用Arabesque的API去产生三个基础的图挖掘问题的分布式解决方案:频繁的子图挖掘，计数的图案，寻找派系。</p>
</blockquote>
<p>Our implementations require a handful of lines of code, scale to trillions of subgraphs, and represent in some cases the first available distributed solutions.</p>
<blockquote>
<p>我们的实现需要很少行的代码，规模数万亿的子图，和在某些情况下第一个可获得的分布式解决方案的示范</p>
</blockquote>
<h4 id="段落翻译"><a href="#段落翻译" class="headerlink" title="段落翻译"></a>段落翻译</h4><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>分布式数据处理平台例如mapreduce和pregel实质上是简化了某些类的分布式图形化分析算法的设计和调度.但是，这些平台没有表现出对分布式图形挖掘问题的匹配。就以在图表中频繁的寻找子图作为例子.给出一个输入图表，这些问题需要扫描（探索）一个数量很多的子图并且寻找和用户期望的一些”兴趣性”准则相匹配的模式（图案,样品）。这些算法对例如社交网络，语义网，和分析复杂生物的学科的领域非常重要。</p>
<p>在这篇文献当中，我们介绍Arabesque,第一个实现图挖掘算法的分布式数据处理平台。Arabesque自动化了探索一个很大数量的子图的流程。他定义了一个高级的过滤过程的计算模型，它简化了可升级的图挖掘算法的开发:Arabesque 探索子图并且将他们传递给应用程序，这个应用程序必须简单的计算输出和决定是否子图应该被进一步的被扩展.我们用Arabesque的API去产生三个基础的图挖掘问题的分布式解决方案:频繁的子图挖掘，计数的图案，寻找派系。我们的实现需要很少行的代码，规模数万亿的子图，和在某些情况下第一个可获得的分布式解决方案的示范</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><span id="Introduction">Introduction</span></h3><h3 id="原文-1"><a href="#原文-1" class="headerlink" title="原文"></a>原文</h3><p>Graph data is ubiquitous in many fields, from the Web to advertising<br>and biology, and the analysis of graphs is becoming<br>increasingly important. The development of algorithms<br>for graph analytics has spawned a large amount of research,<br>especially in recent years. However, graph analytics has traditionally<br>been a challenging problem tackled by expert researchers,<br>who can either design new specialized algorithms<br>for the problem at hand, or pick an appropriate and sound<br>solution from a very vast literature. When the input graph or<br>the intermediate state or computation complexity becomes<br>very large, scalability is an additional challenge.</p>
<p>The development of graph processing systems such as<br>Pregel [25] has changed this scenario and made it simpler to<br>design scalable graph analytics algorithms. Pregel offers a<br>simple “think like a vertex” (TLV) programming paradigm,<br>where each vertex of the input graph is a processing element<br>holding local state and communicating with its neighbors<br>in the graph. TLV is a perfect match for problems that<br>can be represented through linear algebra, where the graph<br>is modeled as an adjacency matrix (or some other variant<br>like the Laplacian matrix) and the current state of each vertex<br>is represented as a vector. We call this class of methods<br>graph computation problems. A good example is computing<br>PageRank [6], which is based on iterative sparse matrix<br>and vector multiplication operations. TLV covers several<br>other algorithms that require a similar computational architecture,<br>for example, shortest path algorithms, and over the<br>years many optimizations of this paradigm have been proposed<br>[17, 26, 36, 42].</p>
<p>Despite this progress, there remains an important class<br>of algorithms that cannot be readily formulated using the<br>TLV paradigm. These are graph mining algorithms used<br>to discover relevant patterns that comprise both structurebased<br>and label-based properties of the graph. Graph mining<br>is widely used for several applications, for example, discovering<br>3D motifs in protein structures or chemical compounds,<br>extracting network motifs or significant subgraphs<br>from protein-protein or gene interaction networks, mining<br>attributed patterns over semantic data (e.g., in Resource<br>Description Framework or RDF format), finding structurecontent<br>relationships in social media data, dense subgraph mining for community and link spam detection in web data,among others. Graph mining algorithms typically take a labeled and immutable graph as input, and mine patterns<br>that have some algorithm-specific property (e.g., frequency<br>above some threshold) by finding all instances of these patterns<br>in the input graph. Some algorithms also compute aggregated<br>metrics based on these subgraphs</p>
<p><img src="./img/1.PNG" alt="图1"></p>
<p>Figure 1: Exponential growth of the intermediate state in<br>graph mining problems (motifs counting, clique finding,<br>FSM: Frequent subgraph mining) on different datasets.</p>
<p>Designing graph mining algorithms is a challenging and<br>active area of research. In particular, scaling graph mining<br>algorithms to even moderately large graphs is hard. The set<br>of possible patterns and their subgraphs in a graph can be<br>exponential in the size of the original graph, resulting in an<br>explosion of the computation and intermediate state. Figure<br>1 shows the exponential growth of the number of “interesting”<br>subgraphs of different sizes in some of the graph<br>mining problems and datasets we will evaluate in this paper.<br>Even graphs with few thousands of edges can quickly generate<br>hundreds of millions of interesting subgraphs. The need<br>for enumerating a large number of subgraphs characterizes<br>graph mining problems and distinguishes them from graph<br>computation problems. Despite this state explosion problem,<br>most graph mining algorithms are centralized because of the<br>complexity of distributed solutions.</p>
<p>In this paper, we propose automatic subgraph exploration<br>as a generic building block for solving graph mining<br>problems, and introduce Arabesque, the first embedding exploration<br>system specifically designed for distributed graph<br>mining. Conceptually, we move from TLV to “think like an<br>embedding” (TLE), where by embedding we denote a subgraph<br>representing a particular instance of a more general<br>template subgraph called a pattern (see Figure 2).</p>
<p>Arabesque defines a high-level filter-process computational<br>model. Given an input graph, the system takes care<br>of automatically and systematically visiting all the embeddings<br>that need to be explored by the user-defined algorithm,<br>performing this exploration in a distributed manner. The system<br>passes all the embeddings it explores to the application,<br>which consists primarily of two functions: filter, which indicates whether an embedding should be processed, and process,<br>which examines an embedding and may produce some<br>output. For example, in the case of finding cliques the filter<br>function prunes embeddings that are not cliques, since none<br>of their extensions can be cliques, and the process function<br>outputs all explored embeddings, which are cliques by construction.<br>Arabesque also supports the pruning of the exploration<br>space based on user-defined metrics aggregated across<br>multiple embeddings.</p>
<p><img src="./img/2.PNG" alt="图2"></p>
<p>Figure 2: Graph mining concepts: an input graph, an example<br>pattern, and the embeddings of the pattern. Colors represent<br>labels. Numbers denote vertex ids. Patterns and embeddings<br>are two types of subgraphs. However, a pattern is<br>a template, whereas an embedding is an instance. In this example,<br>the two embeddings are automorphic.</p>
<p>The Arabesque API simplifies and thus democratizes the<br>design of graph mining algorithms, and automates their execution<br>in a distributed setting. We used Arabesque to implement<br>and evaluate scalable solutions to three fundamental<br>and diverse graph mining problems: frequent subgraph mining,<br>counting motifs, and finding cliques. These problems<br>are defined precisely in Section 2. Some of these algorithms<br>are the first distributed solutions available in the literature,<br>which shows the simplicity and generality of Arabesque.</p>
<p>Arabesque’s embedding-centered API facilitates a highly<br>scalable implementation. The system scales by spreading<br>embeddings uniformly across workers, thus avoiding<br>hotspots. By making it explicit that embeddings are the fundamental<br>unit of exploration, Arabesque is able to use fast<br>coordination-free techniques, based on the notion of embedding<br>canonicality, to avoid redundant work and minimize<br>communication costs. It also enables us to store embeddings<br>efficiently using a new data structure called Overapproximating<br>Directed Acyclic Graph (ODAG), and to devise a<br>new two-level optimization for pattern-based aggregation,<br>which is a common operation in graph mining algorithms.</p>
<p>Arabesque is implemented as a layer on top of Apache<br>Giraph [3], a Pregel-inspired graph computation system,<br>thus allowing both graph computation and graph mining<br>algorithms to run on top of the same infrastructure. The<br>implementation does not use a TLV approach: it considers<br>Giraph just as a regular data parallel system implementing<br>the Bulk Synchronous Processing model.<br>To summarize, we make the following contributions:</p>
<p>• We propose embedding exploration, or “think like an embedding”,<br>as an effective basic building block for graph<br>mining. We introduce the filter-process computational<br>model (Section 3), design an API that enables embedding<br>exploration to be expressed effectively and succinctly,<br>and present three example graph mining applications<br>that can be elegantly expressed using the Arabesque<br>API (Section 4).</p>
<p>• We introduce techniques to make distributed embedding<br>exploration scalable: coordination-free work sharing, ef-<br>ficient storage of embeddings, and an important optimization<br>for pattern-based aggregation (Section 5).</p>
<p>• We demonstrate the scalability of Arabesque on various<br>graphs. We show that Arabesque scales to hundreds of<br>cores over a cluster, obtaining orders of magnitude reduction<br>of running time over the centralized baselines (Section<br>6), and can analyze trillions of embeddings on large<br>graphs.</p>
<p>The Arabesque system, together with all applications<br>used for this paper, is publicly available at the project’s website:<br>www.arabesque.io.</p>
<h4 id="翻译-1"><a href="#翻译-1" class="headerlink" title="翻译"></a>翻译</h4><p>Graph data is ubiquitous in many fields, from the Web to advertising and biology, and the analysis of graphs is becoming increasingly important. </p>
<blockquote>
<p>图形数据在许多领域普遍存在，从网站到广告业和生物学，并且分析图形正在变得越来越重要。</p>
</blockquote>
<p>The development of algorithms for graph analytics has spawned a large amount of research, especially in recent years.</p>
<blockquote>
<p>图形分析算法的发展催生了大量的研究，尤其是在近些年来。</p>
</blockquote>
<p>However, graph analytics has traditionally been a challenging problem tackled by expert researchers, who can either design new specialized algorithms for the problem at hand, or pick an appropriate and sound solution from a very vast literature. </p>
<blockquote>
<p>但是，图形分析历年来是具有挑战性的，由那些能够为了手上的问题设计新的专门的算法或者从非常庞大的文献中选择一个适当并且健全的解决方案的专家去解决。</p>
</blockquote>
<p>When the input graph or the intermediate state or computation complexity becomes very large, scalability is an additional challenge.</p>
<blockquote>
<p>当输入图形或者中间状态或者计算复杂度非常大的时候，可测量性是一额外的挑战。</p>
</blockquote>
<p>The development of graph processing systems such as Pregel [25] has changed this scenario and made it simpler to design scalable graph analytics algorithms. </p>
<blockquote>
<p>图形处理系统的发展例如pregel改变了这种方案，并且使设计可升级的图形分析算法更加的简单。</p>
</blockquote>
<p>Pregel offers a simple “think like a vertex” (TLV) programming paradigm, where each vertex of the input graph is a processing element holding local state and communicating with its neighbors in the graph.</p>
<blockquote>
<p>Pregel 提供了一个简单的”像顶点一样思考”的编程范例，每一个输入图的顶点是一个保持局部状态的处理单元并且在图形中和它的邻点进行通讯。</p>
</blockquote>
<p>TLV is a perfect match for problems that can be represented through linear algebra, where the graph is modeled as an adjacency matrix (or some other variant like the Laplacian matrix) and the current state of each vertex is represented as a vector.</p>
<blockquote>
<p>T L V 对那些通过线性代数表示的问题能够完美的匹配，在那些图形建模为邻接矩阵（或者一些其他变形像路普拉斯矩阵）和每一个顶点的当前状态被表示为一个向量的问题中。</p>
</blockquote>
<p>We call this class of methods graph computation problems.</p>
<blockquote>
<p>我们称这一类的方法叫做图计算问题</p>
</blockquote>
<p>A good example is computing PageRank [6], which is based on iterative sparse matrix and vector multiplication operations.</p>
<blockquote>
<p>一个好的例子就是计算PageRank, 它是基于迭代稀疏矩阵和向量乘法运算。</p>
</blockquote>
<p>TLV covers several other algorithms that require a similar computational architecture, for example, shortest path algorithms, and over the years many optimizations of this paradigm have been proposed [17, 26, 36, 42].</p>
<blockquote>
<p>TLV 涉及了一些其他的算法，它需要相似的计算结构，例如，最短路径算法，多年来，这种模式的许多优化已被提出来。</p>
</blockquote>
<p>Despite this progress, there remains an important class of algorithms that cannot be readily formulated using the TLV paradigm. </p>
<blockquote>
<p>尽管这些进展，这里依然有一类重要的算法不可以使用TLV范例制定。</p>
</blockquote>
<p>These are graph mining algorithms used to discover relevant patterns that comprise both structurebased and label-based properties of the graph.</p>
<blockquote>
<p>这些就是用于发现相关模式的基于结构和基于表的图的性质的图挖掘算法。</p>
</blockquote>
<p> Graph mining is widely used for several applications, for example, discovering 3D motifs in protein structures or chemical compounds, extracting network motifs or significant subgraphs from protein-protein or gene interaction networks, mining attributed patterns over semantic data (e.g., in Resource Description Framework or RDF format), finding structure content relationships in social media data, dense subgraph mining for community and link spam detection in web data,among others.</p>
<blockquote>
<p>图挖掘广泛的用于一些应用，例如发现蛋白质结构中或者化学物质中的3D图案，从蛋白质或者基因交互网络中提取网络图案或者重要的子图，(例如在资源描述框架或者R D F 格式中)，正在使用发音在社会媒体数据，密集的子图挖掘社区和链接的垃圾邮件检测在Web数据中发现结构内容的关系，等等。</p>
</blockquote>
<p> Graph mining algorithms typically take a labeled and immutable graph as input, and mine patterns that have some algorithm-specific property (e.g., frequency above some threshold) by finding all instances of these patterns<br>in the input graph. </p>
<blockquote>
<p>图形挖掘算法通常采用一个标记和不可变的图形作为输入,和具有一些算法特性的挖掘模式（例如频率高于某个阈值），通过在输入图中的样式的所有实例。</p>
</blockquote>
<p>Some algorithms also compute aggregated metrics based on these subgraphs。<br>一些算法也计算基于这些子图的综合指标。</p>
<p>Figure 1: Exponential growth of the intermediate state in<br>graph mining problems (motifs counting, clique finding,<br>FSM: Frequent subgraph mining) on different datasets.</p>
<blockquote>
<p>图一：在不同的数据集中图挖掘问题中的中间状态的指数增长（图案计数，派系的发现，频繁子图挖掘）</p>
</blockquote>
<p>Designing graph mining algorithms is a challenging and<br>active area of research. </p>
<blockquote>
<p>设计图挖掘算法在研究中是一个具有挑战性和活跃的领域</p>
</blockquote>
<p>In particular, scaling graph mining<br>algorithms to even moderately large graphs is hard. </p>
<blockquote>
<p>尤其是，将图挖掘算法应用于中等大小的图是困难的</p>
</blockquote>
<p>The set of possible patterns and their subgraphs in a graph can be<br>exponential in the size of the original graph, resulting in an<br>explosion of the computation and intermediate state.</p>
<blockquote>
<p>图表中的可能的模式集和他们的子图有可能是原始图大小的指数倍，导致了爆炸性的计算和中间状态。</p>
</blockquote>
<p>Figure 1 shows the exponential growth of the number of “interesting”<br>subgraphs of different sizes in some of the graph<br>mining problems and datasets we will evaluate in this paper.</p>
<blockquote>
<p>图一展示在一些图挖掘问题中不同大小的“intersting”子图的数量的爆炸性增长并且我们将评估文本的数据集。</p>
</blockquote>
<p>Even graphs with few thousands of edges can quickly generate<br>hundreds of millions of interesting subgraphs. </p>
<blockquote>
<p>即使是数千个边的图也能生成数亿的”intersting”子图。</p>
</blockquote>
<p>The need for enumerating a large number of subgraphs characterizes<br>graph mining problems and distinguishes them from graph<br>computation problems.</p>
<blockquote>
<p>图挖掘问题以需要枚举很大数量的子图为特征并且将其于图计算问题区别开来。</p>
</blockquote>
<p> Despite this state explosion problem,<br>most graph mining algorithms are centralized because of the<br>complexity of distributed solutions.</p>
<blockquote>
<p>尽管这个状态是爆炸性的问题，但是大多数图形挖掘算法是集中式的，因为<br>分布式解决方案的复杂性。</p>
</blockquote>
<p>In this paper, we propose automatic subgraph exploration<br>as a generic building block for solving graph mining<br>problems, and introduce Arabesque, the first embedding exploration<br>system specifically designed for distributed graph<br>mining. </p>
<blockquote>
<p>在这篇文章中，我们把自动子图搜索看作是一个解决图挖掘问题的通用构建，并且介绍阿拉伯图案<br>，它是第一个为分布式图挖掘设计的嵌入式的探索系统</p>
</blockquote>
<p>Conceptually, we move from TLV to “think like an<br>embedding” (TLE), where by embedding we denote a subgraph<br>representing a particular instance of a more general<br>template subgraph called a pattern (see Figure 2).</p>
<blockquote>
<p>从概念上讲，我们从TLV移动到了“像嵌入一样思考”（TLE）,通过嵌入，我们表示一个子图<br>通过表示一个称之为模式的更一般的模板的特别的实例（看图二）</p>
</blockquote>
<p>Arabesque defines a high-level filter-process computational<br>model.</p>
<blockquote>
<p>阿拉伯图案定义了一个高层次的过滤过程计算模型。</p>
</blockquote>
<p>Given an input graph, the system takes care<br>of automatically and systematically visiting all the embeddings<br>that need to be explored by the user-defined algorithm,<br>performing this exploration in a distributed manner.</p>
<blockquote>
<p>给出一个输入图，系统会自动的，系统性的关注访问所有的那些需要通过通过分布式的方式进行的自定义算法探索的嵌入部分。</p>
</blockquote>
<p>The system passes all the embeddings it explores to the application,<br>which consists primarily of two functions: filter, which indicates whether an embedding should be processed, and process,which examines an embedding and may produce some output. </p>
<blockquote>
<p>系统通过所有的嵌入部分并暴露给应用，应用主要由两个函数组成：过滤器，指示是否嵌入部分应该被处理。处理，审查一个嵌入部分和有可能处理一些输出。</p>
</blockquote>
<p>For example, in the case of finding cliques the filter function prunes embeddings that are not cliques, since none of their extensions can be cliques, and the process function outputs all explored embeddings, which are cliques by construction.</p>
<blockquote>
<p>例如，在发现子图派系的案例中过滤器的功能用于修剪不是派系的嵌入部分，因为他们的拓展没有可能是派系，并且处理函数输出所有探索的嵌入部分，那些嵌入的部分通过建设而形成派系。</p>
</blockquote>
<p>Arabesque also supports the pruning of the exploration<br>space based on user-defined metrics aggregated across multiple embeddings.</p>
<blockquote>
<p>Arabesque还支持修剪基于用户定义的度量标准聚合的空间多次嵌入的探测。</p>
</blockquote>
<p>Figure 2: Graph mining concepts: an input graph, an example<br>pattern, and the embeddings of the pattern. Colors represent<br>labels. Numbers denote vertex ids. Patterns and embeddings<br>are two types of subgraphs. However, a pattern is<br>a template, whereas an embedding is an instance. In this example,<br>the two embeddings are automorphic.</p>
<blockquote>
<p>图二：</p>
</blockquote>
<p>The Arabesque API simplifies and thus democratizes the<br>design of graph mining algorithms, and automates their execution<br>in a distributed setting. We used Arabesque to implement<br>and evaluate scalable solutions to three fundamental<br>and diverse graph mining problems: frequent subgraph mining,<br>counting motifs, and finding cliques. These problems<br>are defined precisely in Section 2. Some of these algorithms<br>are the first distributed solutions available in the literature,<br>which shows the simplicity and generality of Arabesque.</p>
<p>Arabesque’s embedding-centered API facilitates a highly<br>scalable implementation. The system scales by spreading<br>embeddings uniformly across workers, thus avoiding<br>hotspots. By making it explicit that embeddings are the fundamental<br>unit of exploration, Arabesque is able to use fast<br>coordination-free techniques, based on the notion of embedding<br>canonicality, to avoid redundant work and minimize<br>communication costs. It also enables us to store embeddings<br>efficiently using a new data structure called Overapproximating<br>Directed Acyclic Graph (ODAG), and to devise a<br>new two-level optimization for pattern-based aggregation,<br>which is a common operation in graph mining algorithms.</p>
<p>Arabesque is implemented as a layer on top of Apache<br>Giraph [3], a Pregel-inspired graph computation system,<br>thus allowing both graph computation and graph mining<br>algorithms to run on top of the same infrastructure. The<br>implementation does not use a TLV approach: it considers<br>Giraph just as a regular data parallel system implementing<br>the Bulk Synchronous Processing model.<br>To summarize, we make the following contributions:</p>
<p>• We propose embedding exploration, or “think like an embedding”,<br>as an effective basic building block for graph<br>mining. We introduce the filter-process computational<br>model (Section 3), design an API that enables embedding<br>exploration to be expressed effectively and succinctly,<br>and present three example graph mining applications<br>that can be elegantly expressed using the Arabesque<br>API (Section 4).</p>
<p>• We introduce techniques to make distributed embedding<br>exploration scalable: coordination-free work sharing, ef-<br>ficient storage of embeddings, and an important optimization<br>for pattern-based aggregation (Section 5).</p>
<p>• We demonstrate the scalability of Arabesque on various<br>graphs. We show that Arabesque scales to hundreds of<br>cores over a cluster, obtaining orders of magnitude reduction<br>of running time over the centralized baselines (Section<br>6), and can analyze trillions of embeddings on large<br>graphs.</p>
<p>The Arabesque system, together with all applications<br>used for this paper, is publicly available at the project’s website:<br>www.arabesque.io.</p>
<h4 id="段落翻译-1"><a href="#段落翻译-1" class="headerlink" title="段落翻译"></a>段落翻译</h4><p>图形数据在许多领域普遍存在，从网站到广告业和生物学，并且分析图形正在变得越来越重要。<br>图形分析算法的发展催生了大量的研究，尤其是在近些年来。但是，图形分析历年来是具有挑战性的，由那些能够为了手上的问题设计新的专门的算法或者从非常庞大的文献中选择一个适当并且健全的解决方案的专家去解决。当输入图形或者中间状态或者计算复杂度非常大的时候，可测量性是一额外的挑战。</p>
<p>图形处理系统的发展例如pregel 改变了这种方案，并且使设计可升级的图形分析算法更加的简单。<br>Pregel 提供了一个简单的”像顶点一样思考”的编程范例，每一个输入图的顶点是一个保持局部状态的处理单元并且在图形中和它的邻点进行通讯。T L V 对那些通过线性代数表示的问题能够完美的匹配，在那些图形建模为邻接矩阵（或者一些其他变形像路普拉斯矩阵）和每一个顶点的当前状态被表示为一个向量的问题中。我们称这一类的方法叫做图计算问题一个好的例子就是计算PageRank, 它是基于迭代稀疏矩阵和向量乘法运算。TLV 涉及了一些其他的算法，它需要相似的计算结构，例如，最短路径算法，多年来，这种模式的许多优化已被提出来。</p>
<p>尽管这些进展，这里依然有一类重要的算法不可以使用TLV范例制定。这些就是用于发现相关模式的基于结构和基于表的图的性质的图挖掘算法.图挖掘广泛的用于一些应用，例如发现蛋白质结构中或者化学物质中的3D图案，从蛋白质或者基因交互网络中提取网络图案或者重要的子图，(例如在资源描述框架或者R D F 格式中)，正在使用发音在社会媒体数据，密集的子图挖掘社区和链接的垃圾邮件检测在Web数据中发现结构内容的关系，等等。图形挖掘算法通常采用一个标记和不可变的图形作为输入,和具有一些算法特性的挖掘模式（例如频率高于某个阈值），通过在输入图中的样式的所有实例。一些算法也计算基于这些子图的综合指标。</p>
<p><img src="./img/1.PNG" alt="图1"></p>
<p>图一：在不同的数据集中图挖掘问题中的中间状态的指数增长（图案计数，派系的发现，频繁子图挖掘）</p>
<p>设计图挖掘算法在研究中是一个具有挑战性和活跃的领域。尤其是，将图挖掘算法应用于中等大小的图是很困难的。图表中的可能的模式集和他们的子图有可能是原始图大小的指数倍，导致了爆炸性的计算和中间状态。图一展示在一些图挖掘问题中不同大小的“intersting”子图的数量的爆炸性增长并且我们将评估文本的数据集。即使是数千个边的图也能生成数亿的”intersting”子图。图挖掘问题以需要枚举很大数量的子图为特征并且将其于图计算问题区别开来。尽管这个状态是爆炸性的问题，但是大多数图形挖掘算法是集中式的，因为分布式解决方案的复杂性。</p>
<p>在这篇文章中，我们把自动子图搜索看作是一个解决图挖掘问题的通用构建，并且介绍阿拉伯图案<br>，它是第一个为分布式图挖掘设计的嵌入式的探索系统。从概念上讲，我们从TLV移动到了“像嵌入一样思考”（TLE）,通过嵌入，我们表示一个子图通过表示一个称之为模式的更一般的模板的特别的实例（看图二）。阿拉伯图案定义了一个高层次的过滤过程计算模型。给出一个输入图，系统会自动的，系统性的关注访问所有的那些需要通过通过分布式的方式进行的自定义算法探索的嵌入部分。系统通过所有的嵌入部分并暴露给应用，应用主要由两个函数组成：过滤器，指示是否嵌入部分应该被处理。处理，审查一个嵌入部分和有可能处理一些输出。例如，在发现子图派系的案例中过滤器的功能用于修剪不是派系的嵌入部分，因为他们的拓展没有可能是派系，并且处理函数输出所有探索的嵌入部分，那些嵌入的部分通过建设而形成派系。</p>
<p><img src="./img/2.PNG" alt="图2"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Arabesque-A-System-for-Distributed-Graph-Mining&quot;&gt;&lt;a href=&quot;#Arabesque-A-System-for-Distributed-Graph-Mining&quot; class=&quot;headerlink&quot; title=&quot;Arabesque: A System for Distributed Graph Mining&quot;&gt;&lt;/a&gt;Arabesque: A System for Distributed Graph Mining&lt;/h2&gt;&lt;blockquote&gt;
&lt;h3 id=&quot;Arabesque-分布式的图挖掘系统&quot;&gt;&lt;a href=&quot;#Arabesque-分布式的图挖掘系统&quot; class=&quot;headerlink&quot; title=&quot;Arabesque:分布式的图挖掘系统&quot;&gt;&lt;/a&gt;Arabesque:分布式的图挖掘系统&lt;/h3&gt;&lt;/blockquote&gt;
&lt;h4 id=&quot;原文链接：点我跳转&quot;&gt;&lt;a href=&quot;#原文链接：点我跳转&quot; class=&quot;headerlink&quot; title=&quot;原文链接：点我跳转&quot;&gt;&lt;/a&gt;原文链接：&lt;a href=&quot;https://github.com/jChanJi/jchanji.github.com/blob/master/meterial/093-teixeira.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;点我跳转&lt;/a&gt;&lt;/h4&gt;&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;#Abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1.&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="-arcticals" scheme="http://jchanji.github.io/categories/arcticals/"/>
    
    
      <category term="Graph Mining" scheme="http://jchanji.github.io/tags/Graph-Mining/"/>
    
      <category term="distribute system" scheme="http://jchanji.github.io/tags/distribute-system/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu只显示桌面，没有菜单栏</title>
    <link href="http://jchanji.github.io/year/09/03/ubuntu_only_background/"/>
    <id>http://jchanji.github.io/year/09/03/ubuntu_only_background/</id>
    <published>2017-09-03T06:44:05.410Z</published>
    <updated>2017-09-03T07:13:09.263Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>在裝輸入法的時候好像刪了什麼東西導致電腦重啓的時候只能顯示桌面背景和文件，導航等都沒了，頓時嚇壞我了，找了好多教程終於成功了。</p>
</blockquote>
<a id="more"></a>
<p>##安裝unity<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install unity</div></pre></td></tr></table></figure></p>
<p>##刪除配置文件<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo rm -rf .conf</div><div class="line">sudo rm -rf .gconfg</div><div class="line">sudo rm -rf ~/.Xauthority</div><div class="line">reboot</div></pre></td></tr></table></figure></p>
<p>本教程不一定對其他情況也適合</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在裝輸入法的時候好像刪了什麼東西導致電腦重啓的時候只能顯示桌面背景和文件，導航等都沒了，頓時嚇壞我了，找了好多教程終於成功了。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="-ubuntu" scheme="http://jchanji.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>伪分布式spark安装配置</title>
    <link href="http://jchanji.github.io/year/09/03/spark_step/"/>
    <id>http://jchanji.github.io/year/09/03/spark_step/</id>
    <published>2017-09-03T06:44:05.395Z</published>
    <updated>2017-09-03T06:57:00.318Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>本教程为spark的伪分布式教程，学生党条件有限所以伪分布式应该是比较好的选择。其中要注意的是版本匹配的问题。</p>
</blockquote>
<a id="more"></a>
<h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol>
<li>CentOS7</li>
<li>jdk:jdk1.8.0_131</li>
<li>hadoop：2.6.0</li>
<li>scala:2.11.11</li>
<li>spark:2.1.1</li>
</ol>
<h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2><ol>
<li><a href="http://spark.apache.org/downloads.html" target="_blank" rel="external">spark2.1.1</a></li>
<li><a href="http://www.scala-lang.org/download/all.html" target="_blank" rel="external">scala2.11.11</a></li>
</ol>
<h2 id="三、安装配置"><a href="#三、安装配置" class="headerlink" title="三、安装配置"></a>三、安装配置</h2><h3 id="1、解压-修改权限"><a href="#1、解压-修改权限" class="headerlink" title="1、解压,修改权限"></a>1、解压,修改权限</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo tar -zxvf spark-2.1.1-bin-hadoop2.6.tgz -C /usr/local</div><div class="line">cd /usr/local</div><div class="line">sudo chown -R hadoop:hadoop ./spark</div></pre></td></tr></table></figure>
<h3 id="2、在解压的spark目录下新建文件-test-hellospark-写上内容"><a href="#2、在解压的spark目录下新建文件-test-hellospark-写上内容" class="headerlink" title="2、在解压的spark目录下新建文件/test/hellospark,写上内容"></a>2、在解压的spark目录下新建文件/test/hellospark,写上内容</h3><h3 id="3、进入scala模式"><a href="#3、进入scala模式" class="headerlink" title="3、进入scala模式"></a>3、进入scala模式</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /spark/bin</div><div class="line">./spark-shell</div></pre></td></tr></table></figure>
<h3 id="4、运行代码"><a href="#4、运行代码" class="headerlink" title="4、运行代码"></a>4、运行代码</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">5、val lines = sc.textFile(“../test/hellospark”)</div><div class="line">   lines.count()</div><div class="line">   lines.first()</div></pre></td></tr></table></figure>
<h3 id="5、修日志级别"><a href="#5、修日志级别" class="headerlink" title="5、修日志级别"></a>5、修日志级别</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/spark/conf</div><div class="line">cp  log4j.properties.template  log4j.properties   </div><div class="line">sudo vim log4j.properties</div></pre></td></tr></table></figure>
<p>   将其中的rootCategory=INFO 改为 WARN</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;本教程为spark的伪分布式教程，学生党条件有限所以伪分布式应该是比较好的选择。其中要注意的是版本匹配的问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
      <category term="spark" scheme="http://jchanji.github.io/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>sumlime text3 配置Markdown</title>
    <link href="http://jchanji.github.io/year/09/03/markdown/"/>
    <id>http://jchanji.github.io/year/09/03/markdown/</id>
    <published>2017-09-03T06:44:05.362Z</published>
    <updated>2017-09-03T06:57:25.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、sumlime-text3-配置Markdown和常用快捷键"><a href="#一、sumlime-text3-配置Markdown和常用快捷键" class="headerlink" title="一、sumlime text3 配置Markdown和常用快捷键"></a>一、sumlime text3 配置Markdown和常用快捷键</h2><h3 id="1、sumlime-text3-配置Markdown"><a href="#1、sumlime-text3-配置Markdown" class="headerlink" title="1、sumlime text3 配置Markdown"></a>1、sumlime text3 配置Markdown</h3><blockquote>
<h4 id="1、安装package-Control"><a href="#1、安装package-Control" class="headerlink" title="1、安装package Control"></a>1、安装package Control<br></h4><h4 id="2、安装Markdown-Preview"><a href="#2、安装Markdown-Preview" class="headerlink" title="2、安装Markdown Preview"></a>2、安装Markdown Preview<br></h4><p>&ensp; 2.1、 按Shif + Alt + P打开<br><br>&ensp; 2.2、输入pcip,回车（进入install package）<br></p>
</blockquote>
<a id="more"></a>
<blockquote>
<h4 id="3、安装Markdown-Editing"><a href="#3、安装Markdown-Editing" class="headerlink" title="3、安装Markdown  Editing"></a>3、安装Markdown  Editing<br></h4><p>&ensp;3.1、进入 install package<br><br>&ensp;3.2、输入 Markdown Editing // Markdown编辑和语法高亮支持<br></p>
<h4 id="4、安装Markdown-Previewer"><a href="#4、安装Markdown-Previewer" class="headerlink" title="4、安装Markdown  Previewer"></a>4、安装Markdown  Previewer<br></h4><p>&ensp; 4.1、进入 install package<br><br>&ensp; 4.2、Markdown  Previewer  //Markdown导出html预览支持<br></p>
<h4 id="5、安装OmniMarkup-Previewer"><a href="#5、安装OmniMarkup-Previewer" class="headerlink" title="5、安装OmniMarkup Previewer"></a>5、安装OmniMarkup Previewer<br></h4><p>&ensp; 5.1、进入 install package<br><br>&ensp; 5.2、OmniMarkup Previewer //在浏览器中实时预览</p>
</blockquote>
<h3 id="2、常用快捷键"><a href="#2、常用快捷键" class="headerlink" title="2、常用快捷键"></a>2、常用快捷键</h3><blockquote>
<ol>
<li>Ctrl + Alt + O //在浏览器中打开</li>
<li>Alt + M  //生成html文件</li>
<li>Ctrl+Alt+O: Preview Markup in Browser.</li>
<li>Ctrl+Alt+X: Export Markup as HTML.</li>
<li>Ctrl+Alt+C: Copy Markup as HTML.</li>
</ol>
</blockquote>
<h2 id="二、使用Cmd-markdown在线编辑"><a href="#二、使用Cmd-markdown在线编辑" class="headerlink" title="二、使用Cmd markdown在线编辑"></a>二、使用Cmd markdown在线编辑</h2><blockquote>
<ol>
<li>在线<a href="https://www.zybuluo.com/mdeditor" title="Cmd Markdown" target="_blank" rel="external">编辑</a> 网址</li>
<li>也可以下载客户端离线编辑<a href="https://www.zybuluo.com/cmd/" title="下载" target="_blank" rel="external">客户端</a></li>
<li>如果想转成html等其他功能需要付费，不过基础功能已经差不多够用了</li>
</ol>
</blockquote>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="1、Ctrl-Alt-O后打开了浏览器但是不能够预览markdown页面"><a href="#1、Ctrl-Alt-O后打开了浏览器但是不能够预览markdown页面" class="headerlink" title="1、Ctrl+Alt+O后打开了浏览器但是不能够预览ｍａｒｋｄｏｗｎ页面"></a>1、Ctrl+Alt+O后打开了浏览器但是不能够预览ｍａｒｋｄｏｗｎ页面</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">在 Preferences &gt; Package Settings &gt; OmniMarkupPreviewer &gt; Settings - User 中粘贴以下代码即可</div><div class="line"></div><div class="line">&#123;</div><div class="line">"renderer_options-MarkdownRenderer": &#123;</div><div class="line"><span class="code">    "extensions": ["tables", "fenced_code", "codehilite"]</span></div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="2、在sublime-text3-中切换不了中文"><a href="#2、在sublime-text3-中切换不了中文" class="headerlink" title="2、在sublime text3 中切换不了中文"></a>2、在sublime text3 中切换不了中文</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update &amp;&amp; sudo apt-get upgrade</div><div class="line">git clone https://github.com/lyfeyaj/sublime-text-imfix.git</div><div class="line">cd ~/sublime-text-imfix</div><div class="line">sudo ./ sublime-imfix</div><div class="line">然后重启sublime text3</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、sumlime-text3-配置Markdown和常用快捷键&quot;&gt;&lt;a href=&quot;#一、sumlime-text3-配置Markdown和常用快捷键&quot; class=&quot;headerlink&quot; title=&quot;一、sumlime text3 配置Markdown和常用快捷键&quot;&gt;&lt;/a&gt;一、sumlime text3 配置Markdown和常用快捷键&lt;/h2&gt;&lt;h3 id=&quot;1、sumlime-text3-配置Markdown&quot;&gt;&lt;a href=&quot;#1、sumlime-text3-配置Markdown&quot; class=&quot;headerlink&quot; title=&quot;1、sumlime text3 配置Markdown&quot;&gt;&lt;/a&gt;1、sumlime text3 配置Markdown&lt;/h3&gt;&lt;blockquote&gt;
&lt;h4 id=&quot;1、安装package-Control&quot;&gt;&lt;a href=&quot;#1、安装package-Control&quot; class=&quot;headerlink&quot; title=&quot;1、安装package Control&quot;&gt;&lt;/a&gt;1、安装package Control&lt;br&gt;&lt;/h4&gt;&lt;h4 id=&quot;2、安装Markdown-Preview&quot;&gt;&lt;a href=&quot;#2、安装Markdown-Preview&quot; class=&quot;headerlink&quot; title=&quot;2、安装Markdown Preview&quot;&gt;&lt;/a&gt;2、安装Markdown Preview&lt;br&gt;&lt;/h4&gt;&lt;p&gt;&amp;ensp; 2.1、 按Shif + Alt + P打开&lt;br&gt;&lt;br&gt;&amp;ensp; 2.2、输入pcip,回车（进入install package）&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="sublime text3" scheme="http://jchanji.github.io/tags/sublime-text3/"/>
    
      <category term="markdown" scheme="http://jchanji.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>从互联到新工业革命-读后感</title>
    <link href="http://jchanji.github.io/year/09/03/internet_of_everthing_for_new_industrial_revolution/"/>
    <id>http://jchanji.github.io/year/09/03/internet_of_everthing_for_new_industrial_revolution/</id>
    <published>2017-09-03T06:44:05.336Z</published>
    <updated>2017-09-03T06:53:50.510Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><h3 id="一、工业4-0”网红”的养成之路"><a href="#一、工业4-0”网红”的养成之路" class="headerlink" title="一、工业4.0”网红”的养成之路"></a><a href="#one">一、工业4.0”网红”的养成之路</a></h3><h3 id="二、”工业互联网”-VS-“工业4-0”"><a href="#二、”工业互联网”-VS-“工业4-0”" class="headerlink" title="二、”工业互联网” VS “工业4.0”"></a><a href="#two">二、”工业互联网” VS “工业4.0”</a></h3><h3 id="三、中国制造2025"><a href="#三、中国制造2025" class="headerlink" title="三、中国制造2025"></a><a href="#three">三、中国制造2025</a></h3><h3 id="四、工业革命升级技能点"><a href="#四、工业革命升级技能点" class="headerlink" title="四、工业革命升级技能点"></a><a href="#forth">四、工业革命升级技能点</a></h3><h3 id="五、人工智能"><a href="#五、人工智能" class="headerlink" title="五、人工智能"></a><a href="#firth">五、人工智能</a></h3><h3 id="六、工业互联网的智能网络"><a href="#六、工业互联网的智能网络" class="headerlink" title="六、工业互联网的智能网络"></a><a href="#six">六、工业互联网的智能网络</a></h3><h3 id="七、结束语"><a href="#七、结束语" class="headerlink" title="七、结束语"></a><a href="#end">七、结束语</a></h3><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;有幸能够参加东华大学计算机科学与技术学院举办的“大数据与智能制造”暑期夏令营。虽然只有短短的两天时间，但是收获颇多，尤其是听了燕山大学机械学院院长张立杰教授“智能制造和传统制造”的演讲和美国佛罗里达大学教授李晓林教授“Creating Intellignece via big learning”的演讲，对智能制造和人工智能领域了更加深刻的印象。由于我比较愚钝，具体的演讲内容不能详细的复述出来。再次由衷的感谢东华大学的常珊老师免费提供给我们《从互联到新工业革命》这本书，今天在火车上读完了这本通俗易懂而又见解深刻的书,无意发现,张立杰教授，李晓林教授，刘云浩教授，在智能制造方面的见解英雄所见略同，而为了更加体系的介绍和便于自己思路的清理，下面更多介绍清华大学软件学院院长刘云浩教授对互联网时代和新工业革命大潮的理解和体会并且再加上一点我个人的浅陋的见解。 由于本人文采有限，不能够很全面的写出刘老师书中的方方面面也无法用诙谐的语句吸引读者兴趣，所以我极力推荐大家读一读刘云浩老师的作品《从互联到新工业革命》（清华大学出版社）。由于我也只是泛读了一遍,所以写博客的时候也是第二次更加粗略的阅读，当中有什么见解不到的地方欢迎联系我（邮箱见文章底部）。</p>
</blockquote>
<a id="more"></a>
<h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;由于我也是刚接触大数据，人工智能不久，虽然对其中的技术细节还不是非常的了解，但引用刘国华教授”纸上谈兵”的观点，如果不去”纸上谈兵”而直接去埋头编程那么有可能永远都完成不了项目，或者当中算法效率是很低的。同样我觉得，学习一个完全陌生的专业，如果连它的发展趋势和技术线路都没有搞清楚的话，那么也只是一头雾水的填鸭式的学习,不利于以后潜力的挖掘和能力的提升。下面我们就”空谈”一些对技术能力提高”无用”的，空泛的，所谓”夸夸而谈”的观点。</p>
</blockquote>
<h2 id="一、工业4-0”网红”的养成之路-1"><a href="#一、工业4-0”网红”的养成之路-1" class="headerlink" title=" 一、工业4.0”网红”的养成之路"></a><span id="one"> 一、工业4.0”网红”的养成之路</span></h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;目前为止确切的有三次工业革命：1769年瓦特发明蒸汽机，标志着机械化的时代到来,机器代替了人类一部分的体力活动，人类向机械化迈进;1869年，德国西门子公司发明了第一台交流发动机，电器取代了机器，电器动力取代了蒸汽动力，从此促进了大规模，批量化的生产。也是从此时开始，东方开始落后西方；1969年，第一块可编程逻辑控制器Modicon 084问世，这标志这电子信息技术的发明并且直接导致了产品生产的高度自动化。此外还有一件划时代的事发生了，便是阿帕网的形成，也就是互联网的雏形。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;近两百年之前的工业革命，在之后的150年里使西方人均收入增长了13倍，而1800年以前，西方人均经济翻一倍则需要800年。这足以见得这几次工业革命对人类社会生产力的解放是多么的恐怖，这也预示着，人类社会的发展速度将会越来越快。那么第四次工业革命会是在2069年吗？显然，就目前的形式来看完全等不到2069年。新工业革命已经隐约到来，虽然我们不能够从未来的角度来看现在来，判定是否是第四次工业革命，就像前三次工业革命一样，发生之后才意识具有多么伟大的意义，但是，我们已经确切的感受到新工业革命了。”人类第一次成功的在事前预测了一次革命，而不是像以前一样事后才认识到是一场革命”,正如刘老师所言。”工业4.0”<br>由孔翰宁(Henning Kagermann),沃夫冈.瓦尔斯特(“Wolfgang Wahlster”),沃尔夫迪特尔.卢卡斯(Wolf-Dieter Lukas)三位博士提出,由于”产官学”（ 产业界，政府，学术界）属性的与生俱来，很快便由德国工程院，弗劳恩获夫协会，西门子公司等接手，组成了工业4.0小组,于是工业4.0迅速的冲出了德国，走向了世界。所以”工业4.0”正是天时地利人和的结果。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;这里插入一些我的人生观，不想听大道理的可以跳过了。其实很多事情都是水到而渠成的,尤其是知识积累更是如此。面对飞速发展的软件行业，作为初级程序员,很多人都想一口吃个胖子,想要快速的掌握开发技能,喜欢看速成的视频,教程，包括我也是如此,但很多时候却走了很多的弯路。因为基础知识不扎实而找东找西,就是一根筋的想要找和自己的问题一摸一样的解答，却不知道或者懒得去变通一下代码，或者花点时间去专研一下代码中的逻辑思维，从中受到启发。与其花半天时间去研读代码。却更愿意去花一天时间尝遍百度上的所有教程。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;官方对于”工业4.0”的解释是,”工业4.0包括将信息物理系统(Cyber physical System，CPS)技术一体化应用与制造业和物流行业,以及在工业生产过程中采用物联网和服务技术”。从这段定义中个我们可以看到很熟悉的一个词”物联网”,所以从此也可以看出以后的IT热门方向。物联网从前几年开始变得炙手可热，但发展一直没有想象中的那么迅猛，这和很多方面因素有关，包括硬件支持，传输技术，等等。但这丝毫不影响其发展趋势，因为物联网还是在不断发展的，而且越来越快,其模型成熟的时间决定着万物互联时代到来的时间。”工业4.0”产生的”智能工厂”和”智能生产”将改变传统的批量统一化的生产模式,实现高度灵活的个性化和数字化生产及服务，最终使生产更智能，更高效，跟快速，更经济。</p>
</blockquote>
<h2 id="ps1"><a href="#ps1" class="headerlink" title="ps1"></a>ps1</h2><blockquote>
<p>由于我的手速有限，时间紧迫，今天只能谈到这。作为有点学术性质而又不深入具体细节的博客，希望大家能够当成故事看，了解当今的IT界的发展方向。我每天晚上会抽出11点之后断网的时间续写，第二点早上9点跟新,时间有限，我会尽快的完结。</p>
</blockquote>
<h2 id="二、”工业互联网”-VS-“工业4-0”-1"><a href="#二、”工业互联网”-VS-“工业4-0”-1" class="headerlink" title=" 二、”工业互联网” VS “工业4.0”"></a><span id="two"> 二、”工业互联网” VS “工业4.0”</span></h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;面对的德国的高歌猛进，世界第一大国美国怎能无动于衷了？毕竟在这个星球上主导权决定着发言权，就算是没有足够的主导权也不能牵制于人。由沙利文（Frost &amp; Sullivan）这家咨询公司在一份报告中创造性的提出了“工业互联网”这个概念。也因此给沙利文公司在工业制造领域带来了话语权。公司还顺带的设立了“制造领袖奖”,2016年通用公司就很高兴的领了这个奖。2014年3月由通用电气（提供综合技术与服务）联合AT&amp;T(M2M的解决方案)、Cisco(提供网络解决方案)、Intel(半导体、芯片和处理器)、IBM(智慧地球)成立了“工业互联网联盟(Industrial Internet Consortium,IIC)”。很显然工业互联网这块大蛋糕美国怎么会任由他人分割了,到2015年初，该联盟成员已经达到130家，西门子，华为等号称要自己做工业互联网平台的企业也没能抵制住诱惑。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;很失望的是中国还是一如既往的落后与西方国家，作为发展中国家，不得不承认在先进技术和理念方面中国目前只能去模仿，距离成为“中国制造2025”目标中的世界一流的工业水准还是有很大的差距的。毕竟，不要说“工业4.0”，中国大部分企业还停留在“工业2.0”的水准，“工业3.0”水平也是很弱，这和很多因素有关，但我们还是对未来充满希望的，毕竟科技的快速发展网络的普及化，信息的透明化，以及人才的全球流通，给发展中国家带来的好处是可以快速的跟上队伍。我国并不缺少运行产业联盟的企业，但是成功的却非常少，其中企业自身创新能力弱，国际视野的局限性大是一方面，缺乏一个良好的利益共享机制，无法发挥每个企业的特长也是国内产业联盟难以落地的重要原因。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;”工业互联网”和”工业4.0”中国到底应该站在哪一边了？这就要进一步的了解这两个热门词汇了。“工业互联网”可以说是自顶向下，侧重于利用互联网的技术来改善生产设备和产品服务。从物联网、云计算、大数据分析等信息技术的角度出发，将之应用于工业领域，改造工业生产的产品服务和管理过程等。“工业4.0”则是自下而上，侧重于在生产与制造过程的智能化、数字化。以生产设备为核心的CPS为出发点，推进数据融合和服务共享，从而推及工业生产过程以及产品服务等。虽然由于两国的的产业优势不同导致的工业互联网的结构正好颠倒，但其中的核心思想还是十分相似的。2016年3月，”工业4.0平台”和”工业互联网联盟”在瑞士苏黎世初步达成合作意向，开始了强强联合。这也是应了”马太效应”，”凡有的，还要加给他，叫他有余；凡没有的，连他所有的也要夺去”。</p>
</blockquote>
<h2 id="ps2"><a href="#ps2" class="headerlink" title="ps2"></a>ps2</h2><blockquote>
<p>不知不觉已经12点了，为了不打扰舍友休息，今天就到这里。</p>
</blockquote>
<h2 id="三、中国制造2025-1"><a href="#三、中国制造2025-1" class="headerlink" title=" 三、中国制造2025"></a><span id="three"> 三、中国制造2025</span></h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;世界上很多国家都指定了符合本国国情的工业互联网规划，但基本上都是依据“工业4.0”或者“工业互联网”进行改编。同样，中国也不例外。“中国制造2025”以促进制造业创新发展为主题，以提质增效为中心，以加快新一代信息技术与制造业深度融合为主线，以推进智能制造为主攻方向，以满足经济社会发展和国防建设对重大技术装备的需求为目标，强化工业基础能力，提高综合集成水平，完善多层次多类型的培养体系，促进产业转型升级，培育有中国特色的制造文化，实现制造业由大变强的历史跨越。坚持“创新驱动、质量为先、绿色发展、结构优化、人才为本”的基本方针，坚持“市场主导、政府引导，立足当前、着眼长远，整体推进、重点突破，自主发展、开放合作”的基本原则，通过“三步走”实现制造强国的战略目标：第一步，到2025年迈入制造强国行列；第二步，到2035年中国制造业整体达到世界制造强国阵营中等水平；第三步，到新中国成立一百年时，综合实力进入世界制造强国前列。看到这一大推的雄伟措辞，不得不说中国最强的专家真的不是盖的，由50多位院士100多位领域专家共同指定的规划，将中国制造转变为中国智造的伟大目标高调的向全世界展示。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;中国制造业的现状其实和国足相差无几，所以注定不能自上顶而下的对中国的工业基础进行改革，薄弱的工业基础实在是堪忧。从中国的物流业占GDP比重是美国德国的发达国家的两倍左右就可以看出其中的差距。所以“中国制造2025”更倾向于“工业4.0”的自下而上的进行改革。</p>
</blockquote>
<h2 id="四、工业革命升级技能点-1"><a href="#四、工业革命升级技能点-1" class="headerlink" title="四、工业革命升级技能点"></a><span id="forth">四、工业革命升级技能点</span></h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;从技术角度上来说，第四次工业革命是一场从嵌入式系统到信息物理融合系统的技术变革，通过物联网，云计算，大数据在工业中的运用，促成基于网络化的变革。其关键的技术难点和重点在于实现智能化设备自知而治，泛在化网络（无处不在的网络）互联互通，中心化数据实时实效，开放化服务相辅相成，建立能够在联网对象彼此之间，网络对象和网络环境之间，联网对象和人之间共享的工业互联网，形成物联网，数据联网，服务联网以及人员联网的网络化开放平台。</p>
</blockquote>
<h2 id="PS3"><a href="#PS3" class="headerlink" title="PS3"></a>PS3</h2><blockquote>
<p>第二遍浏览时发现想要阐述出书中的核心思想还是很难的，越来越发现写不下去了。所以还是得放下键盘，再仔细思考一番，理清思路。</p>
</blockquote>
<h2 id="五、人工智能-1"><a href="#五、人工智能-1" class="headerlink" title="五、人工智能"></a><span id="firth">五、人工智能</span></h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;谈到智能工业，智能电网，使用的往往是Smart这个词，而人工智能则使用Aritifical Intelligence（AI）。其原因是人工智能突出的是机器的反映方式能够类似人的智能。而半个世纪以来，人工智能的发展历程很坎坷，机器是否智能一直是一个很有争议的话题。之前之所以认为机器不具有智能是因为机器所完成的任务都是人类所定义好的，并没有超出人类的认知范围或者能力限制。而现在有些深度学习训练出来模型很多已经超出了人的认知范围，因为人能通过参数，阈值的对结果值进行調优，但是算法内部到底是怎样实现的却很难被人所知。随之而来的问题就是，不能够确保其训练出来的模型能够永远的正常使用，所以在金融行业，医疗行业等安全系数要求很高的行业中使用起来是需要对其进行风险评估的。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;人工智能发展的阻碍主要有三个方面。第一、计算机的计算能力。随着硬件的不断升级计算计的运算能力显著提升，但是这不一定代表就可以解决全部的问题。仍然有很多无法优化的算法是需要大量的计算资源的，深度学习出来之后CPU就已经不适合用作为机器学习开发的硬件了，GPU（图形处理单元）将逐步的取代CPU在机器学习中的地位。第二、计算机对真实世界的感知能力。道现在为止人类研究的人工智能在“智力“上已经很高了，但是还是无法像人一样感知世界。对计算机而言实现逻辑推理等人类高级智慧只需要相对较少的计算能力，而现实感知、运动等人类低级智慧却需要巨大的计算资源。第三、推理和逻辑框架。人工智能也无法像人类一样在没有老师的情况下还能够自行的推理并且联想学习，也就是不具备迁移学习的能力。所以人工智能要模拟人的智能其难点不在于人脑进行的各种必然性推理，而是最能体现人的能动性和创造性的不确定推理。</p>
</blockquote>
<h2 id="六、工业互联网的智能网络-1"><a href="#六、工业互联网的智能网络-1" class="headerlink" title="六、工业互联网的智能网络"></a><span id="six">六、工业互联网的智能网络</span></h2><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;一个个孤立的点所包含的信息是很少的，但是将这些点之间相互连接起来，其中包含的信息量就极其巨大了。工业互联网的核心就是将原本割裂的工业数据实现流通，从而变成一个智能网络。我们可以概括为四个环节，即”感，联，知，控“。感，即感知层，机器，机组，物料，人员等物体之间能够相互感知，交互协作，从而实现不同生产实体之间的深度协同。联，即网络互联层，旨在将多元对象组成的异构复杂网络之间形成彼此互联互通的泛在化网络（可以简单理解为无处不在的网络）。知，即数据分析层，网络化的数据有些在传输过程中被即时处理，更多的是汇聚到中心节点后被集中处理。数据分析层负责工业大数据的存储、处理、建模、挖掘、和优化等方面。控，即开放服务层，基于工业大数据形成的决策依据，通过多种面向工业生产应用的开放式，共享型的标准化服务，被工业生产部门调用和实施，反馈到工业生产的各个环节，从而实现对工业生产的控制和调节。从网络角度出发，形成了实体联网，数据联网，服务联网的三重联网。</p>
</blockquote>
<h3 id="七、结束语-1"><a href="#七、结束语-1" class="headerlink" title="七、结束语"></a><span id="end">七、结束语</span></h3><blockquote>
<p>&nbsp;&nbsp;&nbsp;&nbsp;按照书本上的章节的话应该还有几章的内容没有讲到，主要内容是作者结合了中国工业的现状，对前几年发生的事情进行分析，对未来的展望。所以，到此为止就算是完结散花了。但总是感觉自己写的还很不到位，估计是因为心境的问题吧，有时候心境达不到，确实不能够写出什么深层次的东西，总是感觉很是词穷。下面又是我个人的主观瞎想了，没有兴趣的可以拜拜了。</p>
<p>&nbsp;&nbsp;&nbsp;&nbsp;有一句话叫做“软件定义世界”，而以后可能是“人工智能定义软件“。现在人工智能已经逐渐的深入到人们的生活当中来，语音识别，图像识别，等传统的机器学习任务现在使用深度学习对其进行训练，其在人脸识别，语音识别，游戏等很多方面已经超过了人类。所以深度学习的出现，可以说是又一次的焕发了机器学习的活力。毕竟人类从上个世界五六十年代一直到到现在在人工智能方面，能够显著的看出成果并取得重大突破的就是现在，几乎每天的新闻都是某某某使用深度学习神经网络实现了啥啥啥历史性的突破。<br>而人工智能，大数据，云计算近几年来的火爆都不是”横空出世“的。个人觉得其最根本的原因在于互联。正是因为互联，所以数据呈现爆炸式的增长，所以单机的性能远远的不能满足大数据的需求。所以便出现了分布式集群，hadoop框架的诞生更是刺激了大数据的飞速发展。然而个人公司想要购置维护一个机器集群其话费可想而知，估计初创公司在第一步的购置硬件支持上就已经阻力重重了。这时候云计算平台的出现无异于雪中送炭，你只需要根据你所需要的服务，按照资源分配的多少，租用的时常，支付相应的费用便可，这样就大大的降低了业务快速上线的难度。云计算不仅仅给大数据提供了可扩展的平台，也是给人工智能提供了便捷，使得个人的算法实践能够方便，快捷，低成本的运行起来，而不必担心购买昂贵的GPU,显卡等硬件配置并且考虑使用完后的处置。我发像我已经犯困了，舍友以后也能好好睡觉了，完结。</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;h3 id=&quot;一、工业4-0”网红”的养成之路&quot;&gt;&lt;a href=&quot;#一、工业4-0”网红”的养成之路&quot; class=&quot;headerlink&quot; title=&quot;一、工业4.0”网红”的养成之路&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#one&quot;&gt;一、工业4.0”网红”的养成之路&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;二、”工业互联网”-VS-“工业4-0”&quot;&gt;&lt;a href=&quot;#二、”工业互联网”-VS-“工业4-0”&quot; class=&quot;headerlink&quot; title=&quot;二、”工业互联网” VS “工业4.0”&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#two&quot;&gt;二、”工业互联网” VS “工业4.0”&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;三、中国制造2025&quot;&gt;&lt;a href=&quot;#三、中国制造2025&quot; class=&quot;headerlink&quot; title=&quot;三、中国制造2025&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#three&quot;&gt;三、中国制造2025&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;四、工业革命升级技能点&quot;&gt;&lt;a href=&quot;#四、工业革命升级技能点&quot; class=&quot;headerlink&quot; title=&quot;四、工业革命升级技能点&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#forth&quot;&gt;四、工业革命升级技能点&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;五、人工智能&quot;&gt;&lt;a href=&quot;#五、人工智能&quot; class=&quot;headerlink&quot; title=&quot;五、人工智能&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#firth&quot;&gt;五、人工智能&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;六、工业互联网的智能网络&quot;&gt;&lt;a href=&quot;#六、工业互联网的智能网络&quot; class=&quot;headerlink&quot; title=&quot;六、工业互联网的智能网络&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#six&quot;&gt;六、工业互联网的智能网络&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;七、结束语&quot;&gt;&lt;a href=&quot;#七、结束语&quot; class=&quot;headerlink&quot; title=&quot;七、结束语&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#end&quot;&gt;七、结束语&lt;/a&gt;&lt;/h3&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有幸能够参加东华大学计算机科学与技术学院举办的“大数据与智能制造”暑期夏令营。虽然只有短短的两天时间，但是收获颇多，尤其是听了燕山大学机械学院院长张立杰教授“智能制造和传统制造”的演讲和美国佛罗里达大学教授李晓林教授“Creating Intellignece via big learning”的演讲，对智能制造和人工智能领域了更加深刻的印象。由于我比较愚钝，具体的演讲内容不能详细的复述出来。再次由衷的感谢东华大学的常珊老师免费提供给我们《从互联到新工业革命》这本书，今天在火车上读完了这本通俗易懂而又见解深刻的书,无意发现,张立杰教授，李晓林教授，刘云浩教授，在智能制造方面的见解英雄所见略同，而为了更加体系的介绍和便于自己思路的清理，下面更多介绍清华大学软件学院院长刘云浩教授对互联网时代和新工业革命大潮的理解和体会并且再加上一点我个人的浅陋的见解。 由于本人文采有限，不能够很全面的写出刘老师书中的方方面面也无法用诙谐的语句吸引读者兴趣，所以我极力推荐大家读一读刘云浩老师的作品《从互联到新工业革命》（清华大学出版社）。由于我也只是泛读了一遍,所以写博客的时候也是第二次更加粗略的阅读，当中有什么见解不到的地方欢迎联系我（邮箱见文章底部）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-articals" scheme="http://jchanji.github.io/categories/articals/"/>
    
    
      <category term="AI" scheme="http://jchanji.github.io/tags/AI/"/>
    
      <category term="Made in China 2025" scheme="http://jchanji.github.io/tags/Made-in-China-2025/"/>
    
  </entry>
  
  <entry>
    <title>win10 安装ubuntu 16.04</title>
    <link href="http://jchanji.github.io/year/09/03/install_ubuntu/"/>
    <id>http://jchanji.github.io/year/09/03/install_ubuntu/</id>
    <published>2017-09-03T06:44:05.312Z</published>
    <updated>2017-09-03T06:47:07.748Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>在win10下安装ubuntu双系统。在笔记本安装ubuntu的时候遇到了很多的挫折，曾经也放弃过，但很不幸的是，在未来的今天又碰上了。有时候问题的答案很简单，但是需要大量的时间去得到它，并不是你的能力不行，而是网上的干扰答案实在是太多，无法分辨谁对谁假的时候，往往会一个个的试过去。我不认为这是个很笨的方法，因为多花点时间总能体会更多的东西。貌似废话有点多，下面直接上干货。</p>
</blockquote>
<a id="more"></a>
<h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol>
<li>win10 企业版</li>
<li>ubuntu 16.04</li>
<li>UltraSO 9.6.6.3300</li>
<li>显卡：GTX 965M</li>
<li>cup:i7-6700HQ</li>
</ol>
<h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2><ol>
<li><a href="http://releases.ubuntu.com/16.04.2/ubuntu-16.04.2-desktop-amd64.iso?_ga=2.92867550.254780022.1497589112-1524410519.1497589112" target="_blank" rel="external">ubuntu 16.04</a></li>
<li><a href="http://172.19.251.251/files/510300000015EB65/dl.softmgr.qq.com/original/Compression/uiso9_cn_9.6.6.3300.exe" target="_blank" rel="external">UltraSO</a></li>
</ol>
<h2 id="三、安装配置"><a href="#三、安装配置" class="headerlink" title="三、安装配置"></a>三、安装配置</h2><h3 id="1-将镜像刻录到u盘（至少4g）"><a href="#1-将镜像刻录到u盘（至少4g）" class="headerlink" title="1.将镜像刻录到u盘（至少4g）"></a>1.将镜像刻录到u盘（至少4g）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">打开UltraSO，点击试用。</div><div class="line">1、文件-&gt;打开-&gt;镜像位置</div><div class="line">2、启动-&gt;写如硬盘映像-&gt;写入</div></pre></td></tr></table></figure>
<h3 id="2、分配空闲分区"><a href="#2、分配空闲分区" class="headerlink" title="2、分配空闲分区"></a>2、分配空闲分区</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1、右击我的电脑-&gt;管理-&gt;磁盘管理</div><div class="line">2、选择非系统盘的主分区，右击-&gt;压缩卷，选择压缩大小，一般为50G,我的是100G,</div><div class="line">根据自己磁盘情况分配。</div></pre></td></tr></table></figure>
<h3 id="3、将电脑设置为U盘启动"><a href="#3、将电脑设置为U盘启动" class="headerlink" title="3、将电脑设置为U盘启动"></a>3、将电脑设置为U盘启动</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">插入U盘，进入BIOS将U盘设置为启动项</div><div class="line">注：不同主板的BIOS大多都不会相同，所以根据自己电脑型号到网上查找。</div></pre></td></tr></table></figure>
<h3 id="4、安装系统"><a href="#4、安装系统" class="headerlink" title="4、安装系统"></a>4、安装系统</h3><h4 id="1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。"><a href="#1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。" class="headerlink" title="1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。"></a>1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。</h4><h4 id="2、卡在logo"><a href="#2、卡在logo" class="headerlink" title="2、卡在logo"></a>2、卡在logo</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">重新启动电脑，在选择系统安装的界面按e,进入grup界面，让后在splash后面加上：空格nomodeset空格，按F10执行。后面重启出了最后一次也是一样操作</div></pre></td></tr></table></figure>
<h4 id="3、创建分区"><a href="#3、创建分区" class="headerlink" title="3、创建分区"></a>3、创建分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">1、选择自己分配的空闲的磁盘，进行分盘</div><div class="line">2、分盘的注意点是：</div><div class="line">    2.1、 /：存储系统文件，建议10GB ~ 15GB,我分配16G；</div><div class="line">    2.2、 swap：交换分区，即Linux系统的虚拟内存，建议是物理内存的2倍,我分配16G；</div><div class="line">    2.3、 /home：建议最后分配所有剩下的空间；</div><div class="line">    2.4、 boot：包含系统内核和系统启动所需的文件，实现双系统的关键所在，建议200M,我分配400M。</div><div class="line">3、其他的默认或者根据自己需求设置，点击安装</div></pre></td></tr></table></figure>
<h3 id="5、分辨率问题"><a href="#5、分辨率问题" class="headerlink" title="5、分辨率问题"></a>5、分辨率问题</h3><p>一般比较新的N（英伟达）卡会出现没有安装驱动的问题，所以屏幕的分率很低，这时候就需要安装N卡驱动。直接安装会导致开机的时候卡在登陆界面进不去，所以必须借助于bumblebee(大黄蜂)，至于原因有兴趣的可以查一查，这里不多阐述。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install bumblebee bumblebee-nvidia primus linux-headers-generic</div><div class="line">Reboot</div></pre></td></tr></table></figure></p>
<p>重新启动后：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sudo apt-get purge nvidia-* #删除所有的N卡驱动</div><div class="line">sudo add-apt-repository ppa:graphics-drivers/ppa  #添加第三方驱动源</div><div class="line">sudo apt-get update #更新源</div><div class="line">sudo  apt-cache search nvidia-*  #查询nvidia驱动可用版本，这里推荐到英伟达官网查看自己显卡驱动的版本，我的是375</div><div class="line">sudo apt-get install nvidia-375 # 安装驱动</div></pre></td></tr></table></figure></p>
<p>打开软件更新器，然后将附加驱动－&gt;未知换成显卡的驱动<br><br>最后重新启动，什么都不用做，等开机！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在win10下安装ubuntu双系统。在笔记本安装ubuntu的时候遇到了很多的挫折，曾经也放弃过，但很不幸的是，在未来的今天又碰上了。有时候问题的答案很简单，但是需要大量的时间去得到它，并不是你的能力不行，而是网上的干扰答案实在是太多，无法分辨谁对谁假的时候，往往会一个个的试过去。我不认为这是个很笨的方法，因为多花点时间总能体会更多的东西。貌似废话有点多，下面直接上干货。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="ubuntu" scheme="http://jchanji.github.io/tags/ubuntu/"/>
    
      <category term="win 10" scheme="http://jchanji.github.io/tags/win-10/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7 安装vnc</title>
    <link href="http://jchanji.github.io/year/09/03/vnc/"/>
    <id>http://jchanji.github.io/year/09/03/vnc/</id>
    <published>2017-09-03T06:44:05.306Z</published>
    <updated>2017-09-03T07:08:42.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-首先执行这一句防止系统文件被修改"><a href="#1-首先执行这一句防止系统文件被修改" class="headerlink" title="1.首先执行这一句防止系统文件被修改"></a>1.首先执行这一句防止系统文件被修改</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">chattr +i /etc/resolv.conf</div></pre></td></tr></table></figure>
<h2 id="2、然后安装tigervnc"><a href="#2、然后安装tigervnc" class="headerlink" title="2、然后安装tigervnc"></a>2、然后安装tigervnc</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install -y tigervnc tigervnc-server</div></pre></td></tr></table></figure>
<a id="more"></a>
<h2 id="3-查看自己的服务器支持安装哪些包"><a href="#3-查看自己的服务器支持安装哪些包" class="headerlink" title="3.查看自己的服务器支持安装哪些包"></a>3.查看自己的服务器支持安装哪些包</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum grouplist</div></pre></td></tr></table></figure>
<h4 id="查看自己的服务器里中Available-Environment-Groups下面有哪些可以安装的Desktop-我这里的是GNOME-Desktop"><a href="#查看自己的服务器里中Available-Environment-Groups下面有哪些可以安装的Desktop-我这里的是GNOME-Desktop" class="headerlink" title="查看自己的服务器里中Available Environment Groups下面有哪些可以安装的Desktop,我这里的是GNOME Desktop"></a>查看自己的服务器里中Available Environment Groups下面有哪些可以安装的Desktop,我这里的是GNOME Desktop</h4><h2 id="4-安装GNOME-Desktop"><a href="#4-安装GNOME-Desktop" class="headerlink" title="4. 安装GNOME Desktop"></a>4. 安装GNOME Desktop</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum groupinstall GNOME Desktop</div></pre></td></tr></table></figure>
<h2 id="5-启动服务"><a href="#5-启动服务" class="headerlink" title="5. 启动服务"></a>5. 启动服务</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vncserver</div></pre></td></tr></table></figure>
<h2 id="6-连接vnc"><a href="#6-连接vnc" class="headerlink" title="6.连接vnc"></a>6.连接vnc</h2><blockquote>
<p>第一次执行会提示输入密码，然后再验证输入一次回车，vnc服务端就算搭建好了！<br>接下来在手机或者电脑上下载vnc客户端，输入你的IP:5901连接<br>然后输入密码就可以看到你的服务器界面了！</p>
</blockquote>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-首先执行这一句防止系统文件被修改&quot;&gt;&lt;a href=&quot;#1-首先执行这一句防止系统文件被修改&quot; class=&quot;headerlink&quot; title=&quot;1.首先执行这一句防止系统文件被修改&quot;&gt;&lt;/a&gt;1.首先执行这一句防止系统文件被修改&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;chattr +i /etc/resolv.conf&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;2、然后安装tigervnc&quot;&gt;&lt;a href=&quot;#2、然后安装tigervnc&quot; class=&quot;headerlink&quot; title=&quot;2、然后安装tigervnc&quot;&gt;&lt;/a&gt;2、然后安装tigervnc&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;sudo yum install -y tigervnc tigervnc-server&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="CentOS" scheme="http://jchanji.github.io/tags/CentOS/"/>
    
      <category term="vnc" scheme="http://jchanji.github.io/tags/vnc/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 7 安装eclipse mars 2</title>
    <link href="http://jchanji.github.io/year/09/03/install_eclipse/"/>
    <id>http://jchanji.github.io/year/09/03/install_eclipse/</id>
    <published>2017-09-03T06:40:47.978Z</published>
    <updated>2017-09-03T07:02:49.922Z</updated>
    
    <content type="html"><![CDATA[<h4 id="操作系统：CentOS-7"><a href="#操作系统：CentOS-7" class="headerlink" title="操作系统：CentOS 7"></a>操作系统：CentOS 7</h4><h4 id="eclispe版本：Eclipse-Mars-2"><a href="#eclispe版本：Eclipse-Mars-2" class="headerlink" title="eclispe版本：Eclipse Mars 2"></a>eclispe版本：Eclipse Mars 2</h4><a id="more"></a>
<h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><blockquote>
<h3 id="1-下载安装eclipse"><a href="#1-下载安装eclipse" class="headerlink" title="1.下载安装eclipse"></a>1.下载安装<a href="http://mirrors.ustc.edu.cn/eclipse/technology/epp/downloads/release/mars/2/eclipse-jee-mars-2-linux-gtk-x86_64.tar.gz" title="eclise下载" target="_blank" rel="external">eclipse</a></h3><h3 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h3></blockquote>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo tar -zxvf [下载的安装包名称] -C [安装的目录]</div></pre></td></tr></table></figure>
<blockquote>
<h3 id="3-创建软链接"><a href="#3-创建软链接" class="headerlink" title="3.创建软链接"></a>3.创建软链接</h3></blockquote>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo ln -s /安装的目录/eclipse/eclipse  /usr/bin/eclipse</div></pre></td></tr></table></figure>
<blockquote>
<h3 id="4-添加图标"><a href="#4-添加图标" class="headerlink" title="4.添加图标"></a>4.添加图标</h3></blockquote>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gedit /usr/share/applications/eclipse.desktop</div></pre></td></tr></table></figure>
<p>将下面内容添加到文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[Desktop Entry]</div><div class="line">Encoding=UTF-8</div><div class="line">Name=Eclipse</div><div class="line">Comment=Eclipse Mar2</div><div class="line">Exec=/usr/bin/eclipse</div><div class="line">Icon=/[解压的目录]/eclipse/icon.xpm</div><div class="line">Categories=Application;Development;Java;IDE</div><div class="line">Version=1.0</div><div class="line">Type=Application</div><div class="line">Terminal=0</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;操作系统：CentOS-7&quot;&gt;&lt;a href=&quot;#操作系统：CentOS-7&quot; class=&quot;headerlink&quot; title=&quot;操作系统：CentOS 7&quot;&gt;&lt;/a&gt;操作系统：CentOS 7&lt;/h4&gt;&lt;h4 id=&quot;eclispe版本：Eclipse-Mars-2&quot;&gt;&lt;a href=&quot;#eclispe版本：Eclipse-Mars-2&quot; class=&quot;headerlink&quot; title=&quot;eclispe版本：Eclipse Mars 2&quot;&gt;&lt;/a&gt;eclispe版本：Eclipse Mars 2&lt;/h4&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="eclispe" scheme="http://jchanji.github.io/tags/eclispe/"/>
    
      <category term="CentOS 7" scheme="http://jchanji.github.io/tags/CentOS-7/"/>
    
  </entry>
  
  <entry>
    <title>伪分布式hbase安装配置</title>
    <link href="http://jchanji.github.io/year/08/30/hbase_step/"/>
    <id>http://jchanji.github.io/year/08/30/hbase_step/</id>
    <published>2017-08-30T14:36:02.631Z</published>
    <updated>2017-09-03T06:37:53.568Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>网上有很多的教程，大体流程都差不多，但是在很多细节配置方面有点区别，本教程适用于伪分布式环境下（一般自己电脑上练习伪分布式够了）的hbase的基本安装配置。hadoop伪分布式环境已经搭建好,如果没有搭建好，推荐教程 <a href="http://www.powerxing.com/install-hadoop-in-centos/" target="_blank" rel="external">hadoop伪分布式教程</a>,hbase官方<a href="http://abloz.com/hbase/book.html" target="_blank" rel="external">中文文档</a></p>
</blockquote>
<a id="more"></a>
<h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol>
<li>CentOS7</li>
<li>jdk:openjdk1.7.0_141</li>
<li>hadoop：2.6.0</li>
<li>hbase:0.98.13</li>
<li>一定要注意jdk,hadoop和hbase的版本匹配问题,可到官网查看！</li>
</ol>
<h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2><p>1.<a href="http://archive.apache.org/dist/hbase/0.98.13/hbase-0.98.13-hadoop2-bin.tar.gz" target="_blank" rel="external">hbase-0.98.13-hadoop2-bin.tar.gz</a></p>
<h2 id="三、安装配置"><a href="#三、安装配置" class="headerlink" title="三、安装配置"></a>三、安装配置</h2><h3 id="1、解压文件到指定目录"><a href="#1、解压文件到指定目录" class="headerlink" title="1、解压文件到指定目录"></a>1、解压文件到指定目录</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">tar -zxvf hbase-0.98.13-hadoop2-bin.tar.gz -C /usr/local</div></pre></td></tr></table></figure>
<h3 id="2、重命名"><a href="#2、重命名" class="headerlink" title="2、重命名"></a>2、重命名</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /usr/local</div><div class="line">sudo mv [解压后的文件名] [hbase]</div></pre></td></tr></table></figure>
<h3 id="3、修改hbase-site-xml"><a href="#3、修改hbase-site-xml" class="headerlink" title="3、修改hbase-site.xml"></a>3、修改hbase-site.xml</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd /hbase/conf</div><div class="line">sudo vim hbase-site.xml</div></pre></td></tr></table></figure>
<p>将内容改为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></div><div class="line"><span class="xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></div><div class="line"><span class="code">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span></div><div class="line"><span class="code">    &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span></div><div class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></div><div class="line">  <span class="xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></div><div class="line"><span class="code">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span></div><div class="line"><span class="code">    &lt;value&gt;/usr/local/hbase/data/zkData&lt;/value&gt;</span></div><div class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></div><div class="line"><span class="xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></div><div class="line"><span class="code">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span></div><div class="line"><span class="code">    &lt;value&gt;true&lt;/value&gt;</span></div><div class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></div><div class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></div></pre></td></tr></table></figure></p>
<p>说明：<br><br>1、很多教程的hbase.rootdir的hdfs的端口都和官网配置一样是8020，这里根据你自己的实际端口号配置，我的默认的为9000（一般都是），如果端口配置错误的话，之后的进程都能启动，但是在hdfs中没有创建hbase文件，也不能通过60010端口访问web UI.<br><br>2、dataDir的目录可以自己定义，不需要预先创建，hbase会根据配置自动生成。</p>
<h3 id="3、修改hbase-env-sh"><a href="#3、修改hbase-env-sh" class="headerlink" title="3、修改hbase-env.sh"></a>3、修改hbase-env.sh</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo vim hbase-env.sh</div></pre></td></tr></table></figure>
<p>添加自己的JAVA_HOME路径<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk/</div></pre></td></tr></table></figure></p>
<h3 id="4、修改regionservers"><a href="#4、修改regionservers" class="headerlink" title="4、修改regionservers"></a>4、修改regionservers</h3><p>在/etc/hosts文件中添加主机名映射，再regionservers中默认的localhost改为主机名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo vim etc/hosts</div></pre></td></tr></table></figure></p>
<p>在最后一行添加 127.0.0.1 master<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo vim regionservers</div></pre></td></tr></table></figure></p>
<p>将localhost改为mater<br><br>说明：如果ip映射出现问题后面的regionserver会启动不了</p>
<h3 id="5、启动服务"><a href="#5、启动服务" class="headerlink" title="5、启动服务"></a>5、启动服务</h3><p>首先先启动hadoop<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">start-all.sh</div></pre></td></tr></table></figure></p>
<p>再启动hbase<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">cd /usr/local/hbase/bin</div><div class="line">./hbase-daemon.sh start zookeeper</div><div class="line">./hbase-daemon.sh start regionserver</div><div class="line">./hbase-daemon.sh start master</div></pre></td></tr></table></figure></p>
<h3 id="6、查看web-UI"><a href="#6、查看web-UI" class="headerlink" title="6、查看web UI"></a>6、查看web UI</h3><p>在浏览器中输入localhost:60010<br><br>如果能正常显示页面说明配置成功<br><br>说明：刚开启服务后由于hadoop处于安全模式导致不能访问，可以等几十秒再次访问或者通过命令<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hadoop dfsadmin -safemode leave</div></pre></td></tr></table></figure></p>
<p>解除保护</p>
<h2 id="四、常用的一些命令"><a href="#四、常用的一些命令" class="headerlink" title="四、常用的一些命令"></a>四、常用的一些命令</h2><h3 id="1、从hdfs导入导出表"><a href="#1、从hdfs导入导出表" class="headerlink" title="1、从hdfs导入导出表"></a>1、从hdfs导入导出表</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">1）导入</div><div class="line">./hbase org.apache.hadoop.hbase.mapreduce.Driver import 表名    数据文件位置</div><div class="line"></div><div class="line">2)导出</div><div class="line">./hbase org.apache.hadoop.hbase.mapreduce.Driver export 表名    数据文件位置</div></pre></td></tr></table></figure>
<p>注意：直接操作会报没有jar包的错误，根据提示将hbase的jar包put进提示的hdfs路径中即可</p>
<h2 id="五、遇到的错误和解决办法"><a href="#五、遇到的错误和解决办法" class="headerlink" title="五、遇到的错误和解决办法"></a>五、遇到的错误和解决办法</h2><h3 id="1、无法启动HRegionServer和HMaster"><a href="#1、无法启动HRegionServer和HMaster" class="headerlink" title="1、无法启动HRegionServer和HMaster"></a>1、无法启动HRegionServer和HMaster</h3><p>报错日志<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">2017-06-13 19:10:12,458 ERROR [main] master.HMasterCommandLine: Master exiting</div><div class="line">java.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster</div><div class="line">  at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:3033)</div><div class="line">  at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:193)</div><div class="line">  at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:135)</div><div class="line">  at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)</div><div class="line">  at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)</div><div class="line">  at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:3047)</div><div class="line">Caused by: java.net.BindException: 无法指定被请求的地址</div><div class="line">  at sun.nio.ch.Net.bind0(Native Method)</div><div class="line">  at sun.nio.ch.Net.bind(Net.java:463)</div><div class="line">  at sun.nio.ch.Net.bind(Net.java:455)</div><div class="line">  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)</div><div class="line">  at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)</div><div class="line">  at org.apache.hadoop.hbase.ipc.RpcServer.bind(RpcServer.java:2488)</div><div class="line">  at org.apache.hadoop.hbase.ipc.RpcServer$Listener.<span class="xml"><span class="tag">&lt;<span class="name">init</span>&gt;</span></span>(RpcServer.java:590)</div><div class="line">  at org.apache.hadoop.hbase.ipc.RpcServer.<span class="xml"><span class="tag">&lt;<span class="name">init</span>&gt;</span></span>(RpcServer.java:1956)</div><div class="line">  at org.apache.hadoop.hbase.master.HMaster.<span class="xml"><span class="tag">&lt;<span class="name">init</span>&gt;</span></span>(HMaster.java:507)</div><div class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</div><div class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</div><div class="line">  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</div><div class="line">  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</div><div class="line">  at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:3028)</div><div class="line">  ... 5 more</div></pre></td></tr></table></figure></p>
<p>解决办法<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">我们可以看到Caused by: java.net.BindException: 无法指定被请求的地址，所以有可能是外网的的影响，所以先关闭网络连接，再启动服务，发现成功了，然后再开启网络。</div></pre></td></tr></table></figure></p>
<h3 id="2、启动hbase服务时找不到pid文件"><a href="#2、启动hbase服务时找不到pid文件" class="headerlink" title="2、启动hbase服务时找不到pid文件"></a>2、启动hbase服务时找不到pid文件</h3><p>问题原因<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="bullet">1. </span>hbase进行大量的插入时region server 所分配的内存堆过小</div><div class="line"><span class="bullet">2. </span>pid文件保存在tmp目录下容易丢失。</div></pre></td></tr></table></figure></p>
<p>解决办法<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="bullet">1. </span>在hb的hbase-env.sh中</div><div class="line"><span class="section"># The maximum amount of heap to use, in MB. Default is 1000.</span></div><div class="line"><span class="section"># export HBASE_HEAPSIZE=1000</span></div><div class="line">将1000改成30720</div><div class="line"></div><div class="line"><span class="bullet">2. </span>在hbase-env.sh中修改pid文件的存放路径：</div><div class="line">在hbase-env.sh中下面的文字默认是注释掉的，放开即可，也可以自己指定存放位置：</div><div class="line"><span class="section"># The directory where pid files are stored. /tmp by default.  </span></div><div class="line"> export HBASE<span class="emphasis">_PID_</span>DIR=/var/hadoop/pids</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;网上有很多的教程，大体流程都差不多，但是在很多细节配置方面有点区别，本教程适用于伪分布式环境下（一般自己电脑上练习伪分布式够了）的hbase的基本安装配置。hadoop伪分布式环境已经搭建好,如果没有搭建好，推荐教程 &lt;a href=&quot;http://www.powerxing.com/install-hadoop-in-centos/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;hadoop伪分布式教程&lt;/a&gt;,hbase官方&lt;a href=&quot;http://abloz.com/hbase/book.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;中文文档&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
      <category term="hbase" scheme="http://jchanji.github.io/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>使用java将文件夹下的文件批量的从gbk编码转化成utf-8编码</title>
    <link href="http://jchanji.github.io/year/08/30/codeparse_gbk2utf/"/>
    <id>http://jchanji.github.io/year/08/30/codeparse_gbk2utf/</id>
    <published>2017-08-30T14:32:48.163Z</published>
    <updated>2017-09-03T05:51:28.647Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>使用java,对文件遍历，修改文件编码</p>
</blockquote>
<a id="more"></a>
<h3 id="一、建立java项目，导入commons-io-jar"><a href="#一、建立java项目，导入commons-io-jar" class="headerlink" title="一、建立java项目，导入commons-io-*.jar"></a>一、建立java项目，导入<a href="http://mirror.bit.edu.cn/apache//commons/io/binaries/commons-io-2.5-bin.zip" title="commons-io-*.jar" target="_blank" rel="external">commons-io-*.jar</a></h3><h3 id="二、新建class，文件名随便起，我的是Codeparse-包名为exchangecode"><a href="#二、新建class，文件名随便起，我的是Codeparse-包名为exchangecode" class="headerlink" title="二、新建class，文件名随便起，我的是Codeparse,包名为exchangecode"></a>二、新建class，文件名随便起，我的是Codeparse,包名为exchangecode</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">package exchangecode;</div><div class="line"></div><div class="line">import java.io.File;</div><div class="line">import java.io.IOException;</div><div class="line">import java.util.Collection;</div><div class="line"></div><div class="line">import org.apache.commons.io.FileUtils;</div><div class="line"></div><div class="line">public class Codeparse &#123;</div><div class="line"></div><div class="line"><span class="code">    public static void main(String[] args) throws IOException &#123;</span></div><div class="line"><span class="code">        //GBK编码格式源码路径,根据自己的文件路径写 </span></div><div class="line"><span class="code">        String srcDirPath = "F:\\test"; </span></div><div class="line"><span class="code">        //转为UTF-8编码格式源码路径，根据自己的文件路径写 </span></div><div class="line"><span class="code">        String utf8DirPath ="F:\\out"; </span></div><div class="line"><span class="code">                </span></div><div class="line"><span class="code">        //获取所有txt文件,如果是其他类型的文件，将&#123;“txt”&#125;中的txt换为其他文件的后缀名</span></div><div class="line"><span class="code">        @SuppressWarnings("unchecked")</span></div><div class="line"><span class="code">        Collection&lt;File&gt; javaGbkFileCol =  FileUtils.listFiles(new File(srcDirPath), new String[]&#123;"txt"&#125;, true); </span></div><div class="line"><span class="code">                </span></div><div class="line"><span class="code">        for (File javaGbkFile : javaGbkFileCol) &#123; </span></div><div class="line"><span class="code">              //UTF8格式文件路径 </span></div><div class="line"><span class="code">              String utf8FilePath = utf8DirPath+javaGbkFile.getAbsolutePath().substring(srcDirPath.length()); </span></div><div class="line"><span class="code">              </span></div><div class="line"><span class="code">              //使用GBK读取数据，然后用UTF-8写入数据 </span></div><div class="line"><span class="code">              FileUtils.writeLines(new File(utf8FilePath), "UTF-8", FileUtils.readLines(javaGbkFile, "GBK"));        </span></div><div class="line"><span class="code">        &#125;</span></div><div class="line"><span class="code">        System.out.println("success!");</span></div><div class="line"><span class="code">    &#125;</span></div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="三、运行"><a href="#三、运行" class="headerlink" title="三、运行"></a>三、运行</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;使用java,对文件遍历，修改文件编码&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="-java" scheme="http://jchanji.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu16.04 anaconda环境下安装tensoflow(GPU)</title>
    <link href="http://jchanji.github.io/year/08/29/tensorflow_step/"/>
    <id>http://jchanji.github.io/year/08/29/tensorflow_step/</id>
    <published>2017-08-29T13:38:28.881Z</published>
    <updated>2017-09-03T06:36:41.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote>
<p>目前深度学习炙手可热的框架毫无疑问是tensorflow,在本教程主要介绍tensorflow在anaconda中的安装，在火车上实在是无聊，电脑又没有网络，只能打发一下时间。</p>
</blockquote>
<a id="more"></a>
<h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol>
<li>anaconda 4.3.21</li>
<li>python 3.5</li>
<li>tensorflow 1.2.0(github上目前最新版本)</li>
<li>ubuntu 16.04<h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2></li>
<li><a href="">Anaconda3-4.4.0-Linux-x86_64.sh</a></li>
<li><a href="">tensorflow_gpu-1.2.1-cp35-cp35m-linux_x86_64.whl</a><br>##三、注意事项<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">1、电脑上已经安装了cadu8.0和cudnn5.1环境</div><div class="line">2、tensorflow1.2.0版本支持cadu8.0,其他低版本的tensorflow会发生找不到依赖的错误。</div><div class="line">３、安装后运行会出现CPU computations,cpu指令集优化的警告，目前没有很好的解决办法，不过影响不大，因为我们主要使用的是GPU.</div></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="四、安装配置"><a href="#四、安装配置" class="headerlink" title="四、安装配置"></a>四、安装配置</h2><h3 id="1、安装anaconda"><a href="#1、安装anaconda" class="headerlink" title="1、安装anaconda"></a>1、安装anaconda</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo bash Anaconda3-4.4.0-Linux-x86_64.sh</div></pre></td></tr></table></figure>
<h3 id="2、-安装python3-5环境"><a href="#2、-安装python3-5环境" class="headerlink" title="2、　安装python3.5环境"></a>2、　安装python3.5环境</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda create -n tensorflow python = 3.5</div></pre></td></tr></table></figure>
<h3 id="3、安装tensorflow"><a href="#3、安装tensorflow" class="headerlink" title="3、安装tensorflow"></a>3、安装tensorflow</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">source activate tensorflow #进入刚才安装好的环境</div><div class="line">cd ~/下载　＃进入tensorflow　的pip安装文件的目录</div><div class="line">pip install tensorflow<span class="emphasis">_gpu-1.2.1-cp35-cp35m-linux_</span>x86_64.whl #安装tensorflow</div></pre></td></tr></table></figure>
<h2 id="五、测试"><a href="#五、测试" class="headerlink" title="五、测试"></a>五、测试</h2><p>进入python环境<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">python</div></pre></td></tr></table></figure></p>
<p>运行代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line">sess = tf.Session()</div><div class="line"></div><div class="line">a = tf.constant(<span class="number">10</span>)</div><div class="line">b = tf.constant(<span class="number">20</span>)</div><div class="line"></div><div class="line">print(sess.run(a+b))</div></pre></td></tr></table></figure></p>
<p>输出结果<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">3</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;目前深度学习炙手可热的框架毫无疑问是tensorflow,在本教程主要介绍tensorflow在anaconda中的安装，在火车上实在是无聊，电脑又没有网络，只能打发一下时间。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="deeplearning" scheme="http://jchanji.github.io/tags/deeplearning/"/>
    
      <category term="tensorflow" scheme="http://jchanji.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>机器学习-无监督学习-聚类K-means算法-对31省消费水平分类</title>
    <link href="http://jchanji.github.io/year/08/29/citycosumption/"/>
    <id>http://jchanji.github.io/year/08/29/citycosumption/</id>
    <published>2017-08-29T13:17:37.902Z</published>
    <updated>2017-09-03T06:35:05.356Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><blockquote>
<p>此篇笔记主要根据南京大学礼欣老师的<a href="http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce" target="_blank" rel="external">《Python机器学习应用》</a>整理而成，详细内容请看礼欣老师的mooc课程。</p>
</blockquote>
<a id="more"></a>
<h2 id="数据介绍："><a href="#数据介绍：" class="headerlink" title="数据介绍："></a>数据介绍：</h2><p>现有1999年全国31个省份城镇居民家庭平均每人全年消费性支出的八个主<br>要变量数据，这八个变量分别是：食品、衣着、家庭设备用品及服务、医疗<br>保健、交通和通讯、娱乐教育文化服务、居住以及杂项商品和服务。利用已<br>有数据，对31个省份进行聚类。。数据下载<a href="https://github.com/jChanJi/static_resource/blob/master/clustering/TestData.txt" target="_blank" rel="external">点击我</a></p>
<h2 id="主要参数"><a href="#主要参数" class="headerlink" title="主要参数"></a>主要参数</h2><ol>
<li>n_clusters：用于指定聚类中心的个数</li>
<li>init：初始聚类中心的初始化方法</li>
<li>max_iter：最大的迭代次数</li>
<li>一般调用时只用给出n_clusters即可，init<br>默认是k-means++，max_iter默认是300</li>
<li>data：加载的数据</li>
<li>label：聚类后各数据所属的标签</li>
<li>axis: 按行求和</li>
<li>fit_predict()：计算簇中心以及为簇分配序号</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"> </div><div class="line"> </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filePath)</span>:</span></div><div class="line">    fr = open(filePath,<span class="string">'r+'</span>)</div><div class="line">    lines = fr.readlines()</div><div class="line">    retData = []</div><div class="line">    retCityName = []</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</div><div class="line">        items = line.strip().split(<span class="string">","</span>)</div><div class="line">        retCityName.append(items[<span class="number">0</span>])</div><div class="line">        retData.append([float(items[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(items))])</div><div class="line">    <span class="keyword">return</span> retData,retCityName</div><div class="line"> </div><div class="line">     </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</div><div class="line">    data,cityName = loadData(<span class="string">'F:/data/clustering/city.txt'</span>)</div><div class="line">    km = KMeans(n_clusters=<span class="number">4</span>)</div><div class="line">    label = km.fit_predict(data)</div><div class="line">    expenses = np.sum(km.cluster_centers_,axis=<span class="number">1</span>)</div><div class="line">    <span class="comment">#print(expenses)</span></div><div class="line">    CityCluster = [[],[],[],[]]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(cityName)):</div><div class="line">        CityCluster[label[i]].append(cityName[i])</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(CityCluster)):</div><div class="line">        print(<span class="string">"Expenses:%.2f"</span> % expenses[i])</div><div class="line">        print(CityCluster[i])</div></pre></td></tr></table></figure>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">Expenses:4441.04</div><div class="line">['安徽', '湖南', '湖北', '广西', '海南', '四川', '云南']</div><div class="line">Expenses:7754.66</div><div class="line">['北京', '上海', '广东']</div><div class="line">Expenses:5567.33</div><div class="line">['天津', '江苏', '浙江', '福建', '重庆', '西藏']</div><div class="line">Expenses:3788.76</div><div class="line">['河北', '山西', '内蒙古', '辽宁', '吉林', '黑龙江', '江西', '山东', '河南', '贵州', '陕西', '甘肃', '青海', '宁夏', '新疆']</div></pre></td></tr></table></figure>
<h2 id="注：当改变簇n-clusters为8-CityCluster长度也设置为8-时结果"><a href="#注：当改变簇n-clusters为8-CityCluster长度也设置为8-时结果" class="headerlink" title="注：当改变簇n_clusters为8(CityCluster长度也设置为8)时结果"></a>注：当改变簇n_clusters为8(CityCluster长度也设置为8)时结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">Expenses:<span class="number">3497.85</span></div><div class="line">[<span class="string">'山西'</span>, <span class="string">'内蒙古'</span>, <span class="string">'黑龙江'</span>, <span class="string">'河南'</span>, <span class="string">'宁夏'</span>]</div><div class="line">Expenses:<span class="number">5311.98</span></div><div class="line">[<span class="string">'天津'</span>, <span class="string">'江苏'</span>, <span class="string">'重庆'</span>, <span class="string">'云南'</span>]</div><div class="line">Expenses:<span class="number">7010.02</span></div><div class="line">[<span class="string">'北京'</span>, <span class="string">'浙江'</span>]</div><div class="line">Expenses:<span class="number">7517.80</span></div><div class="line">[<span class="string">'广东'</span>]</div><div class="line">Expenses:<span class="number">4357.67</span></div><div class="line">[<span class="string">'安徽'</span>, <span class="string">'湖南'</span>, <span class="string">'湖北'</span>, <span class="string">'广西'</span>, <span class="string">'海南'</span>, <span class="string">'四川'</span>]</div><div class="line">Expenses:<span class="number">5287.90</span></div><div class="line">[<span class="string">'福建'</span>, <span class="string">'西藏'</span>]</div><div class="line">Expenses:<span class="number">8247.69</span></div><div class="line">[<span class="string">'上海'</span>]</div><div class="line">Expenses:<span class="number">3934.21</span></div><div class="line">[<span class="string">'河北'</span>, <span class="string">'辽宁'</span>, <span class="string">'吉林'</span>, <span class="string">'江西'</span>, <span class="string">'山东'</span>, <span class="string">'贵州'</span>, <span class="string">'陕西'</span>, <span class="string">'甘肃'</span>, <span class="string">'青海'</span>, <span class="string">'新疆'</span>]</div></pre></td></tr></table></figure>
<p>我们发现簇多所分的层次就越多</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;此篇笔记主要根据南京大学礼欣老师的&lt;a href=&quot;http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;《Python机器学习应用》&lt;/a&gt;整理而成，详细内容请看礼欣老师的mooc课程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="python" scheme="http://jchanji.github.io/tags/python/"/>
    
      <category term="machinelearning" scheme="http://jchanji.github.io/tags/machinelearning/"/>
    
      <category term="K-means" scheme="http://jchanji.github.io/tags/K-means/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7安装配置hadoop集群</title>
    <link href="http://jchanji.github.io/year/08/29/hadoop/"/>
    <id>http://jchanji.github.io/year/08/29/hadoop/</id>
    <published>2017-08-29T11:10:45.044Z</published>
    <updated>2017-09-03T06:37:47.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="简单的的vim命令"><a href="#简单的的vim命令" class="headerlink" title="简单的的vim命令"></a><a href="http://www.cnblogs.com/jeakon/archive/2012/08/13/2816802.html" target="_blank" rel="external">简单的的vim命令</a></h3><h3 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a><a href="http://www.weixuehao.com/archives/25" target="_blank" rel="external">linux常用命令</a></h3><h3 id="linux命令查找网站"><a href="#linux命令查找网站" class="headerlink" title="linux命令查找网站"></a><a href="http://man.linuxde.net/" target="_blank" rel="external">linux命令查找网站</a></h3><h1 id="CentOS-下安装hadoop"><a href="#CentOS-下安装hadoop" class="headerlink" title="CentOS 下安装hadoop"></a>CentOS 下安装hadoop<br></h1><a id="more"></a>
<h2 id="一、安装Vmware-12"><a href="#一、安装Vmware-12" class="headerlink" title="一、安装Vmware 12"></a>一、安装Vmware 12</h2><blockquote>
<ol>
<li>官网下载<a href="http://www.vmware.com/cn/products/workstation/workstation-evaluation.html" title="Vmware 下载地址" target="_blank" rel="external">VMware-Workstation-Full-*.bundle</a></li>
<li>sudo ssh./VMware-Workstation-Full-*.bundle</li>
<li>破解：破解工具<a href="http://chanji-1252400803.costj.myqcloud.com/VMware12.Keymaker.exe" title="VMware12.Keymaker" target="_blank" rel="external">VMware12.Keymaker</a> </li>
<li>根据提示安装</li>
</ol>
</blockquote>
<h2 id="二、安装CentOS-7"><a href="#二、安装CentOS-7" class="headerlink" title="二、安装CentOS 7"></a>二、安装CentOS 7</h2><blockquote>
<ol>
<li>下载<a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso" title="CentOS 7" target="_blank" rel="external">镜像</a></li>
<li>新建虚拟机，根据提示操作（注意选择安装GNOME桌面），设置主机名为CentOSMaster点击安装</li>
<li>设置root密码和添加hadoop用户（设置为管理员）</li>
<li>等待安装，完成后重启，连接网络，完成配置 </li>
<li>语言选择汉语（pinyin）</li>
</ol>
</blockquote>
<h2 id="三、-安装hadoop集群"><a href="#三、-安装hadoop集群" class="headerlink" title="三、 安装hadoop集群"></a>三、 安装hadoop集群</h2><h4 id="参考教程："><a href="#参考教程：" class="headerlink" title="参考教程："></a>参考教程：</h4><h4 id="单机-伪分布式：http-www-powerxing-com-install-hadoop-in-centos"><a href="#单机-伪分布式：http-www-powerxing-com-install-hadoop-in-centos" class="headerlink" title="单机/伪分布式：http://www.powerxing.com/install-hadoop-in-centos/"></a>单机/伪分布式：<a href="http://www.powerxing.com/install-hadoop-in-centos/" target="_blank" rel="external">http://www.powerxing.com/install-hadoop-in-centos/</a></h4><h4 id="分布式集群：http-www-powerxing-com-install-hadoop-cluster"><a href="#分布式集群：http-www-powerxing-com-install-hadoop-cluster" class="headerlink" title="分布式集群：http://www.powerxing.com/install-hadoop-cluster/"></a>分布式集群：<a href="http://www.powerxing.com/install-hadoop-cluster/" target="_blank" rel="external">http://www.powerxing.com/install-hadoop-cluster/</a></h4><blockquote>
<h3 id="1-创建hadoop用户-如果没有"><a href="#1-创建hadoop用户-如果没有" class="headerlink" title="1. 创建hadoop用户(如果没有)"></a>1. 创建hadoop用户(如果没有)</h3></blockquote>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="bullet">1. </span>su                               # 上述提到的以 root 用户登录</div><div class="line"><span class="bullet">2. </span>useradd -m hadoop -s /bin/bash   # 创建新用户hadoop</div><div class="line"><span class="bullet">3. </span>passwd hadoop                    #设置密码</div><div class="line"><span class="bullet">4. </span>visudo                           #增加管理员权限</div></pre></td></tr></table></figure>
<p>找到 root  ALL=(ALL)  ALL 这行,下一行增加:hadoop ensp; ensp; ALL=(ALL)  ensp;ensp; ALL<br></p>
<blockquote>
<h3 id="2-安装Java环境-在hadoop用户下"><a href="#2-安装Java环境-在hadoop用户下" class="headerlink" title="2. 安装Java环境(在hadoop用户下)"></a>2. 安装Java环境(在hadoop用户下)</h3></blockquote>
<ol>
<li>安装openjdk<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel</div></pre></td></tr></table></figure>
</li>
</ol>
<p>如果遇到yum进程被占用，删除yum.pid<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo rm -rf /var/run/yum.pid</div></pre></td></tr></table></figure></p>
<ol>
<li><p>配置JAVA_HOME<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim ~/.bashrc</div></pre></td></tr></table></figure>
<p> 在文件最后面添加如下单独一行（指向 JDK 的安装位置)<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk</div></pre></td></tr></table></figure>
</li>
<li><p>使配置生效<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">source ~/.bashrc</div></pre></td></tr></table></figure>
</li>
<li><p>检验是否配置成功<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">echo $JAVA_HOME  #检验变量值</div><div class="line">java -version </div><div class="line">   %JAVA_HOME/bin/java -version</div></pre></td></tr></table></figure>
</li>
</ol>
<p>如果java -version 和 %JAVA_HOME/bin/java -version一样表示成功,否则看5<br><br></p>
<ol>
<li><p>如果和以前的jdk版本冲突的:<br><br> 查找当前的安装的jdk版本<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="code">    rpm -q |grep java</span></div><div class="line"><span class="code">```  </span></div><div class="line"><span class="code">    删除openjdk版本意外的版本&lt;br&gt;</span></div><div class="line"><span class="code">```markdown</span></div><div class="line"><span class="code">    rpm -e --nodeps java版本的名称</span></div><div class="line"><span class="code">```  </span></div><div class="line"><span class="code"></span></div><div class="line"><span class="code">&gt;### 3.安装配置hadoop2集群</span></div><div class="line"><span class="code"></span></div><div class="line"><span class="code">1. 下载hadoop压缩包，选择[hadoop-2.x.y.tar.gz][5]文件,这里我选择的是2.6.1版本&lt;br&gt;&lt;br&gt;</span></div><div class="line"><span class="code">2. 解压&lt;br&gt;</span></div><div class="line"><span class="code">```markdown</span></div><div class="line"><span class="code">    sudo tar -zxf ~/下载/hadoop-2.6.1.tar.gz -C /usr/local    # 解压到/usr/local中</span></div><div class="line"><span class="code">    cd /usr/local/  #打开/usr/local目录</span></div><div class="line"><span class="code">    sudo mv ./hadoop-2.6.1/ ./hadoop  # 将文件夹名改为hadoop</span></div><div class="line"><span class="code">    sudo chown -R hadoop:hadoop ./hadoop  # 修改文件权限，冒号后没有空格</span></div></pre></td></tr></table></figure>
</li>
<li><p>显示版本<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cd  /usr/local/hadoop</div><div class="line">./bin/hadoop version</div></pre></td></tr></table></figure>
</li>
<li><p>配置环境变量<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">gedit ~/.bashrc (vim ~/.bashrc)</div></pre></td></tr></table></figure>
<p> 在文件中添加：<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="section">#Hadoop Environment Variables</span></div><div class="line">export HADOOP_HOME=/usr/local/hadoop</div><div class="line">export HADOOP<span class="emphasis">_INSTALL=$HADOOP_</span>HOME</div><div class="line">export HADOOP<span class="emphasis">_MAPRED_</span>HOME=$HADOOP_HOME</div><div class="line">export HADOOP<span class="emphasis">_COMMON_</span>HOME=$HADOOP_HOME</div><div class="line">export HADOOP<span class="emphasis">_HDFS_</span>HOME=$HADOOP_HOME</div><div class="line">export YARN<span class="emphasis">_HOME=$HADOOP_</span>HOME</div><div class="line">export HADOOP<span class="emphasis">_COMMON_</span>LIB<span class="emphasis">_NATIVE_</span>DIR=$HADOOP_HOME/lib/native</div><div class="line">export PATH=$PATH:$HADOOP<span class="emphasis">_HOME/sbin:$HADOOP_</span>HOME/bin</div></pre></td></tr></table></figure>
</li>
<li><p>使配置生效<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">source ~/.bashrc</div><div class="line">hadoop version #验证</div></pre></td></tr></table></figure>
</li>
<li><p>关闭虚拟机，克隆两个虚拟机，命名为CentOSSlave1,CentOSSlave2,注意要选择完整克隆<br><br></p>
</li>
<li>依次打开CentOSMaster,CentOSSlave1,CentOSSlave2,查看各自的ip,(ens**下的inet内容)<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="code">    ifconfig</span></div><div class="line"><span class="code">```   </span></div><div class="line"><span class="code">9. 修改各自的主机名(Matser，Slave1，Slave2)：&lt;br&gt;</span></div><div class="line"><span class="code">在master节点上：&lt;br&gt;</span></div><div class="line"><span class="code">```markdown</span></div><div class="line"><span class="code">    sudo hostnamectl  set-hostname Master</span></div></pre></td></tr></table></figure>
</li>
</ol>
<p>在Slave1节点上：<br><br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo hostnamectl  set-hostname Slave1</div></pre></td></tr></table></figure></p>
<p>在Slave2节点上：<br><br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo hostnamectl  set-hostname Slave2</div></pre></td></tr></table></figure></p>
<ol>
<li><p>修改ip映射（三个节点都要修改）：<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo vim /etc/hosts</div></pre></td></tr></table></figure>
<p>在末尾加上:<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ip1   Master</div><div class="line">ip2   Slave1</div><div class="line">   ip3   Slave2</div></pre></td></tr></table></figure>
</li>
<li><p>设置开机启动网络<br><br>修改 /etc/sysconfig/network-scripts/ifcfg-ens*（具体文件名每个人有可能不同）,将最后一行的ONBOOT 改为yes    </p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="code">    vim  /etc/sysconfig/network-scripts/ifcfg-ens33  #我的文件名称为ifcfg-ens33</span></div><div class="line"><span class="code">``` </span></div><div class="line"><span class="code">12. 通过在终端分别执行ping Master，ping Slave1，ping Slave2,看是否能通，ctrl+C停止&lt;br&gt;&lt;br&gt;</span></div><div class="line"><span class="code">13. 主节点Master使用ssh无密钥登陆节点（注意ssh登陆的用户名）&lt;br&gt;&lt;br&gt;</span></div><div class="line"><span class="code">    a. 首先生成 Master 节点的公匙，在 Master 节点的终端中执行：&lt;br&gt;</span></div><div class="line"><span class="code">```markdown</span></div><div class="line"><span class="code">       su hadoop               #登陆到hadoop用户,所有操作都使hadoop用户的行为</span></div><div class="line"><span class="code">       cd ~/.ssh               # 如果没有该目录，先执行一次ssh Master</span></div><div class="line"><span class="code">       rm ./id_rsa*            # 删除之前生成的公匙（如果有）</span></div><div class="line"><span class="code">       ssh-keygen -t rsa       # 一直按回车就可以</span></div></pre></td></tr></table></figure>
<p>b. 让Master节点需能无密码ssh本机，在 Master 节点上执行：<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cat ./id<span class="emphasis">_rsa.pub &gt;&gt; ./authorized_</span>keys</div><div class="line">chmod 600 ./authorized_keys    # 修改文件权限</div></pre></td></tr></table></figure>
<p>  完成后可执行 ssh Master 验证一下（可能需要输入 yes，成功后执行 exit 返回原来的终端）。<br><br><br>c. 将上公匙传输到 Slave1 节点(Slave2也是一样操作将Slave1改成Slave2):<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="code">       scp ~/.ssh/id_rsa.pub hadoop@Slave1:/home/hadoop/&lt;br&gt;&lt;br&gt;</span></div><div class="line"><span class="code">``` </span></div><div class="line"><span class="code">    d. 在Slave1和Slave2节点上 操作：&lt;br&gt;</span></div><div class="line"><span class="code">```markdown</span></div><div class="line"><span class="code">       mkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略</span></div><div class="line"><span class="code">       cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></div><div class="line"><span class="code">       rm ~/id_rsa.pub    # 用完就可以删掉了</span></div><div class="line"><span class="code">`</span></div></pre></td></tr></table></figure>
<p>e. 在Master节点上ssh Slave1和Slave2，验证是否能连接上<br><br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">ssh Slave1</div><div class="line"></div><div class="line">ssh Slave2</div></pre></td></tr></table></figure>
</li>
<li><p>在Master节点上操作，cd /usr/local/hadoop/etc/hadoop,进入root模式<br><br>a. 修改slaves文件,将localhost注释，添加Slave1,换行，Slave2<br><br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim slaves</div></pre></td></tr></table></figure>
<p>b. 修改core-site.xml<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&lt;configuration&gt;</div><div class="line"><span class="code">       &lt;property&gt;</span></div><div class="line"><span class="code">            &lt;name&gt;fs.defaultFS&lt;/name&gt;</span></div><div class="line"><span class="code">            &lt;value&gt;hdfs://Master:9000&lt;/value&gt;</span></div><div class="line"><span class="code">        &lt;/property&gt;</span></div><div class="line"><span class="code">        &lt;property&gt;</span></div><div class="line"><span class="code">            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span></div><div class="line"><span class="code">            &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span></div><div class="line">                &lt;description&gt;tmp directories&lt;/description&gt;</div><div class="line">            &lt;/property&gt;</div><div class="line">&lt;/configuration&gt;</div></pre></td></tr></table></figure>
<p>c. 修改hdfs-site.xml,其中的dfs.replication的value根据Slave的个数填写<br></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div><div class="line">`</div></pre></td></tr></table></figure>
<p>d. 重命名 mapred-site.xml.template为mapred-site.xml,并修改mapred-site.xml为：<br></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div><div class="line">`</div></pre></td></tr></table></figure>
<p>e. 修改yarn.site.xml为：<br></p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
<li><p>配置好后,将Master上的/usr/local/hadoop文件夹复制到各个节点上。如果有临时文件和日志文件先删除,在Master节点上执行:<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">cd /usr/local</div><div class="line">sudo rm -r ./hadoop/tmp                    # 删除 Hadoop 临时文件</div><div class="line">sudo rm -r ./hadoop/logs/*                 # 删除日志文件</div><div class="line">tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制</div><div class="line">cd ~</div><div class="line">scp ./hadoop.master.tar.gz Slave1:/home/hadoop</div></pre></td></tr></table></figure>
<p>如果有其他节点再执行：scp ./hadoop.master.tar.gz Slave(n):/home/hadoop<br><br></p>
</li>
<li><p>分别在slave节点上执行<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo rm -r /usr/local/hadoop    # 删掉旧的（如果存在）</div><div class="line">sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/local</div><div class="line">sudo chown -R hadoop /usr/local/hadoop   #给hadoop用户读写/usr/local/hadoop的权限</div></pre></td></tr></table></figure>
</li>
<li><p>首次启动需要先在 Master 节点执行 NameNode 的格式化：<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hdfs namenode -format       # 首次运行需要执行初始化，之后不需要，status=0，表示成功</div></pre></td></tr></table></figure>
</li>
<li><p>关闭防火墙(所有机器)：<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">systemctl stop firewalld.service    # 关闭firewall<span class="xml"><span class="tag">&lt;</span></span></div><div class="line"><span class="xml">systemctl disable firewalld.service # 禁止firewall开机启动</span></div></pre></td></tr></table></figure>
</li>
<li><p>启动服务<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">start-yarn.sh</div><div class="line">start-dfs.sh</div><div class="line">mr-jobhistory-daemon.sh start historyserver</div></pre></td></tr></table></figure>
</li>
<li><p>在master节点上查看java进程<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jps</div></pre></td></tr></table></figure>
<p>如果有JobHistoryServer,SecondaryNameNode,Jsp,ResourceManager,NameNode四个进程代表Master上没问题<br><br></p>
</li>
<li><p>在slave节点上执行<br></p>
<figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jps</div></pre></td></tr></table></figure>
<p>如果有Jps，DataNode,NodeManager,三个节点表示配置成功<br><br></p>
</li>
<li>关闭服务<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">stop-yarn.sh</div><div class="line">stop-dfs.sh</div><div class="line">mr-jobhistory-daemon.sh stop historyserver</div></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本操作&quot;&gt;&lt;a href=&quot;#基本操作&quot; class=&quot;headerlink&quot; title=&quot;基本操作&quot;&gt;&lt;/a&gt;基本操作&lt;/h2&gt;&lt;h3 id=&quot;简单的的vim命令&quot;&gt;&lt;a href=&quot;#简单的的vim命令&quot; class=&quot;headerlink&quot; title=&quot;简单的的vim命令&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.cnblogs.com/jeakon/archive/2012/08/13/2816802.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;简单的的vim命令&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;linux常用命令&quot;&gt;&lt;a href=&quot;#linux常用命令&quot; class=&quot;headerlink&quot; title=&quot;linux常用命令&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.weixuehao.com/archives/25&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;linux常用命令&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;linux命令查找网站&quot;&gt;&lt;a href=&quot;#linux命令查找网站&quot; class=&quot;headerlink&quot; title=&quot;linux命令查找网站&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://man.linuxde.net/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;linux命令查找网站&lt;/a&gt;&lt;/h3&gt;&lt;h1 id=&quot;CentOS-下安装hadoop&quot;&gt;&lt;a href=&quot;#CentOS-下安装hadoop&quot; class=&quot;headerlink&quot; title=&quot;CentOS 下安装hadoop&quot;&gt;&lt;/a&gt;CentOS 下安装hadoop&lt;br&gt;&lt;/h1&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="hadoop" scheme="http://jchanji.github.io/tags/hadoop/"/>
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://jchanji.github.io/year/08/29/hello-world/"/>
    <id>http://jchanji.github.io/year/08/29/hello-world/</id>
    <published>2017-08-29T07:35:40.594Z</published>
    <updated>2017-08-29T13:31:43.559Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
