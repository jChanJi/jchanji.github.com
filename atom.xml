<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chanji</title>
  
  <subtitle>Stay Hungry,Stay Foolish</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jchanji.github.io/"/>
  <updated>2018-01-11T18:43:17.704Z</updated>
  <id>http://jchanji.github.io/</id>
  
  <author>
    <name>Chanji</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>digitalocean服务器搭建ss服务器</title>
    <link href="http://jchanji.github.io/year/01/12/vps/"/>
    <id>http://jchanji.github.io/year/01/12/vps/</id>
    <published>2018-01-11T18:33:39.700Z</published>
    <updated>2018-01-11T18:43:17.704Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote><p>经朋友介绍digitalocean服务器对于学生而言比较划算，最基础的服务器5$一个月，通过推荐链接注册可以领取10$的优惠或通过其他网上的推荐码获得15$,25$等不等的优惠券，如果使用github学生优惠券的还可以领50$的优惠。我找到的是15$优惠券。这里比较人性的是这里的优惠券是直接充值到账户上的，和腾讯，阿里的的只能使用一次的优惠券有点不同。需要注意的是优惠码只能用一次，如果使用过一次再使用github优惠码的时候会失败。网上的说法是直接联系客服，联系后的回信是。</p><p>Hello, and thank you for contacting DigitalOcean!<br>I apologize for the inconvenience, however as you already have a promotional code on this account, we are unable to apply any additional codes at this time.<br></p><p>很无奈</p></blockquote><h1 id="前期准备"><a href="#前期准备" class="headerlink" title="前期准备"></a>前期准备</h1><blockquote><p>1、登陆github,settings-&gt;emails-&gt;add email address,这里需要注意的是邮箱必须是你学校的邮箱，edu结尾的，每个学生都有。<br><br>2、申请学生包：<a href="https://education.github.com/" target="_blank" rel="noopener">https://education.github.com/</a>  填写信息，如果收到通过邮件则成功。然后在get your pack页面下领取优惠码，如下图:<br></p></blockquote><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/digitalocean1.PNG" alt="img1"></p><p>3、点击<a href="https://m.do.co/c/efc4e6d6df86" target="_blank" rel="noopener">https://m.do.co/c/efc4e6d6df86</a> 邀请注册用户，可以得到10$优惠,注册邮箱随便填。<br><br>4、到support中提交申请工单，点击Go To Tricks，如下图所示：<br></p><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/digitalocean2.PNG" alt="img2"></p><p>4、创建5$套餐的vps,选择ubuntu16.04版本的服务器，我这里服务器地点选择的是新加坡，感觉网网速还可以<br><br>5、下载<a href="http://rj.baidu.com/soft/detail/15699.html?ald" target="_blank" rel="noopener">putty</a>,登陆服务器，登陆用户名为root,密码会发到你邮箱。登录后会要求修改密码。第一次输入原始密码，第二次输入修改后的密码，第三次重复密码。到此为止服务器已经搭建完成。<br></p><h1 id="搭建shadowsocks"><a href="#搭建shadowsocks" class="headerlink" title="搭建shadowsocks"></a>搭建shadowsocks</h1><blockquote><p>1、教程各种版本的教程见 <a href="https://github.com/teddysun/shadowsocks_install" target="_blank" rel="noopener">https://github.com/teddysun/shadowsocks_install</a> 我们选择的是 shadowsocks-libev.sh的安装方式。其中我的加密方式为aes-256-cfb,这是没问题的。Your Server IP填写服务器外网地址，Your Server Port改一下,不要用默认的。<br><br>2、搭建完成后下载window的shadowsocks的客户端 <a href="https://github.com/shadowsocks/shadowsocks-windows/releases" target="_blank" rel="noopener">https://github.com/shadowsocks/shadowsocks-windows/releases</a> 我安装的是4.0.7版本,解压运行，根据刚才配置的服务器填写选项。在用户栏右击图标，启用系统代理，代理模式选择全局。<br><br>3、到此为止就可以科学上网了。</p></blockquote><h1 id="使用google的BBR加速"><a href="#使用google的BBR加速" class="headerlink" title="使用google的BBR加速"></a>使用google的BBR加速</h1><p>加速后和前根本不是一个概念，所以装不装很明显，哈哈。<br。 下载安装包="" <figure="" class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9.9/linux-headers-4.9.9-040909<span class="emphasis">_4.9.9-040909.201702090333_</span>all.deb</span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9.9/linux-headers-4.9.9-040909-generic<span class="emphasis">_4.9.9-040909.201702090333_</span>amd64.deb</span><br><span class="line">wget http://kernel.ubuntu.com/~kernel-ppa/mainline/v4.9.9/linux-image-4.9.9-040909-generic<span class="emphasis">_4.9.9-040909.201702090333_</span>amd64.deb</span><br><span class="line">dpkg -i [依次是上面三个文件]</span><br><span class="line">dpkg -l | grep linux-image #查看当前内核</span><br><span class="line">apt remove linux-image-4.4.0-108-generic #删除旧版本内核</span><br><span class="line">update-grub #跟新grub文件</span><br><span class="line">ls /boot/vmlinuz*  #查看是否有4.9内核</span><br><span class="line">reboot #重启</span><br><span class="line">uname -a #查看当前内核</span><br><span class="line">vim /etc/sysctl.conf #配置文件</span><br></pre></td></tr></table></br。></p><p>内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.core.default_qdisc=fq</span><br><span class="line">net.ipv4.tcp<span class="emphasis">_congestion_</span>control=bbr</span><br></pre></td></tr></table></figure></p><p>保存后执行<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -p</span><br></pre></td></tr></table></figure></p><p>出现 sysctl net.ipv4.tcp_congestion_control则已经启动，至此，就可以在youtube上纵享1080p了</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;经朋友介绍digitalocean服务器对于学生而言比较划算，最基础的服务器5$一个月，通过推荐链接注册可以领取1
      
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="ubuntu" scheme="http://jchanji.github.io/tags/ubuntu/"/>
    
      <category term="vps" scheme="http://jchanji.github.io/tags/vps/"/>
    
  </entry>
  
  <entry>
    <title>spark之combineByKey</title>
    <link href="http://jchanji.github.io/year/01/11/combineByKey/"/>
    <id>http://jchanji.github.io/year/01/11/combineByKey/</id>
    <published>2018-01-11T00:33:21.078Z</published>
    <updated>2018-01-11T00:49:27.972Z</updated>
    
    <content type="html"><![CDATA[<h1 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">combineByKey(creatCombiner,mergeValue,mergeCombiners,partitioner)</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="实例讲解"><a href="#实例讲解" class="headerlink" title="实例讲解"></a>实例讲解</h1><h2 id="生成数据"><a href="#生成数据" class="headerlink" title="生成数据"></a>生成数据</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">val scores = sc.parallelize(Array(("jake",80.0),</span><br><span class="line"><span class="code">                                  ("jake",90.0),</span></span><br><span class="line"><span class="code">                                  ("jake",85.0),</span></span><br><span class="line"><span class="code">                                  ("mike",85.0),</span></span><br><span class="line"><span class="code">                                  ("mike",92.0),</span></span><br><span class="line"><span class="code">                                  ("mike",90.0)))</span></span><br></pre></td></tr></table></figure><p>查看数据<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scores.foreach(println)</span><br><span class="line">(jake,90.0)</span><br><span class="line">(jake,80.0)</span><br><span class="line">(jake,85.0)</span><br><span class="line">(mike,92.0)</span><br><span class="line">(mike,85.0)</span><br><span class="line">(mike,90.0)</span><br></pre></td></tr></table></figure></p><h2 id="求三门总分"><a href="#求三门总分" class="headerlink" title="求三门总分"></a>求三门总分</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">val score2 = scores.combineByKey(score=&gt;(1,score),</span><br><span class="line"><span class="code">                                (c1:(Int,Double),newScore)=&gt;(c1._1+1,c1._2+newScore),</span></span><br><span class="line"><span class="code">                                (c1:(Int,Double),c2:(Int,Double))=&gt;(c1._1+c2._1,c1._2+c2._2))</span></span><br></pre></td></tr></table></figure><p>查看scores内容<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">score2.foreach(println)</span><br><span class="line">(mike,(3,267.0))</span><br><span class="line">(jake,(3,255.0))</span><br></pre></td></tr></table></figure></p><p>注解：<br><br>1、Int表示科目出现的数目，Double表示第一门与第二门课累加之后的值，newScore表示的是遍历的时候出现的新的分数<br><br>2、当遍历到第二个时因为jake已经遍历过所以调用mergeValue这个参数，科目数变为 +1 分数也变为和newScore之和<br><br>mergeCombiner:c1和c2的科目数和分数相加（所有key相同的value相加）<br><br>3、partitioner一般用不到</p><h2 id="求平均分"><a href="#求平均分" class="headerlink" title="求平均分"></a>求平均分</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val average = score2.map&#123;case(name,(num,score))=&gt;(name,score/num)&#125;</span><br><span class="line">average.foreach(println)</span><br></pre></td></tr></table></figure><p>运行结果<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(mike,89.0)</span><br><span class="line">(jake,85.0)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;函数&quot;&gt;&lt;a href=&quot;#函数&quot; class=&quot;headerlink&quot; title=&quot;函数&quot;&gt;&lt;/a&gt;函数&lt;/h1&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;combineByKey(creatCombiner,mergeValue,mergeCombiners,partitioner)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://jchanji.github.io/tags/spark/"/>
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>scala 运行spark程序</title>
    <link href="http://jchanji.github.io/year/01/11/idea_scala_spark/"/>
    <id>http://jchanji.github.io/year/01/11/idea_scala_spark/</id>
    <published>2018-01-11T00:14:52.449Z</published>
    <updated>2018-01-11T00:52:12.259Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h1><h2 id="配置maven镜像地址"><a href="#配置maven镜像地址" class="headerlink" title="配置maven镜像地址"></a>配置maven镜像地址</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">mirror</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;id&gt;alimaven&lt;/id&gt;</span></span><br><span class="line"><span class="code">    &lt;name&gt;aliyun maven&lt;/name&gt;</span></span><br><span class="line"><span class="code">    &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span></span><br><span class="line"><span class="code">    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;      </span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">mirror</span>&gt;</span></span></span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="sbt仓库地址"><a href="#sbt仓库地址" class="headerlink" title="sbt仓库地址"></a>sbt仓库地址</h2><p>编辑~/.sbt/repositories（没有就新建）<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[repositories]</span><br><span class="line">local</span><br><span class="line">dl bintray: https://dl.bintray.com/typesafe/ivy-releases/, [<span class="string">organization</span>]/[<span class="string">module</span>]/[<span class="string">revision</span>]/[<span class="string">type</span>]s/[<span class="string">artifact</span>](<span class="link">-[classifier]</span>).[ext], bootOnly</span><br><span class="line">jcenter: https://jcenter.bintray.com/</span><br><span class="line">typesafe-ivy-releases: https://repo.typesafe.com/typesafe/ivy-releases/, [<span class="string">organization</span>]/[<span class="string">module</span>]/[<span class="string">revision</span>]/[<span class="string">type</span>]s/[<span class="string">artifact</span>](<span class="link">-[classifier]</span>).[ext], bootOnly</span><br><span class="line">maven-central</span><br><span class="line">sonatype-snapshots: https://oss.sonatype.org/content/repositories/snapshot</span><br></pre></td></tr></table></figure></p><h1 id="版本"><a href="#版本" class="headerlink" title="版本"></a>版本</h1><blockquote><p>jdk1.8</p><p>spark 2.2  </p><p>scala 2.11.11</p><p>sbt0.13.16</p></blockquote><h1 id="sbt依赖"><a href="#sbt依赖" class="headerlink" title="sbt依赖"></a>sbt依赖</h1><p>build.sbt文件内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">name := "scala_test"</span><br><span class="line">version := "1.0"</span><br><span class="line">scalaVersion := "2.11.11" #后面不需要配置</span><br><span class="line">libraryDependencies ++= Seq("org.apache.spark" %% "spark-core" % "2.2.0")</span><br><span class="line">libraryDependencies += "org.apache.spark" % "spark-streaming_2.11" % "2.2.0"  </span><br><span class="line">libraryDependencies += "org.apache.spark" % "spark-mllib_2.11" % "2.2.0"  </span><br><span class="line">libraryDependencies += "org.apache.spark" % "spark-sql_2.11" % "2.2.0"  </span><br><span class="line">libraryDependencies += "org.apache.spark" % "spark-streaming-kafka-0-8_2.11" % "2.2.0"  </span><br><span class="line">libraryDependencies += "org.apache.spark" % "spark-streaming-flume_2.11" % "2.2.0"  </span><br><span class="line">libraryDependencies += "org.apache.spark" % "spark-hive_2.11" % "2.2.0" % "provided"  </span><br><span class="line">libraryDependencies += "org.scalanlp" % "breeze_2.11" % "0.11"  </span><br><span class="line">libraryDependencies += "org.scalanlp" % "breeze-natives_2.11" % "0.11"  </span><br><span class="line">libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.6.0"</span><br></pre></td></tr></table></figure></p><p>然后使用sbt打包,如下图所示</p><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/1.png" alt="图一"></p><p><strong>注意本机实验的时候一定要先登录localhost的ssh不然到提交作业的时候会没有权限写入文件</strong></p><h1 id="运行服务"><a href="#运行服务" class="headerlink" title="运行服务"></a>运行服务</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-master.sh # 启动spark</span><br><span class="line">./bin/spark-class org.apache.spark.deploy.worker.Worker spark://localhost.localdomain:7077 # 启动worker, spark://localhost.localdomain:7077到8080端口查看   </span><br><span class="line">./bin/spark-submit --master spark://localhost.localdomain:7077  --class WordCount /home/chanji/scala_test.jar # 提交作业</span><br></pre></td></tr></table></figure><p>再到<a href="http://localhost:8080/jobs" target="_blank" rel="noopener">http://localhost:8080/jobs</a> 查看作业</p><h1 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h1><h2 id="无法解析主机名"><a href="#无法解析主机名" class="headerlink" title="无法解析主机名"></a>无法解析主机名</h2><blockquote><p>无法解析主机：promote.cache-dns.local＜br&gt;</p><p>hostname分为三种类型：<br><br>静态的（static），瞬态的（transient），和灵活的（pret不然ty）<br><br>执行系统命令hostname得到的是瞬态的主机名，扫描文件中得到的是静态主机名。<br><br>将这两者统一起来就可以解决问题了。<br><br>1.自定义hostname，假设myname<br><br>2.执行hostnamectl set-hostname myname<br><br>这个命令能同时修改三种类型的主机名定义。<br><br>重启</p></blockquote><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><blockquote><p>Drive program 是程序的入口,包含这main函数<br>context起着和集群连接的作用<br>RDDs　弹性分布式数据集,实质就是一个数据集，指向一个变量，数据集可以被分为若干部分存在与不同的主机上，但操作这些数据只能通过定义的数据集变量。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;环境配置&quot;&gt;&lt;a href=&quot;#环境配置&quot; class=&quot;headerlink&quot; title=&quot;环境配置&quot;&gt;&lt;/a&gt;环境配置&lt;/h1&gt;&lt;h2 id=&quot;配置maven镜像地址&quot;&gt;&lt;a href=&quot;#配置maven镜像地址&quot; class=&quot;headerlink&quot; title=&quot;配置maven镜像地址&quot;&gt;&lt;/a&gt;配置maven镜像地址&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;&lt;span class=&quot;name&quot;&gt;mirror&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;id&amp;gt;alimaven&amp;lt;/id&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;name&amp;gt;aliyun maven&amp;lt;/name&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;url&amp;gt;http://maven.aliyun.com/nexus/content/groups/public/&amp;lt;/url&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;code&quot;&gt;    &amp;lt;mirrorOf&amp;gt;central&amp;lt;/mirrorOf&amp;gt;      &lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;xml&quot;&gt;&lt;span class=&quot;tag&quot;&gt;&amp;lt;/&lt;span class=&quot;name&quot;&gt;mirror&lt;/span&gt;&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://jchanji.github.io/tags/spark/"/>
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下Docker的安装和使用</title>
    <link href="http://jchanji.github.io/year/01/10/docker/"/>
    <id>http://jchanji.github.io/year/01/10/docker/</id>
    <published>2018-01-09T16:00:00.000Z</published>
    <updated>2018-01-09T17:25:55.413Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安装环境检查"><a href="#安装环境检查" class="headerlink" title="安装环境检查"></a>安装环境检查</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uname -a</span><br><span class="line">ls -l /sys/class/misc/device-mapper</span><br></pre></td></tr></table></figure><a id="more"></a><h1 id="使用ubuntu-apt-get安装"><a href="#使用ubuntu-apt-get安装" class="headerlink" title="使用ubuntu apt-get安装"></a>使用ubuntu apt-get安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install docker.io</span><br><span class="line">source /etc/bash_completion.d/docker.io</span><br></pre></td></tr></table></figure><blockquote><p>由于使用ubuntu安装不是最新的版本所以使用docker提供的方法安装，先卸载</p><h1 id="apt安装的卸载"><a href="#apt安装的卸载" class="headerlink" title="apt安装的卸载"></a>apt安装的卸载</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove docker</span><br><span class="line">sudo apt-get remove --auto-remove docker  </span><br><span class="line">sudo apt-get remove --purge docker.io  </span><br><span class="line">sudo apt-get autoremove --purge</span><br></pre></td></tr></table></figure></blockquote><h1 id="docker提供的方式安装"><a href="#docker提供的方式安装" class="headerlink" title="docker提供的方式安装"></a>docker提供的方式安装</h1><h2 id="安装crul"><a href="#安装crul" class="headerlink" title="安装crul"></a>安装crul</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install -y curl</span><br></pre></td></tr></table></figure><h2 id="安装docker"><a href="#安装docker" class="headerlink" title="安装docker"></a>安装docker</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL get.docker.com -o get-docker.sh</span><br></pre></td></tr></table></figure><p>如果报错没有source.list文件则新建<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sh get-docker.sh</span><br></pre></td></tr></table></figure></p><p>docker默认只能由root权限运行，所以给当前用户权限<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker your-user</span><br></pre></td></tr></table></figure></p><p>创建docker组并将当前用户加入组<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo groupadd docker</span><br><span class="line">sudo usermod-aG docker $USERE</span><br></pre></td></tr></table></figure></p><p>修改镜像源地址为Daocloud</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://efd9e468.m.daocloud.io</span><br></pre></td></tr></table></figure><p>#　docker安装版本的卸载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get purge docker-ce</span><br><span class="line">sudo rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure></p><h1 id="常用的命令"><a href="#常用的命令" class="headerlink" title="常用的命令"></a>常用的命令</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker run nginx #运行容器</span><br><span class="line">docker images  #查看镜像</span><br><span class="line">docker run -p 8080:80 -d daocloud.io/nginx #在8080：80端口使用Daemon模式运行以daocloud.io/nginx为镜像的容器</span><br><span class="line">docker ps</span><br><span class="line">sudo vim index.html</span><br></pre></td></tr></table></figure><p>在index.html中写<br><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">h1</span>&gt;</span>Docker is fun<span class="tag">&lt;/<span class="name">h1</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure></p><p>将文件上传到docker的容器中<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker cp index.html 3a90426cbb80://usr/share/nginx/html  #其中‘3a90426cbb80’是容器号，后面是容器中文件地址</span><br><span class="line">docker stop 3a90426cbb80　#停止容器</span><br></pre></td></tr></table></figure></p><p>当我们再次运行容器的时候发现是一开始未改动的结果，因为docker在容器内作的改动都是暂时的，不会保存下来,如果需要保存则要生成一个新的容器<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit -m "fun" 4138300aa8eb nginx-fun # nginx-fun为新容器的名字</span><br></pre></td></tr></table></figure></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">docker ps #查看当前运行的容器</span><br><span class="line">docker ps -aq #相当于列出所有的容器ID，然后docker rm它们</span><br><span class="line">docker pull #获取image</span><br><span class="line">docker build #创建image</span><br><span class="line">docker images #列出images</span><br><span class="line">docker run #运行container</span><br><span class="line">docker ps #列出container</span><br><span class="line">docker rm # 删除container</span><br><span class="line">docker rmi # 删除image</span><br><span class="line">docker cp # 在host和container之间拷贝文件</span><br><span class="line">docker commit 保存改动为新的image</span><br></pre></td></tr></table></figure><h1 id="通过dockerfile创建容器"><a href="#通过dockerfile创建容器" class="headerlink" title="通过dockerfile创建容器"></a>通过dockerfile创建容器</h1><h2 id="简单的例子"><a href="#简单的例子" class="headerlink" title="简单的例子"></a>简单的例子</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure><p>在其中写入<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM alpine:latest #基础镜像，alpine是专门针对linux的非常小的镜像</span><br><span class="line">MAINTAINER chanji　#用户声明</span><br><span class="line">CMD echo "hello Docker" #执行命令</span><br></pre></td></tr></table></figure></p><p>build镜像<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">build -t hello_docker . # -t　标签参数; '.' 意思为文件路径下所有内容都送给docker engine</span><br><span class="line">docker run hello_docker #生成镜像后运行</span><br></pre></td></tr></table></figure></p><p>可以看到结果<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hello Docker</span><br></pre></td></tr></table></figure></p><h2 id="稍微复杂的例子"><a href="#稍微复杂的例子" class="headerlink" title="稍微复杂的例子"></a>稍微复杂的例子</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir Dockerfile2</span><br><span class="line">cd Dockerfile2</span><br><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure><p>在其中写入<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">MAINTAINER chanji</span><br><span class="line">RUN apt-get update</span><br><span class="line">RUN　apt-get install -y nginx</span><br><span class="line">COPY　index.html /var/www/html</span><br><span class="line">ENTRYPOINT　["/usr/sbin/nginx","-g","daemon off;"]  #将nginx在前台执行</span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure></p><p>再编辑index.html文件<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim index.html</span><br></pre></td></tr></table></figure></p><p>在其中写入<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hello docker</span><br></pre></td></tr></table></figure></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker build -t chanji/hello-nginx . #构建</span><br><span class="line">docker run -d -p 80:80 chanji/hello-nginx #以deamon模式在80:80端口运行</span><br><span class="line">curl http://localhost #显示index.html中的信息hello docker</span><br></pre></td></tr></table></figure><h2 id="dockerfile简单语法"><a href="#dockerfile简单语法" class="headerlink" title="dockerfile简单语法"></a>dockerfile简单语法</h2><table><thead><tr><th>关键字</th><th>含义</th></tr></thead><tbody><tr><td>FROM</td><td>基础镜像</td></tr><tr><td>RUN　　</td><td>执行命令</td></tr><tr><td>ADD</td><td>添加文件</td></tr><tr><td>COPY</td><td>拷贝文件</td></tr><tr><td>CMD　　　</td><td>执行命令</td></tr><tr><td>EXPSOE</td><td>暴露端口</td></tr><tr><td>WORKDIR</td><td>指定路径     </td></tr><tr><td>MAINTAINER</td><td>　维护者</td></tr><tr><td>ENV</td><td>设定环境变量</td></tr><tr><td>USER　　</td><td>制定用户</td></tr><tr><td>VOLUME</td><td>mount point挂载的卷</td></tr></tbody></table><h1 id="镜像分层"><a href="#镜像分层" class="headerlink" title="镜像分层"></a>镜像分层</h1><blockquote><p>dockerfile中的每一行语句都有id,分层存储，已有的镜像是只读的，运行生成容器的时候容器有读写权限</p></blockquote><h1 id="Volume"><a href="#Volume" class="headerlink" title="Volume"></a>Volume</h1><blockquote><p>提供独立于程序之外的持久化存储，即将容器运行时的数据保存下来，这样就不用重新生成新的容器</p></blockquote><h2 id="将本地文件位置挂在到docker容器内的位置-本地路径默认的"><a href="#将本地文件位置挂在到docker容器内的位置-本地路径默认的" class="headerlink" title="将本地文件位置挂在到docker容器内的位置,本地路径默认的"></a>将本地文件位置挂在到docker容器内的位置,本地路径默认的</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name nginx -v /usr/share/nginx/html nginx # -d :表示开启Daemon模式,--name: 名称, -v [容器内路径] [基础镜像]: 基础镜像中挂载卷的位置</span><br><span class="line">docker aspect nginx # 查看容器信息中mount的host的地址</span><br><span class="line">su # 进入root</span><br><span class="line">cd /var/lib/docker/volumes/6f01d9e35da52f4a8157f9e2099937cb9040a351b6a327fd3a69b53065836bcb/data # 可以看到当中的信息与nginx容器挂载路径下的内容一致</span><br><span class="line">echo "hello volum!" &gt; index.html # 将内容写入到本地挂载的文件</span><br><span class="line">docker exec -it nginx /bin/bash #进入docker容器</span><br><span class="line">cd /usr/share/nginx/html # 打开目录</span><br><span class="line">cat index.html # 发现文件已经改变</span><br></pre></td></tr></table></figure><h2 id="将本地目录挂载在容器中的指定位置，本地路径自己定义"><a href="#将本地目录挂载在容器中的指定位置，本地路径自己定义" class="headerlink" title="将本地目录挂载在容器中的指定位置，本地路径自己定义"></a>将本地目录挂载在容器中的指定位置，本地路径自己定义</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br><span class="line">docker stop 776eee319d2b #停止占用80端口的容器</span><br><span class="line">mkdir -p /vol/html</span><br><span class="line">cd /vol/html</span><br><span class="line">vim index.html # 在其中添加内</span><br><span class="line">cd .. # 到上一级目录</span><br><span class="line">docker run -p 80:80 -d -v $PWD/html:/usr/share/nginx/html nginx # 将当前目录下的html文件夹挂载到nginx容器的/usr/share/nginx/html目录</span><br><span class="line">vim /html/index.html #修改其中内容为hello volume</span><br><span class="line">curl localhost # 发现和刚才修改的内容一致</span><br></pre></td></tr></table></figure><h2 id="创建一个仅有数据的容器，并将此容器挂载到其他容器"><a href="#创建一个仅有数据的容器，并将此容器挂载到其他容器" class="headerlink" title="创建一个仅有数据的容器，并将此容器挂载到其他容器"></a>创建一个仅有数据的容器，并将此容器挂载到其他容器</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /vol2/data</span><br><span class="line">docker create  -v $PWD/vol2/data:/var/mydata --name data<span class="emphasis">_container ubuntu #创建一个容器，名为data_</span>container，将本地目录挂载在其相应目录，基础镜像为ubuntu</span><br><span class="line">docker run -it --volumes-form data_container ubuntu /bin/bash #运行一个新的容器，将数据容器挂载在新的容器上，并进入交互模式</span><br><span class="line">mount #查看挂载，发现有/var/mydata目录</span><br><span class="line">cd /var/mydata</span><br><span class="line">touch hello.txt</span><br><span class="line">Crtl + d #退出容器</span><br><span class="line">cd data</span><br><span class="line">ls # 出现了hello.txt文件</span><br></pre></td></tr></table></figure><h1 id="Registry"><a href="#Registry" class="headerlink" title="Registry"></a>Registry</h1><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker search whalesay #搜索镜像</span><br><span class="line">docker pull docker/whalesay # 将镜像pull下来,REPOSITORY可以理解为镜像的名字</span><br><span class="line">docker run docker/whalesay cowsay Docker很好玩 #运行容器,调用cowsay，可以打印出鲸鱼和文字</span><br><span class="line">docker tag docker/whalesay chanji/whalesay # 产生和docker/whalesay镜像相同的dockerID/whalesay镜像</span><br><span class="line">docker login # dockerhub的用户名和密码，需要注册，很卡</span><br><span class="line">docker push chanji/whalesay #上传到镜像库</span><br></pre></td></tr></table></figure><h1 id="compose"><a href="#compose" class="headerlink" title="compose"></a>compose</h1><blockquote><p>多容器app</p></blockquote><h2 id="安装docker-compose"><a href="#安装docker-compose" class="headerlink" title="安装docker-compose"></a>安装docker-compose</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">su #进入root模式</span><br><span class="line">curl -L https://github.com/docker/compose/releases/download/1.9.0/docker-compose-$(uname -s)-$(uname -m) &gt; /usr/local/bin/docker-compose # 将二进制文件写入到bin中</span><br><span class="line">chmod a+x /usr/local/bin/docker-compose　#修改权限为可执行</span><br><span class="line">docker-compose --version　#查看版本</span><br></pre></td></tr></table></figure><h2 id="创建多容器app"><a href="#创建多容器app" class="headerlink" title="创建多容器app"></a>创建多容器app</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir ghost</span><br><span class="line">cd ghost</span><br><span class="line">mkdir ghost</span><br><span class="line">mkdir nginx</span><br><span class="line">mkdir data</span><br><span class="line">cd ghost</span><br><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure><p>其内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM ghost</span><br><span class="line">COPY ./config.js /var/lib/ghost/config.js</span><br><span class="line">EXPOSE 2368</span><br><span class="line">CMD ["npm","start","--production"]</span><br></pre></td></tr></table></figure></p><p>编辑config.js<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim config.js</span><br></pre></td></tr></table></figure></p><p>内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">var path = require('path'),</span><br><span class="line">config;</span><br><span class="line"></span><br><span class="line">config = &#123;</span><br><span class="line">production: &#123;</span><br><span class="line"><span class="code">    url: 'http://mytestblog.com',   </span></span><br><span class="line"><span class="code">    mail:&#123;&#125;,</span></span><br><span class="line"><span class="code">    database: &#123;</span></span><br><span class="line"><span class="code">        client: 'mysql',</span></span><br><span class="line"><span class="code">        connection: &#123;</span></span><br><span class="line"><span class="code">            host:'db',</span></span><br><span class="line"><span class="code">            user:'ghost',</span></span><br><span class="line"><span class="code">            password:'ghost',</span></span><br><span class="line"><span class="code">            database:'ghost,</span></span><br><span class="line"><span class="code">            port:'3306',</span></span><br><span class="line"><span class="code">            charset:'utf8',</span></span><br><span class="line"><span class="code">       &#125;,</span></span><br><span class="line"><span class="code">    debug: false</span></span><br><span class="line"><span class="code">    &#125;,</span></span><br><span class="line"><span class="code">    paths:&#123;</span></span><br><span class="line"><span class="code">        contentPath: path.join(process.env.GHOST_CONTENT, '/')</span></span><br><span class="line"><span class="code">    &#125;,</span></span><br><span class="line"><span class="code">    server: &#123;</span></span><br><span class="line"><span class="code">    host:'0.0.0.0',</span></span><br><span class="line"><span class="code">    port:'2368'</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line">   &#125;</span><br><span class="line">&#125;;</span><br><span class="line">module.exports = config;</span><br></pre></td></tr></table></figure></p><p>编辑nginx模块<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ../nginx</span><br><span class="line">vim Dockerfile</span><br></pre></td></tr></table></figure></p><p>内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FROM nginx</span><br><span class="line">COPY nginx.conf /etc/nginx/nginx.conf</span><br><span class="line">EXPOSE 80</span><br></pre></td></tr></table></figure></p><p>配置nginx.conf<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim nginx.config</span><br></pre></td></tr></table></figure></p><p>内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">worker_processes 4;</span><br><span class="line">events &#123;worker_connections 1024;&#125;</span><br><span class="line">http&#123;</span><br><span class="line"><span class="code">    server&#123;</span></span><br><span class="line"><span class="code">        listen 80;</span></span><br><span class="line"><span class="code">    location /&#123;</span></span><br><span class="line"><span class="code">        proxy_pass http://ghost-app:2368;</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>准备一个compose文件<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd .. #进入第一层的ghost目录</span><br><span class="line">vim docker-compose.yml</span><br></pre></td></tr></table></figure></p><p>其内容为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">version: '2'</span><br><span class="line">networks:</span><br><span class="line"><span class="code">    ghost:</span></span><br><span class="line">services:</span><br><span class="line"><span class="code">    ghost-app:</span></span><br><span class="line"><span class="code">        build: ghost</span></span><br><span class="line"><span class="code">        networks:</span></span><br><span class="line"><span class="code">            - ghost</span></span><br><span class="line"><span class="code">        depends_on:</span></span><br><span class="line"><span class="code">            - db</span></span><br><span class="line"><span class="code">        ports:</span></span><br><span class="line"><span class="code">            - "2368:2368"</span></span><br><span class="line"><span class="code">    nginx:</span></span><br><span class="line"><span class="code">        build: nginx</span></span><br><span class="line"><span class="code">        networks:</span></span><br><span class="line"><span class="code">            - ghost</span></span><br><span class="line"><span class="code">        depends_on:</span></span><br><span class="line"><span class="code">            - ghost-app</span></span><br><span class="line"><span class="code">        ports:</span></span><br><span class="line"><span class="code">            - "80:80"</span></span><br><span class="line"><span class="code">    db:</span></span><br><span class="line"><span class="code">        image: "mysql:5.7.15"</span></span><br><span class="line"><span class="code">        networks:</span></span><br><span class="line"><span class="code">            - ghost</span></span><br><span class="line"><span class="code">        environment:</span></span><br><span class="line"><span class="code">            MYSQL_ROOT_PASSWORD: mysqlroot</span></span><br><span class="line"><span class="code">            MYSQL_USER: ghost</span></span><br><span class="line"><span class="code">            MYSQL_PASSWORD: ghost</span></span><br><span class="line"><span class="code">        volumes:</span></span><br><span class="line"><span class="code">            - $PWD/data:/var/lib/mysql</span></span><br><span class="line"><span class="code">        ports:</span></span><br><span class="line"><span class="code">            - "3360:3360"</span></span><br></pre></td></tr></table></figure></p><p>上面的代码注意缩进和‘－’后的空格。文件下载<a href="https://raw.githubusercontent.com/jChanJi/static_resource/master/docker/docker-compose.yml" target="_blank" rel="noopener">docker-compsoe.yml</a><br><br>如果运行结果显示80端口被占用，docker stop [id]停止占用端口的容器<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker-compose stop #先停掉拉起来的服务</span><br><span class="line">docker-compose rm #删除停掉的服务</span><br><span class="line">docker-compose build　＃第一次未生成镜像时会自动构建，但是出错后不会再自动构建，需要build</span><br><span class="line">docker-compose up -d #再一次的拉起服务</span><br></pre></td></tr></table></figure></p><p>上述命令必须在ghost的文件中执行<br><br>以上配置基本完成，打开浏览器<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost #打开浏览器输入lcoalhost，出现ghost页面则成功</span><br><span class="line">localhost/ghost #配置ghost路径</span><br></pre></td></tr></table></figure></p><p>下面给出几张成功的页面<br><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ghost.png" alt="image1"></p><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ghost2.png" alt="mage2"></p><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/ghost3.png" alt="image3"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><blockquote><p>docker作为目前极为流行的环境部署容器,使用起来确实是方便快捷效率高,以上使用了多app容器实现了mysql,ghost和数据容器的整合，只需要一些简单的命令便可以搭建起来一个服务。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;安装环境检查&quot;&gt;&lt;a href=&quot;#安装环境检查&quot; class=&quot;headerlink&quot; title=&quot;安装环境检查&quot;&gt;&lt;/a&gt;安装环境检查&lt;/h1&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;uname -a&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;ls -l /sys/class/misc/device-mapper&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="ubuntu" scheme="http://jchanji.github.io/tags/ubuntu/"/>
    
      <category term="docker" scheme="http://jchanji.github.io/tags/docker/"/>
    
  </entry>
  
  <entry>
    <title>NMF和PCA算法对人脸进行特征提取并且进行对比</title>
    <link href="http://jchanji.github.io/year/01/09/NMF_PCA/"/>
    <id>http://jchanji.github.io/year/01/09/NMF_PCA/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-01-09T17:18:38.836Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><blockquote><p>此篇笔记主要根据南京大学礼欣老师的<a href="http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce" target="_blank" rel="noopener">《Python机器学习应用》</a>整理而成，详细内容请看礼欣老师的mooc课程。</p></blockquote><h2 id="数据介绍："><a href="#数据介绍：" class="headerlink" title="数据介绍："></a>数据介绍：</h2><p>分别使用NMF和PCA算法对人脸进行特征提取并且进行对比<br><a id="more"></a></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">n_row, n_col = <span class="number">2</span>, <span class="number">3</span></span><br><span class="line">n_components = n_row * n_col</span><br><span class="line">image_shape = (<span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="comment"># Load faces data</span></span><br><span class="line">dataset = fetch_olivetti_faces(shuffle=<span class="keyword">True</span>, random_state=RandomState(<span class="number">0</span>))</span><br><span class="line">faces = dataset.data</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_gallery</span><span class="params">(title, images, n_col=n_col, n_row=n_row)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">2.</span> * n_col, <span class="number">2.26</span> * n_row))</span><br><span class="line">    plt.suptitle(title, size=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, comp <span class="keyword">in</span> enumerate(images):</span><br><span class="line">        plt.subplot(n_row, n_col, i + <span class="number">1</span>)</span><br><span class="line">        vmax = max(comp.max(), -comp.min())</span><br><span class="line"></span><br><span class="line">        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,</span><br><span class="line">                   interpolation=<span class="string">'nearest'</span>, vmin=-vmax, vmax=vmax)</span><br><span class="line">        plt.xticks(())</span><br><span class="line">        plt.yticks(())</span><br><span class="line">    plt.subplots_adjust(<span class="number">0.01</span>, <span class="number">0.05</span>, <span class="number">0.99</span>, <span class="number">0.94</span>, <span class="number">0.04</span>, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plot_gallery(<span class="string">"First centered Olivetti faces"</span>, faces[:n_components])</span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"></span><br><span class="line">estimators = [</span><br><span class="line">    (<span class="string">'Eigenfaces - PCA using randomized SVD'</span>,</span><br><span class="line">         decomposition.PCA(n_components=<span class="number">6</span>,whiten=<span class="keyword">True</span>)),</span><br><span class="line"></span><br><span class="line">    (<span class="string">'Non-negative components - NMF'</span>,</span><br><span class="line">         decomposition.NMF(n_components=<span class="number">6</span>, init=<span class="string">'nndsvda'</span>, tol=<span class="number">5e-3</span>))</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, estimator <span class="keyword">in</span> estimators:</span><br><span class="line">    print(<span class="string">"Extracting the top %d %s..."</span> % (n_components, name))</span><br><span class="line">    print(faces.shape)</span><br><span class="line">    estimator.fit(faces)</span><br><span class="line">    components_ = estimator.components_</span><br><span class="line">    plot_gallery(name, components_[:n_components])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">downloading Olivetti faces from http://cs.nyu.edu/~roweis/data/olivettifaces.mat to C:\Users\ChanJi\scikit<span class="emphasis">_learn_</span>data</span><br><span class="line">Extracting the top 6 Eigenfaces - PCA using randomized SVD...</span><br><span class="line">(400, 4096)</span><br><span class="line">Extracting the top 6 Non-negative components - NMF...</span><br><span class="line">(400, 4096)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/face_ori.PNG" alt="图1"><br><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/face_PCA.PNG" alt="图2"><br><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/face_NMF.PNG" alt="图3"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;此篇笔记主要根据南京大学礼欣老师的&lt;a href=&quot;http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Python机器学习应用》&lt;/a&gt;整理而成，详细内容请看礼欣老师的mooc课程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;数据介绍：&quot;&gt;&lt;a href=&quot;#数据介绍：&quot; class=&quot;headerlink&quot; title=&quot;数据介绍：&quot;&gt;&lt;/a&gt;数据介绍：&lt;/h2&gt;&lt;p&gt;分别使用NMF和PCA算法对人脸进行特征提取并且进行对比&lt;br&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="python" scheme="http://jchanji.github.io/tags/python/"/>
    
      <category term="machinelearning" scheme="http://jchanji.github.io/tags/machinelearning/"/>
    
      <category term="NMF" scheme="http://jchanji.github.io/tags/NMF/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法DBSCAN实现大学生上网时长分类</title>
    <link href="http://jchanji.github.io/year/01/09/studentOnline_timeslot/"/>
    <id>http://jchanji.github.io/year/01/09/studentOnline_timeslot/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-01-08T16:47:48.022Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><blockquote><p>此篇笔记主要根据南京大学礼欣老师的<a href="http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce" target="_blank" rel="noopener">《Python机器学习应用》</a>整理而成，详细内容请看礼欣老师的mooc课程。</p></blockquote><a id="more"></a><h2 id="数据介绍："><a href="#数据介绍：" class="headerlink" title="数据介绍："></a>数据介绍：</h2><p>现有大学校园网的日志数据，290条大学生的校园网使用情况数据，数据包<br>括用户ID，设备的MAC地址，IP地址，开始上网时间，停止上网时间，上<br>网时长，校园网套餐等。利用已有数据，分析学生上网的模式。数据下载<a href="https://github.com/jChanJi/jchanji.github.com/tree/master/meterial/data/clustering" target="_blank" rel="noopener">点击我</a></p><h2 id="主要参数"><a href="#主要参数" class="headerlink" title="主要参数"></a>主要参数</h2><h3 id="eps-两个样本被看作邻居节点的最大距离"><a href="#eps-两个样本被看作邻居节点的最大距离" class="headerlink" title="eps: 两个样本被看作邻居节点的最大距离"></a>eps: 两个样本被看作邻居节点的最大距离</h3><h3 id="min-samples-簇的样本数"><a href="#min-samples-簇的样本数" class="headerlink" title="min_samples: 簇的样本数"></a>min_samples: 簇的样本数</h3><h3 id="metric：距离计算方式"><a href="#metric：距离计算方式" class="headerlink" title="metric：距离计算方式"></a>metric：距离计算方式</h3><h2 id="上网时间段"><a href="#上网时间段" class="headerlink" title="上网时间段"></a>上网时间段</h2><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.cluster <span class="keyword">as</span> skc</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">mac2id=dict()</span><br><span class="line">onlinetimes=[]</span><br><span class="line">f=open(<span class="string">'F:\data\clustering\TestData.txt'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    mac=line.split(<span class="string">','</span>)[<span class="number">2</span>]</span><br><span class="line">    onlinetime=int(line.split(<span class="string">','</span>)[<span class="number">6</span>])</span><br><span class="line">    starttime=int(line.split(<span class="string">','</span>)[<span class="number">4</span>].split(<span class="string">' '</span>)[<span class="number">1</span>].split(<span class="string">':'</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> mac <span class="keyword">not</span> <span class="keyword">in</span> mac2id:</span><br><span class="line">        mac2id[mac]=len(onlinetimes)</span><br><span class="line">        onlinetimes.append((starttime,onlinetime))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]</span><br><span class="line">real_X=np.array(onlinetimes).reshape((<span class="number">-1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">X=real_X[:,<span class="number">0</span>:<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">db=skc.DBSCAN(eps=<span class="number">0.01</span>,min_samples=<span class="number">20</span>).fit(X)</span><br><span class="line">labels = db.labels_</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Labels:'</span>)</span><br><span class="line">print(labels)</span><br><span class="line">raito=len(labels[labels[:] == <span class="number">-1</span>]) / len(labels)</span><br><span class="line">print(<span class="string">'Noise raito:'</span>,format(raito, <span class="string">'.2%'</span>))</span><br><span class="line"></span><br><span class="line">n_clusters_ = len(set(labels)) - (<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> labels <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">print(<span class="string">"Silhouette Coefficient: %0.3f"</span>% metrics.silhouette_score(X, labels))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    print(<span class="string">'Cluster '</span>,i,<span class="string">':'</span>)</span><br><span class="line">    print(list(X[labels == i].flatten()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.hist(X,<span class="number">24</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Labels:</span><br><span class="line">[ 0 -1  0  1 -1  1  0  1  2 -1  1  0  1  1  3 -1 -1  3 -1  1  1 -1  1  3  4</span><br><span class="line"> -1  1  1  2  0  2  2 -1  0  1  0  0  0  1  3 -1  0  1  1  0  0  2 -1  1  3</span><br><span class="line">  1 -1  3 -1  3  0  1  1  2  3  3 -1 -1 -1  0  1  2  1 -1  3  1  1  2  3  0</span><br><span class="line">  1 -1  2  0  0  3  2  0  1 -1  1  3 -1  4  2 -1 -1  0 -1  3 -1  0  2  1 -1</span><br><span class="line"> -1  2  1  1  2  0  2  1  1  3  3  0  1  2  0  1  0 -1  1  1  3 -1  2  1  3</span><br><span class="line">  1  1  1  2 -1  5 -1  1  3 -1  0  1  0  0  1 -1 -1 -1  2  2  0  1  1  3  0</span><br><span class="line">  0  0  1  4  4 -1 -1 -1 -1  4 -1  4  4 -1  4 -1  1  2  2  3  0  1  0 -1  1</span><br><span class="line">  0  0  1 -1 -1  0  2  1  0  2 -1  1  1 -1 -1  0  1  1 -1  3  1  1 -1  1  1</span><br><span class="line">  0  0 -1  0 -1  0  0  2 -1  1 -1  1  0 -1  2  1  3  1  1 -1  1  0  0 -1  0</span><br><span class="line">  0  3  2  0  0  5 -1  3  2 -1  5  4  4  4 -1  5  5 -1  4  0  4  4  4  5  4</span><br><span class="line">  4  5  5  0  5  4 -1  4  5  5  5  1  5  5  0  5  4  4 -1  4  4  5  4  0  5</span><br><span class="line">  4 -1  0  5  5  5 -1  4  5  5  5  5  4  4]</span><br><span class="line">Noise raito: 22.15%</span><br><span class="line">Estimated number of clusters: 6</span><br><span class="line">Silhouette Coefficient: 0.710</span><br><span class="line">Cluster  0 :</span><br><span class="line">[22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22]</span><br><span class="line">Cluster  1 :</span><br><span class="line">[23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23]</span><br><span class="line">Cluster  2 :</span><br><span class="line">[20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]</span><br><span class="line">Cluster  3 :</span><br><span class="line">[21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]</span><br><span class="line">Cluster  4 :</span><br><span class="line">[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]</span><br><span class="line">Cluster  5 :</span><br><span class="line">[7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/stuonline1.PNG" alt="图一"></p><h2 id="上网时长"><a href="#上网时长" class="headerlink" title="上网时长"></a>上网时长</h2><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sklearn.cluster <span class="keyword">as</span> skc</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">mac2id=dict()</span><br><span class="line">onlinetimes=[]</span><br><span class="line">f=open(<span class="string">'F:\data\clustering\TestData.txt'</span>,encoding=<span class="string">'utf-8'</span>)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">    mac=line.split(<span class="string">','</span>)[<span class="number">2</span>]</span><br><span class="line">    onlinetime=int(line.split(<span class="string">','</span>)[<span class="number">6</span>])</span><br><span class="line">    starttime=int(line.split(<span class="string">','</span>)[<span class="number">4</span>].split(<span class="string">' '</span>)[<span class="number">1</span>].split(<span class="string">':'</span>)[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">if</span> mac <span class="keyword">not</span> <span class="keyword">in</span> mac2id:</span><br><span class="line">        mac2id[mac]=len(onlinetimes)</span><br><span class="line">        onlinetimes.append((starttime,onlinetime))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        onlinetimes[mac2id[mac]]=[(starttime,onlinetime)]</span><br><span class="line">real_X=np.array(onlinetimes).reshape((<span class="number">-1</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">X=np.log(<span class="number">1</span>+real_X[:,<span class="number">1</span>:])</span><br><span class="line">db = skc.DBSCAN(eps=<span class="number">0.14</span>,min_samples=<span class="number">10</span>).fit(X)</span><br><span class="line">labels = db.labels_</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Labels:'</span>)</span><br><span class="line">print(labels)</span><br><span class="line">ratio=len(labels[labels[:] == <span class="number">-1</span>])/len(labels)</span><br><span class="line">print(<span class="string">'Noise raito:'</span>,format(ratio,<span class="string">'.2%'</span>))</span><br><span class="line"></span><br><span class="line">n_clusters_ = len(set(labels))-(<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> labels <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Estimated number of clusters: %d'</span> % n_clusters_)</span><br><span class="line">print(<span class="string">"Silhouette Coefficient :%0.3f"</span>% metrics.silhouette_score(X, labels))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    print(<span class="string">'Cluster'</span>,i,<span class="string">':'</span>)</span><br><span class="line">    count = len(X[labels ==i])</span><br><span class="line">    mean = np.mean(real_X[labels == i][:,<span class="number">1</span>])</span><br><span class="line">    std=np.std(real_X[labels ==i][:,<span class="number">1</span>])</span><br><span class="line">    print(<span class="string">'\t number of sample : '</span>,count)</span><br><span class="line">    print(<span class="string">'\t mean of sample: '</span>,format(mean,<span class="string">'.1f'</span>))</span><br><span class="line">    print(<span class="string">'\t std of sample: '</span>,format(std,<span class="string">'.1f'</span>))</span><br><span class="line"></span><br><span class="line">plt.hist(X,<span class="number">24</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h3 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">Labels:</span><br><span class="line">[ 0  1  0  4  1  2  0  2  0  3 -1  0 -1 -1  0  3  1  0  3  2  2  1  2  0  1</span><br><span class="line">  1 -1 -1  0  0  0  0  1  0 -1  0  0  0  2  0  1  0 -1 -1  0  0  0  3  2  0</span><br><span class="line"> -1  1  0  1  0  0 -1  2  0  0  0  1  3  3  0  2  0 -1  3  0  0  2  0  0  0</span><br><span class="line">  2  1 -1  0  0  0  0  0  0  1 -1  0  3  1  0  1  1  0  1  0  1  0  0 -1  1</span><br><span class="line">  1  0  0  2  0  0  0  2  2  0  0  0 -1  0  0  4  0  1  2 -1  0  1  0  2  0</span><br><span class="line"> -1 -1 -1  0  1  1  3 -1  0  1  0  2  0  0  2  1  1  0  0  0  0  4 -1  0  0</span><br><span class="line">  0  0  2  0  0  0  0 -1  2  0  0  0  0  4  0  0 -1  0  2  0  0 -1  0  1  4</span><br><span class="line">  0  0 -1  1  1  0  0  2  0  0  3 -1 -1 -1  1  0  0  2  1  0 -1 -1  3  2  2</span><br><span class="line">  0  0  3  0  1  0  0  0  3  2  0 -1  0  1 -1 -1  0  2  2  1  4  0  0  1  0</span><br><span class="line">  2  0  0  0  0  1  1  0  0  1  0  4 -1 -1  0  0  0 -1 -1  1 -1  4 -1  0  2</span><br><span class="line">  2 -1  2  1  2 -1  0 -1  0  2  2  1 -1  0  1  2 -1 -1  1 -1  2 -1 -1  1  4</span><br><span class="line">  2  3  1  0  4  0  0  4  2  4  0  0  2 -1]</span><br><span class="line">Noise raito: 16.96%</span><br><span class="line">Estimated number of clusters: 5</span><br><span class="line">Silhouette Coefficient :0.227</span><br><span class="line">Cluster 0 :</span><br><span class="line"><span class="code">     number of sample :  128</span></span><br><span class="line"><span class="code">     mean of sample:  5864.3</span></span><br><span class="line"><span class="code">     std of sample:  3498.1</span></span><br><span class="line">Cluster 1 :</span><br><span class="line"><span class="code">     number of sample :  46</span></span><br><span class="line"><span class="code">     mean of sample:  36835.1</span></span><br><span class="line"><span class="code">     std of sample:  11314.1</span></span><br><span class="line">Cluster 2 :</span><br><span class="line"><span class="code">     number of sample :  40</span></span><br><span class="line"><span class="code">     mean of sample:  843.2</span></span><br><span class="line"><span class="code">     std of sample:  242.9</span></span><br><span class="line">Cluster 3 :</span><br><span class="line"><span class="code">     number of sample :  14</span></span><br><span class="line"><span class="code">     mean of sample:  16581.6</span></span><br><span class="line"><span class="code">     std of sample:  1186.7</span></span><br><span class="line">Cluster 4 :</span><br><span class="line"><span class="code">     number of sample :  12</span></span><br><span class="line"><span class="code">     mean of sample:  338.4</span></span><br><span class="line"><span class="code">     std of sample:  31.9</span></span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/stuonline2.PNG" alt="图二"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;此篇笔记主要根据南京大学礼欣老师的&lt;a href=&quot;http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Python机器学习应用》&lt;/a&gt;整理而成，详细内容请看礼欣老师的mooc课程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="python" scheme="http://jchanji.github.io/tags/python/"/>
    
      <category term="machinelearning" scheme="http://jchanji.github.io/tags/machinelearning/"/>
    
      <category term="DBSCAN" scheme="http://jchanji.github.io/tags/DBSCAN/"/>
    
  </entry>
  
  <entry>
    <title>对四维的鸢尾花数据使用PCA进行降维</title>
    <link href="http://jchanji.github.io/year/01/09/pca/"/>
    <id>http://jchanji.github.io/year/01/09/pca/</id>
    <published>2018-01-08T16:00:00.000Z</published>
    <updated>2018-01-08T16:47:30.892Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><blockquote><p>此篇笔记主要根据南京大学礼欣老师的<a href="http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce" target="_blank" rel="noopener">《Python机器学习应用》</a>整理而成，详细内容请看礼欣老师的mooc课程。</p></blockquote><h2 id="数据介绍："><a href="#数据介绍：" class="headerlink" title="数据介绍："></a>数据介绍：</h2><p>对四维的鸢尾花数据使用PCA进行降维并且可视化，数据格式如下：<br><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/iris.PNG" alt="图1"><br><a id="more"></a></p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">data = load_iris()</span><br><span class="line">y = data.target</span><br><span class="line">X = data.data</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">reduced_X = pca.fit_transform(X)</span><br><span class="line"></span><br><span class="line">red_x, red_y = [], []</span><br><span class="line">blue_x, blue_y = [], []</span><br><span class="line">green_x, green_y = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(reduced_X)):</span><br><span class="line">    <span class="keyword">if</span> y[i] == <span class="number">0</span>:</span><br><span class="line">        red_x.append(reduced_X[i][<span class="number">0</span>])</span><br><span class="line">        red_y.append(reduced_X[i][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">elif</span> y[i] == <span class="number">1</span>:</span><br><span class="line">        blue_x.append(reduced_X[i][<span class="number">0</span>])</span><br><span class="line">        blue_y.append(reduced_X[i][<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        green_x.append(reduced_X[i][<span class="number">0</span>])</span><br><span class="line">        green_y.append(reduced_X[i][<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.scatter(red_x, red_y, c=<span class="string">'r'</span>, marker=<span class="string">'x'</span>)</span><br><span class="line">plt.scatter(blue_x, blue_y, c=<span class="string">'b'</span>, marker=<span class="string">'D'</span>)</span><br><span class="line">plt.scatter(green_x, green_y, c=<span class="string">'g'</span>, marker=<span class="string">'.'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/img/iris_res.PNG" alt="图1"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;此篇笔记主要根据南京大学礼欣老师的&lt;a href=&quot;http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Python机器学习应用》&lt;/a&gt;整理而成，详细内容请看礼欣老师的mooc课程。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;数据介绍：&quot;&gt;&lt;a href=&quot;#数据介绍：&quot; class=&quot;headerlink&quot; title=&quot;数据介绍：&quot;&gt;&lt;/a&gt;数据介绍：&lt;/h2&gt;&lt;p&gt;对四维的鸢尾花数据使用PCA进行降维并且可视化，数据格式如下：&lt;br&gt;&lt;img src=&quot;https://raw.githubusercontent.com/jChanJi/static_resource/master/img/iris.PNG&quot; alt=&quot;图1&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="python" scheme="http://jchanji.github.io/tags/python/"/>
    
      <category term="machinelearning" scheme="http://jchanji.github.io/tags/machinelearning/"/>
    
      <category term="PCA" scheme="http://jchanji.github.io/tags/PCA/"/>
    
  </entry>
  
  <entry>
    <title>Single Shot MultiBox Detector翻译</title>
    <link href="http://jchanji.github.io/year/12/29/SSD-tensorflow/"/>
    <id>http://jchanji.github.io/year/12/29/SSD-tensorflow/</id>
    <published>2017-12-29T02:31:55.900Z</published>
    <updated>2018-01-07T17:49:04.870Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>毕设的课题为基于SSD的深度学习目标检测研究，首先要对SSD框架的思想做到非常的了解，借此机会准备详细的翻译理解一下SSD的内容，如有错误指出，欢迎指出。原文链接：<a href="点我">https://arxiv.org/abs/1512.02325</a></p></blockquote><h2 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><blockquote><p>我们展示了使用单层神经网络来检测图片中的物体的方法，称之为SSD,它将输出空间的bounding boxes (边界框)离散化为每一个特征图上的一系列不同纵横比和尺寸的（default boxes）默认框。在进行预测的时候，神经网络会对每个默认框中的每一个物体种类的存在形成一个分数，并且对默认框进行调整以更好的匹配目标的形状 。另外，这个网络还用了不同的方法来联合不同特征图上的预测结果以自然的解决不同尺寸的物体。SSD是一个相对简单的方法，它需要候选对象，因为它完全排除了生成候选对象和接下来对像素和特征重新采样的阶段并且将所有的计算都压缩在了一个单层网络里。这让SSD很容易的训练并且直接的应用到需要检测组件的系统中去。在PASACAL、VOC、COCO和ILASVRC数据集上的实验证明了SSD在精确度上和那些利用额外的候选目标步骤的方法相比很有竞争力并且更快，于此同时SSD还为训练和接口提供了一个统一的框架。，对PASCAL VOC 2007的数据集，在300×300像素的尺寸输入，SSD在Nvidia Titan X上59FPS时达到72.1％的mAP，500×500像素尺寸输入SSD达到75.1％的mAP，超过了相比较的state of art 模型Faster R-CNN 。和其他的单个步骤的方法相比，SSD对于更小尺寸图片的输入有着更好的检出率。<br><a id="more"></a></p><h3 id="The-Single-Shot-Detector-SSD"><a href="#The-Single-Shot-Detector-SSD" class="headerlink" title="The Single Shot Detector (SSD)"></a>The Single Shot Detector (SSD)</h3><p>在模型的训练期间中只需要输入原图和ground truth boxes（真实边框图）（如：图四 (a)）。卷积处理中，在许多不同尺寸的特征图（例如8x8 feature map和4x4 feature map）中评估出一系列不同宽比的边界框值，其中默认的为四个，对于每一个默认框评估出所有目标的形状偏移和置信度。在训练时，我们首先将这些默认框匹配到真实标签框。这些框为正，其余视为负。模型损失是定位损失和置信损失之间的加权和。</p></blockquote><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/SSD/feature_map.PNG" alt="feature map"></p><h3 id="model"><a href="#model" class="headerlink" title="model"></a>model</h3><blockquote><p>SSD基于前馈卷积网络，其产生固定大小的边界框集合和框中对象类别的分数，接着是非最大化抑制步骤以产生最终检测。早期的网络基于使用高质量图片分类的标准结构，我们称之为基础网络。后来在基础网络的上进行了改进加上了辅助结构(图四 SSD的Extra Feature Layers部分)。其中关键的特征如下：</p><p>⑴在检测的时候使用不同尺寸的特征图<br><br>在被删去的基础网络后面加上了卷积特征层，这些特征层减少了逐渐增多的<br>尺寸并且能够对不同尺寸的对象进行预测。这个预测检测目标的卷积模型每一个特征层都和YOLO不相同。</p><p>⑵检测的卷积预测器<br><br>每一个添加了特征层或者从基础网络中选择了一个特征层的网络能够使用一个卷积过滤器的集合产生一个预测目标的确定集合。在图五SSD模型的顶部表明了这一特点。对一个m x n并且有p个通道的特征层来说，用来预测潜在目标的参数的基础元素有3 x 3 x p个小核心，它用来产生分数或者种类或者相对于默认框的相对形状偏移量。在每个核心应用到的m x n区域内它产生一个输出值。<br>边界框偏移量的输出值是相对与默认框的位置和特征图的位置测量的，而YOLO在这一步中使用中间全连接层而不是卷积过滤器。<br>与YOLO相比，他有如下的特征：首先它在基础网络的后面加上了一些特征层，从而能够对默认框的不同尺寸和方面的比例和他们置信联系的偏移进行预测。在VOC2007测试数据集上，SSD对于300x300尺寸的输入图片的训练结果远远的超过了YOLO对于424x425尺寸图片的训结果，并且在速度上也有很大的改进。</p><p>⑶默认框和纵横比<br><br>针对网络顶部的多特征图，我们将默认的边界框集合和每一个特征图单元联系起来。默认框使用卷积的方式拼接特征图所以每一个默认框的位置相对于他对应的单元是固定的。在每一个特征图单元中我们能够预测每个单元中相对于默认框的形状偏移量和在每一个边框中表明一个类实体存在的分数。特别的，对于给定k个边框的每个边框，我们计算c类的分数和4个相对于原始默认框形状的偏移。在特征图的每个特征单元上需要有（c+4）x k个过滤器，对于m x n的特征图就会产生（c + 4） x k x m x n个输出。这里的默认框很类似于Faster R-CNN中的锚框，但是这里将他应用到不同解决方案的一些特征图中。在一些特征图中允许不同形状的默认框能偶有效的离散化可能的输出框形状的空间。</p></blockquote><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/SSD/model.PNG" alt="model"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;毕设的课题为基于SSD的深度学习目标检测研究，首先要对SSD框架的思想做到非常的了解，借此机会准备详细的翻译理解一下SSD的内容，如有错误指出，欢迎指出。原文链接：&lt;a href=&quot;点我&quot;&gt;https://arxiv.org/abs/1512.02325&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;翻译&quot;&gt;&lt;a href=&quot;#翻译&quot; class=&quot;headerlink&quot; title=&quot;翻译&quot;&gt;&lt;/a&gt;翻译&lt;/h2&gt;&lt;h3 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;我们展示了使用单层神经网络来检测图片中的物体的方法，称之为SSD,它将输出空间的bounding boxes (边界框)离散化为每一个特征图上的一系列不同纵横比和尺寸的（default boxes）默认框。在进行预测的时候，神经网络会对每个默认框中的每一个物体种类的存在形成一个分数，并且对默认框进行调整以更好的匹配目标的形状 。另外，这个网络还用了不同的方法来联合不同特征图上的预测结果以自然的解决不同尺寸的物体。SSD是一个相对简单的方法，它需要候选对象，因为它完全排除了生成候选对象和接下来对像素和特征重新采样的阶段并且将所有的计算都压缩在了一个单层网络里。这让SSD很容易的训练并且直接的应用到需要检测组件的系统中去。在PASACAL、VOC、COCO和ILASVRC数据集上的实验证明了SSD在精确度上和那些利用额外的候选目标步骤的方法相比很有竞争力并且更快，于此同时SSD还为训练和接口提供了一个统一的框架。，对PASCAL VOC 2007的数据集，在300×300像素的尺寸输入，SSD在Nvidia Titan X上59FPS时达到72.1％的mAP，500×500像素尺寸输入SSD达到75.1％的mAP，超过了相比较的state of art 模型Faster R-CNN 。和其他的单个步骤的方法相比，SSD对于更小尺寸图片的输入有着更好的检出率。&lt;br&gt;
    
    </summary>
    
      <category term="-artical" scheme="http://jchanji.github.io/categories/artical/"/>
    
    
      <category term="object detection" scheme="http://jchanji.github.io/tags/object-detection/"/>
    
  </entry>
  
  <entry>
    <title>知识图谱</title>
    <link href="http://jchanji.github.io/year/11/19/knowledgegraph/"/>
    <id>http://jchanji.github.io/year/11/19/knowledgegraph/</id>
    <published>2017-11-19T14:52:15.921Z</published>
    <updated>2017-11-23T01:31:47.214Z</updated>
    
    <content type="html"><![CDATA[<h1 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h1><h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;知识图谱于2012年5月17日被Google正式提出， 其初衷是为了提高搜索引擎的能力，增强用户的搜索质量以及搜索体验。，RDF (resource description framework)<sup>[1]</sup>模式(RDF schema) （应用）和万维网本体语言(Web ontology language，OWL) 的形式化模型就是基于上述目的产生的。</p></blockquote><a id="more"></a><blockquote><p>[1]RDF: RDF是一个处理元数据的XML,RDF使用XML语法和RDF Schema（RDFS）来将元数据描述成为数据模型。是描述语义层面的本体关系的语言。</p><p>[2]数据模型: 数据模型（Data Model）是数据特征的抽象。数据（Data）是描述事物的符号记录，模型（Model)是现实世界的抽象。数据模型所描述的内容有三部分：数据结构、数据操作和数据约束。</p></blockquote><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;知识图谱是Google用于增强其搜索引擎功能的知识库。本质上,知识图谱是一种揭示实体之间关系的 <strong>语义网络（semantic network）</strong> ,即具 有有向图结构的一个知识库，其中图的结点代 表实体（entity）或者概念（concept），而图的 边代表实体/ 概念之间的各种语义关系，可以对现实世界的事物及其相互关系进行形式化地描述。现在的知识图谱已被用来泛指各种大规模的知识库。</p></blockquote><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/knowledgegraph2.png" alt="知识图谱"></p><h2 id="应用领域"><a href="#应用领域" class="headerlink" title="应用领域"></a>应用领域</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;广泛应用于智能搜索、 智能问答、个性化推荐等领域。尤其是在智能搜索中，用户的搜索请求不再局限于简单的关键词匹配，搜索将根据用户查询的情境与意图进行推理，实现 概念检索。与此同时，用户的搜索结果将具有层次 化、结构化等重要特征。</p></blockquote><h2 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;<strong>知识图谱也可分为通用知识图谱(开放链接知识库)和行业知识图谱(垂直行业知识库)</strong>。</p></blockquote><h3 id="通用知识图谱"><a href="#通用知识图谱" class="headerlink" title="通用知识图谱"></a>通用知识图谱</h3><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;通用知识图谱注重广度，强调融合更多的实体，较行业知识图谱而言,其准确度不够高,并且受概念范围的影响,很难借助本体库对公理、规则以及约束条件的支持能力规范其实 体、属性、实体间的关系等。通用知识图谱主要应 用于智能搜索等领域。行业知识图谱通常需要依靠 特定行业的数据来构建，具有特定的行业意义。</p></blockquote><h3 id="行业知识图谱"><a href="#行业知识图谱" class="headerlink" title="行业知识图谱"></a>行业知识图谱</h3><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;行业知识图谱中，实体的属性与数据模式往往比较丰富，需要考虑到不同的业务场景与使用人员。</p></blockquote><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><h3 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h3><blockquote><p> &nbsp; &nbsp; &nbsp; &nbsp;知识图谱在逻辑上可分为 <strong>模式层</strong> 与 <strong>数据层</strong> 两个层次.</p></blockquote><h4 id="1、数据层"><a href="#1、数据层" class="headerlink" title="1、数据层"></a>1、数据层</h4><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用(实体1，关系， 实体2)、(实体、属性，属性值)这样的三元组来表达事实,可选择 <strong>图数据库</strong> 作为存储介质。</p></blockquote><h4 id="2、模式层"><a href="#2、模式层" class="headerlink" title="2、模式层"></a>2、模式层</h4><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;模式层构建在数据层之上，主要是通过 <strong>本体库</strong> 来规范数据层的一系列事实表达。 <strong>本体是结构化知识库的概念模板</strong> ，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</p></blockquote><h3 id="体系结构（构建模式）"><a href="#体系结构（构建模式）" class="headerlink" title="体系结构（构建模式）"></a>体系结构（构建模式）</h3><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;知识图谱的体系架构知识图谱的体系架构是其指构建模式结构，如图1所示。其中虚线框内的部分为知识图谱的构建过 程，该过程需要随人的认知能力不断更新迭代。知识图谱主要有 <strong>自顶向下(top-down)</strong> 与 <strong>自底向上(bottom-up)</strong> 两种构建方式。</p></blockquote><h4 id="自顶向上"><a href="#自顶向上" class="headerlink" title="自顶向上"></a>自顶向上</h4><blockquote><p>自顶向下指的是: <strong>先为知识图谱定义好本体与数据模式，再将实体加入到知识库</strong>。该构建方式需要利用一些现有的结构化知识库作为其基础知识库，例如Freebase项目就是采用这种方式，它的绝大部分数据是从维基百科中得到的。</p></blockquote><h4 id="自底向上"><a href="#自底向上" class="headerlink" title="自底向上"></a>自底向上</h4><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;自底向上指的是 <strong>从一些开放链接数据中提取出实体，选择其中置信度较高的加入到知识库，再构建顶层的本体模式</strong> 。目前，大多数知识图谱都采用自底向上的方式进行构建，其中典型就是 Google的Knowledge Vault。</p></blockquote><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/1.PNG" alt="图1"></p><h2 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h2><h3 id="知识抽取"><a href="#知识抽取" class="headerlink" title="知识抽取"></a>知识抽取</h3><h4 id="知识抽取-1"><a href="#知识抽取-1" class="headerlink" title="知识抽取"></a>知识抽取</h4><h5 id="基于规则与词典的实体抽取方法"><a href="#基于规则与词典的实体抽取方法" class="headerlink" title="基于规则与词典的实体抽取方法"></a>基于规则与词典的实体抽取方法</h5><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;早期的实体抽取是在限定文本领域、限定语义 单元类型的条件下进行的，主要采用的是基于规则与词典的方法， <strong>例如使用已定义的规则，抽取出文本中的人名、地名、组织机构名、特定时间等实体。</strong></p></blockquote><h5 id="基于统计机器学习的实体抽取方法"><a href="#基于统计机器学习的实体抽取方法" class="headerlink" title="基于统计机器学习的实体抽取方法"></a>基于统计机器学习的实体抽取方法</h5><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;随后，研究者尝试将机器学习中的 <strong>监督学习</strong> 算法用于命名实体的抽取问题上。单纯的监督学习算法在性能上 不仅受到训练集合的限制，并且算法的准确率与召回率都不够理想。相关研究者认识到监督学习算法的制约性后，尝试将监督学习算法与规则相互结合。</p></blockquote><h5 id="面向开放域的实体抽取方法"><a href="#面向开放域的实体抽取方法" class="headerlink" title="面向开放域的实体抽取方法"></a>面向开放域的实体抽取方法</h5><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;其基本思想是通过 <strong>少量的实体实例建立特征模型，再通过该模型应用于新的数据集得到新的命名实体</strong>。基于 <strong>无监督学习</strong> 的开放域聚类算法，其基本思想是基于已知实体的语义特征去搜索日志中识别出命名的实体，然后进行聚类。</p></blockquote><h4 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h4><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;关系抽取的目标是解决实体间语义链接的问题。主要有效的方法是基于马尔可夫逻辑网和基于本体推理的深层隐含关系抽取方法，主要有一下俩个分类。</p></blockquote><h5 id="开放式实体关系抽取"><a href="#开放式实体关系抽取" class="headerlink" title="开放式实体关系抽取"></a>开放式实体关系抽取</h5><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;开放式实体关系抽取可分为二元开放式关系抽 取和n元开放式关系抽取。</p></blockquote><h5 id="基于联合推理的实体关系抽取"><a href="#基于联合推理的实体关系抽取" class="headerlink" title="基于联合推理的实体关系抽取"></a>基于联合推理的实体关系抽取</h5><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;联合推理的关系抽取中的典型方法是马尔可夫逻辑网MLN(Markov logic network)<sup>[1]</sup>。</p><p>[1] 马尔可夫逻辑网:</p></blockquote><h3 id="属性抽取"><a href="#属性抽取" class="headerlink" title="属性抽取"></a>属性抽取</h3><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;属性抽取主要是针对实体而言的，通过属性可形成对实体的完整勾画。由于实体的属性可以看成是 <strong>实体与属性值之间的一种名称性关系</strong> ，因此可以将实体属性的抽取问题转换为关系抽取问题。</p><p>&nbsp; &nbsp; &nbsp; &nbsp;大量的属性数据主要存在于半结构化、非结构化的大规模开放域数据集中。抽取这些属性的方法，一种是将上述从百科网站上抽取的 <strong>结构化数据作为可用于属性抽取的训练集，然后再将该模型应用于开放域中的实体属性抽取</strong> 。另一种是 <strong>根据实体属性与属性值之间的关系模式，直接从开放域数据集上抽取属性。</strong></p></blockquote><h1 id="本体"><a href="#本体" class="headerlink" title="本体"></a>本体</h1><h2 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h2><blockquote><p> &nbsp; &nbsp; &nbsp; &nbsp;Gruber给出了Ontology的一个最为流行的定义,即“Ontology是概念模型<sup>[1]</sup> 的明确的规范说明”。</p><p>[1] 概念模型: “概念模型” 指通过抽象出客观世界中一些现象的相关概念而得到的模型。首先把现实世界中的客观对象抽象为某一种信息结构，这种信息结构并不依赖于具体的计算机系统，不是某一个数据库管理系统（DBMS）支持的 <strong>数据模型</strong> ，而是概念级的模型，称为概念模型。<br></p><p>[2] 数据模型: 数据模型（Data Model）是数据特征的抽象。数据是描述事物的符号记录，模型是现实世界的抽象。数据模型为数据库系统的信息表示与操作提供了一个抽象的框架。数据模型所描述的内容有三部分：数据结构、数据操作和数据约束。</p></blockquote><h2 id="举例解释"><a href="#举例解释" class="headerlink" title="举例解释"></a>举例解释</h2><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;上面的概念很抽象，不是很好理解，其实本体的概念有两层意思，一层是哲学层面的意思，一层是引申到信息科学中的语义层面的意思。</p></blockquote><h3 id="哲学上的本体"><a href="#哲学上的本体" class="headerlink" title="哲学上的本体"></a>哲学上的本体</h3><blockquote><p>“鼠标”，“mouse”,</p></blockquote><p><img src="https://raw.githubusercontent.com/jChanJi/static_resource/master/knowledgegraph/entity1.PNG" alt="符号"></p><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;等都是表示”鼠标”这个本体的的”符号”。由此可见“本体”是只可意会不可言传的，因为所有的描述都成为了“本体”的外在符号，我们世界上的所有图像、语言、我们看到的、听到的、感受到的，都成为符号到本体的某种映射。</p></blockquote><h3 id="信息科学中的本体"><a href="#信息科学中的本体" class="headerlink" title="信息科学中的本体"></a>信息科学中的本体</h3><blockquote><p>&nbsp; &nbsp; &nbsp; &nbsp;Ontology是一种 <strong>描述术语</strong> （包含哪些词汇）及 <strong>术语间关系</strong> （描述苹果、香蕉、水果之间的关系）的概念模型。Ontology的形式可简单可复杂。最简单的词汇表（只定义术语集合，不定义术语之间的关系）也可以看成是一种“本体”；但严格意义上的本体，是既定义了术语、也定义了术语之间关系的。生活中，最常见、最成熟的本体，就属图书馆里的图书分类法。本体，以图书分类法为例，一方面限定了术语集合（即规定大家必须采用共同承认的一套词汇，禁止私自发明新词），另一方面定义术语之间的上下位关系（如：计算机技术隶属于工业技术，软件技术隶属于计算机技术，等等）。只要大家都认同该本体，并在实践中长期遵守该本体，依照它来编排和索引书目，那么日后寻找一本书就会非常方便。</p></blockquote><h2 id="本体与知识图谱-语义网-的区别"><a href="#本体与知识图谱-语义网-的区别" class="headerlink" title="本体与知识图谱(语义网)的区别"></a>本体与知识图谱(语义网)的区别</h2><blockquote><p>1、Ontology 是对共享概念模型的规范说明 ,这里所说的“共享概念模型” 指该模型中的 <strong>概念是公认</strong> 的 ,至少在某个特定的领域是公认的。一般情况下 ,Ontology 是 <strong>面向特定领域</strong>  , 用于描述特定领域的概念模型。<br></p><p>2、语义网络从数学上说 ,是一种带有标记的有向图。它最初用 于表示命题信息 ,现广泛应用于专家系统表示知识。语义网络中节点表示物理实体、概念或状态 ,连接节点的边用于表示关系。语义网络中对节点和边没有其他特殊的规定 ,因此 <strong>语义网络描述的对象或范围比Ontology广。</strong></p></blockquote><p>引用<br><br>[1] 徐增林, 盛泳潘, 贺丽荣,等. 知识图谱技术综述[J]. 电子科技大学学报, 2016, 45(4):589-606.<br><br>[2] 漆桂林, 高桓, 吴天星. 知识图谱研究进展[J]. 情报工程, 2017, 3(1):4-25.<br><br>[3] 邓志鸿, 唐世渭, 张铭,等. Ontology研究综述[J]. 北京大学学报(自然科学版), 2002, 38(5):730-738.<br><br>[4]李国洪, 梁保城, 赵毅,等. Ontology研究的知识图谱演化[J]. 情报杂志, 2013(3):101-105.<br><br>[5] Gruber T R. A Translation Approach to Portable Ontology Specifications. Knowledge Acquisition ,1993 ,5 :199～220</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;知识图谱&quot;&gt;&lt;a href=&quot;#知识图谱&quot; class=&quot;headerlink&quot; title=&quot;知识图谱&quot;&gt;&lt;/a&gt;知识图谱&lt;/h1&gt;&lt;h2 id=&quot;起源&quot;&gt;&lt;a href=&quot;#起源&quot; class=&quot;headerlink&quot; title=&quot;起源&quot;&gt;&lt;/a&gt;起源&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;知识图谱于2012年5月17日被Google正式提出， 其初衷是为了提高搜索引擎的能力，增强用户的搜索质量以及搜索体验。，RDF (resource description framework)&lt;sup&gt;[1]&lt;/sup&gt;模式(RDF schema) （应用）和万维网本体语言(Web ontology language，OWL) 的形式化模型就是基于上述目的产生的。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-arcticals" scheme="http://jchanji.github.io/categories/arcticals/"/>
    
    
  </entry>
  
  <entry>
    <title>Arabesque:A System for Distributed Graph Mining</title>
    <link href="http://jchanji.github.io/year/10/30/artical_1/"/>
    <id>http://jchanji.github.io/year/10/30/artical_1/</id>
    <published>2017-10-30T14:35:37.000Z</published>
    <updated>2018-01-08T16:21:28.422Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Arabesque-A-System-for-Distributed-Graph-Mining"><a href="#Arabesque-A-System-for-Distributed-Graph-Mining" class="headerlink" title="Arabesque: A System for Distributed Graph Mining"></a>Arabesque: A System for Distributed Graph Mining</h2><blockquote><h3 id="Arabesque-分布式的图挖掘系统"><a href="#Arabesque-分布式的图挖掘系统" class="headerlink" title="Arabesque:分布式的图挖掘系统"></a>Arabesque:分布式的图挖掘系统</h3></blockquote><h4 id="原文链接：点我跳转"><a href="#原文链接：点我跳转" class="headerlink" title="原文链接：点我跳转"></a>原文链接：<a href="https://github.com/jChanJi/jchanji.github.com/blob/master/meterial/093-teixeira.pdf" target="_blank" rel="noopener">点我跳转</a></h4><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><p><a href="#Abstract">Abstract</a></p></li><li><p>1.<a href="#introduction">Introduction</a></p></li></ul><a id="more"></a><ul><li><p>2.Graph Mining Problems</p></li><li><p>3.The Filter-Process Model</p><ul><li><p>3.1.Computational Model</p></li><li><p>3.2.Alternative Paradigms: Think Like a Vertex and Think Like a Pattern</p></li></ul></li><li><p>4.Arabesque: API, Programming, and Implementation</p><ul><li><p>4.1 Arabesque API</p></li><li><p>4.2 Programming with Arabesque</p></li><li><p>4.3 Arabesque implementation</p></li></ul></li><li><p>5.Graph Exploration Techniques</p><ul><li><p>5.1 Coordination-Free Exploration Strategy</p></li><li><p>5.2 Storing Embeddings Compactly</p></li><li><p>5.3 Partitioning Embeddings for Load Balancing</p></li><li><p>5.4 Two-Level Pattern Aggregation for Fast Pattern Canonicality Checking</p></li></ul></li><li><p>6.Evaluation</p><ul><li><p>6.1 Experimental Setup</p></li><li><p>6.2 Alternative Paradigms: TLV and TLP</p></li><li><p>6.3 Arabesque: The TLE Paradigm</p></li><li><p>6.4 Large Graphs with Arabesque</p></li></ul></li><li><p>7.Related Work</p></li><li><p>8.Conclusions</p></li><li><p>9.Acknowledgments</p></li><li><p>10.References</p></li></ul><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a><span id="Abstract">Abstract</span></h3><h4 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h4><h4 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h4><p>  Distributed data processing platforms such as MapReduce<br>and Pregel have substantially simplified the design and deployment<br>of certain classes of distributed graph analytics algorithms.<br>However, these platforms do not represent a good<br>match for distributed graph mining problems, as for example<br>finding frequent subgraphs in a graph. Given an input<br>graph, these problems require exploring a very large number<br>of subgraphs and finding patterns that match some “interestingness”<br>criteria desired by the user. These algorithms are<br>very important for areas such as social networks, semantic<br>web, and bioinformatics.</p><p>  In this paper, we present Arabesque, the first distributed<br>data processing platform for implementing graph mining<br>algorithms. Arabesque automates the process of exploring<br>a very large number of subgraphs. It defines a high-level<br>filter-process computational model that simplifies the development<br>of scalable graph mining algorithms: Arabesque explores<br>subgraphs and passes them to the application, which<br>must simply compute outputs and decide whether the subgraph<br>should be further extended. We use Arabesque’s API<br>to produce distributed solutions to three fundamental graph<br>mining problems: frequent subgraph mining, counting motifs,<br>and finding cliques. Our implementations require a<br>handful of lines of code, scale to trillions of subgraphs, and<br>represent in some cases the first available distributed solutions.</p><h4 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h4><p>  Distributed data processing platforms such as MapReduce<br>and Pregel have substantially simpliﬁed the design and deployment of certain classes of distributed graph analyticsal gorithms.</p><blockquote><p>分布式数据处理平台例如mapreduce和pregel实质上是简化了某些类的分布式图形化分析算法的设计和调度</p></blockquote><p> However, these platforms do not represent a good<br>match for distributed graph mining problems, as for example<br>finding frequent subgraphs in a graph.</p><blockquote><p>但是，这些平台没有表现出对分布式图形挖掘问题的匹配。就以在图表中频繁的寻找子图作为例子</p></blockquote><p>Given an input graph, these problems require exploring a very large number of subgraphs and finding patterns that match some “interestingness” criteria desired by the user.</p><blockquote><p>给出一个输入图表，这些问题需要扫描（探索）一个数量很多的子图并且寻找和用户期望的一些”兴趣性”准则相匹配的模式（图案,样品）。</p></blockquote><p>These algorithms are very important for areas such as social networks, semantic web, and bioinformatics.</p><blockquote><p>这些算法对例如社交网络，语义网，和分析复杂生物的学科的领域非常重要。</p></blockquote><p>In this paper, we present Arabesque, the first distributed<br>data processing platform for implementing graph mining<br>algorithms.<br>Arabesque automates the process of exploring a very large number of subgraphs.</p><blockquote><p>在这篇文献当中，我们介绍Arabesque,第一个实现图挖掘算法的分布式数据处理平台。Arabesque自动化了探索一个很大数量的子图的流程。</p></blockquote><p>It defines a high-level filter-process computational model that simplifies the development of scalable graph mining algorithms: Arabesque explores subgraphs and passes them to the application, which must simply compute outputs and decide whether the subgraph should be further extended.</p><blockquote><p>他定义了一个高级的过滤过程的计算模型，它简化了可升级的图挖掘算法的开发:Arabesque 探索子图并且将他们传递给应用程序，这个应用程序必须简单的计算输出和决定是否子图应该被进一步的被扩展。</p></blockquote><p>We use Arabesque’s API to produce distributed solutions to three fundamental graph mining problems: frequent subgraph mining, counting motifs,and finding cliques.</p><blockquote><p>我们用Arabesque的API去产生三个基础的图挖掘问题的分布式解决方案:频繁的子图挖掘，计数的图案，寻找派系。</p></blockquote><p>Our implementations require a handful of lines of code, scale to trillions of subgraphs, and represent in some cases the first available distributed solutions.</p><blockquote><p>我们的实现需要很少行的代码，规模数万亿的子图，和在某些情况下第一个可获得的分布式解决方案的示范</p></blockquote><h4 id="段落翻译"><a href="#段落翻译" class="headerlink" title="段落翻译"></a>段落翻译</h4><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>分布式数据处理平台例如mapreduce和pregel实质上是简化了某些类的分布式图形化分析算法的设计和调度.但是，这些平台没有表现出对分布式图形挖掘问题的匹配。就以在图表中频繁的寻找子图作为例子.给出一个输入图表，这些问题需要扫描（探索）一个数量很多的子图并且寻找和用户期望的一些”兴趣性”准则相匹配的模式（图案,样品）。这些算法对例如社交网络，语义网，和分析复杂生物的学科的领域非常重要。</p><p>在这篇文献当中，我们介绍Arabesque,第一个实现图挖掘算法的分布式数据处理平台。Arabesque自动化了探索一个很大数量的子图的流程。他定义了一个高级的过滤过程的计算模型，它简化了可升级的图挖掘算法的开发:Arabesque 探索子图并且将他们传递给应用程序，这个应用程序必须简单的计算输出和决定是否子图应该被进一步的被扩展.我们用Arabesque的API去产生三个基础的图挖掘问题的分布式解决方案:频繁的子图挖掘，计数的图案，寻找派系。我们的实现需要很少行的代码，规模数万亿的子图，和在某些情况下第一个可获得的分布式解决方案的示范</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a><span id="Introduction">Introduction</span></h3><h3 id="原文-1"><a href="#原文-1" class="headerlink" title="原文"></a>原文</h3><p>Graph data is ubiquitous in many fields, from the Web to advertising<br>and biology, and the analysis of graphs is becoming<br>increasingly important. The development of algorithms<br>for graph analytics has spawned a large amount of research,<br>especially in recent years. However, graph analytics has traditionally<br>been a challenging problem tackled by expert researchers,<br>who can either design new specialized algorithms<br>for the problem at hand, or pick an appropriate and sound<br>solution from a very vast literature. When the input graph or<br>the intermediate state or computation complexity becomes<br>very large, scalability is an additional challenge.</p><p>The development of graph processing systems such as<br>Pregel [25] has changed this scenario and made it simpler to<br>design scalable graph analytics algorithms. Pregel offers a<br>simple “think like a vertex” (TLV) programming paradigm,<br>where each vertex of the input graph is a processing element<br>holding local state and communicating with its neighbors<br>in the graph. TLV is a perfect match for problems that<br>can be represented through linear algebra, where the graph<br>is modeled as an adjacency matrix (or some other variant<br>like the Laplacian matrix) and the current state of each vertex<br>is represented as a vector. We call this class of methods<br>graph computation problems. A good example is computing<br>PageRank [6], which is based on iterative sparse matrix<br>and vector multiplication operations. TLV covers several<br>other algorithms that require a similar computational architecture,<br>for example, shortest path algorithms, and over the<br>years many optimizations of this paradigm have been proposed<br>[17, 26, 36, 42].</p><p>Despite this progress, there remains an important class<br>of algorithms that cannot be readily formulated using the<br>TLV paradigm. These are graph mining algorithms used<br>to discover relevant patterns that comprise both structurebased<br>and label-based properties of the graph. Graph mining<br>is widely used for several applications, for example, discovering<br>3D motifs in protein structures or chemical compounds,<br>extracting network motifs or significant subgraphs<br>from protein-protein or gene interaction networks, mining<br>attributed patterns over semantic data (e.g., in Resource<br>Description Framework or RDF format), finding structurecontent<br>relationships in social media data, dense subgraph mining for community and link spam detection in web data,among others. Graph mining algorithms typically take a labeled and immutable graph as input, and mine patterns<br>that have some algorithm-specific property (e.g., frequency<br>above some threshold) by finding all instances of these patterns<br>in the input graph. Some algorithms also compute aggregated<br>metrics based on these subgraphs</p><p><img src="./img/1.PNG" alt="图1"></p><p>Figure 1: Exponential growth of the intermediate state in<br>graph mining problems (motifs counting, clique finding,<br>FSM: Frequent subgraph mining) on different datasets.</p><p>Designing graph mining algorithms is a challenging and<br>active area of research. In particular, scaling graph mining<br>algorithms to even moderately large graphs is hard. The set<br>of possible patterns and their subgraphs in a graph can be<br>exponential in the size of the original graph, resulting in an<br>explosion of the computation and intermediate state. Figure<br>1 shows the exponential growth of the number of “interesting”<br>subgraphs of different sizes in some of the graph<br>mining problems and datasets we will evaluate in this paper.<br>Even graphs with few thousands of edges can quickly generate<br>hundreds of millions of interesting subgraphs. The need<br>for enumerating a large number of subgraphs characterizes<br>graph mining problems and distinguishes them from graph<br>computation problems. Despite this state explosion problem,<br>most graph mining algorithms are centralized because of the<br>complexity of distributed solutions.</p><p>In this paper, we propose automatic subgraph exploration<br>as a generic building block for solving graph mining<br>problems, and introduce Arabesque, the first embedding exploration<br>system specifically designed for distributed graph<br>mining. Conceptually, we move from TLV to “think like an<br>embedding” (TLE), where by embedding we denote a subgraph<br>representing a particular instance of a more general<br>template subgraph called a pattern (see Figure 2).</p><p>Arabesque defines a high-level filter-process computational<br>model. Given an input graph, the system takes care<br>of automatically and systematically visiting all the embeddings<br>that need to be explored by the user-defined algorithm,<br>performing this exploration in a distributed manner. The system<br>passes all the embeddings it explores to the application,<br>which consists primarily of two functions: filter, which indicates whether an embedding should be processed, and process,<br>which examines an embedding and may produce some<br>output. For example, in the case of finding cliques the filter<br>function prunes embeddings that are not cliques, since none<br>of their extensions can be cliques, and the process function<br>outputs all explored embeddings, which are cliques by construction.<br>Arabesque also supports the pruning of the exploration<br>space based on user-defined metrics aggregated across<br>multiple embeddings.</p><p><img src="./img/2.PNG" alt="图2"></p><p>Figure 2: Graph mining concepts: an input graph, an example<br>pattern, and the embeddings of the pattern. Colors represent<br>labels. Numbers denote vertex ids. Patterns and embeddings<br>are two types of subgraphs. However, a pattern is<br>a template, whereas an embedding is an instance. In this example,<br>the two embeddings are automorphic.</p><p>The Arabesque API simplifies and thus democratizes the<br>design of graph mining algorithms, and automates their execution<br>in a distributed setting. We used Arabesque to implement<br>and evaluate scalable solutions to three fundamental<br>and diverse graph mining problems: frequent subgraph mining,<br>counting motifs, and finding cliques. These problems<br>are defined precisely in Section 2. Some of these algorithms<br>are the first distributed solutions available in the literature,<br>which shows the simplicity and generality of Arabesque.</p><p>Arabesque’s embedding-centered API facilitates a highly<br>scalable implementation. The system scales by spreading<br>embeddings uniformly across workers, thus avoiding<br>hotspots. By making it explicit that embeddings are the fundamental<br>unit of exploration, Arabesque is able to use fast<br>coordination-free techniques, based on the notion of embedding<br>canonicality, to avoid redundant work and minimize<br>communication costs. It also enables us to store embeddings<br>efficiently using a new data structure called Overapproximating<br>Directed Acyclic Graph (ODAG), and to devise a<br>new two-level optimization for pattern-based aggregation,<br>which is a common operation in graph mining algorithms.</p><p>Arabesque is implemented as a layer on top of Apache<br>Giraph [3], a Pregel-inspired graph computation system,<br>thus allowing both graph computation and graph mining<br>algorithms to run on top of the same infrastructure. The<br>implementation does not use a TLV approach: it considers<br>Giraph just as a regular data parallel system implementing<br>the Bulk Synchronous Processing model.<br>To summarize, we make the following contributions:</p><p>• We propose embedding exploration, or “think like an embedding”,<br>as an effective basic building block for graph<br>mining. We introduce the filter-process computational<br>model (Section 3), design an API that enables embedding<br>exploration to be expressed effectively and succinctly,<br>and present three example graph mining applications<br>that can be elegantly expressed using the Arabesque<br>API (Section 4).</p><p>• We introduce techniques to make distributed embedding<br>exploration scalable: coordination-free work sharing, ef-<br>ficient storage of embeddings, and an important optimization<br>for pattern-based aggregation (Section 5).</p><p>• We demonstrate the scalability of Arabesque on various<br>graphs. We show that Arabesque scales to hundreds of<br>cores over a cluster, obtaining orders of magnitude reduction<br>of running time over the centralized baselines (Section<br>6), and can analyze trillions of embeddings on large<br>graphs.</p><p>The Arabesque system, together with all applications<br>used for this paper, is publicly available at the project’s website:<br>www.arabesque.io.</p><h4 id="翻译-1"><a href="#翻译-1" class="headerlink" title="翻译"></a>翻译</h4><p>Graph data is ubiquitous in many fields, from the Web to advertising and biology, and the analysis of graphs is becoming increasingly important.</p><blockquote><p>图形数据在许多领域普遍存在，从网站到广告业和生物学，并且分析图形正在变得越来越重要。</p></blockquote><p>The development of algorithms for graph analytics has spawned a large amount of research, especially in recent years.</p><blockquote><p>图形分析算法的发展催生了大量的研究，尤其是在近些年来。</p></blockquote><p>However, graph analytics has traditionally been a challenging problem tackled by expert researchers, who can either design new specialized algorithms for the problem at hand, or pick an appropriate and sound solution from a very vast literature.</p><blockquote><p>但是，图形分析历年来是具有挑战性的，由那些能够为了手上的问题设计新的专门的算法或者从非常庞大的文献中选择一个适当并且健全的解决方案的专家去解决。</p></blockquote><p>When the input graph or the intermediate state or computation complexity becomes very large, scalability is an additional challenge.</p><blockquote><p>当输入图形或者中间状态或者计算复杂度非常大的时候，可测量性是一额外的挑战。</p></blockquote><p>The development of graph processing systems such as Pregel [25] has changed this scenario and made it simpler to design scalable graph analytics algorithms.</p><blockquote><p>图形处理系统的发展例如pregel改变了这种方案，并且使设计可升级的图形分析算法更加的简单。</p></blockquote><p>Pregel offers a simple “think like a vertex” (TLV) programming paradigm, where each vertex of the input graph is a processing element holding local state and communicating with its neighbors in the graph.</p><blockquote><p>Pregel 提供了一个简单的”像顶点一样思考”的编程范例，每一个输入图的顶点是一个保持局部状态的处理单元并且在图形中和它的邻点进行通讯。</p></blockquote><p>TLV is a perfect match for problems that can be represented through linear algebra, where the graph is modeled as an adjacency matrix (or some other variant like the Laplacian matrix) and the current state of each vertex is represented as a vector.</p><blockquote><p>T L V 对那些通过线性代数表示的问题能够完美的匹配，在那些图形建模为邻接矩阵（或者一些其他变形像路普拉斯矩阵）和每一个顶点的当前状态被表示为一个向量的问题中。</p></blockquote><p>We call this class of methods graph computation problems.</p><blockquote><p>我们称这一类的方法叫做图计算问题</p></blockquote><p>A good example is computing PageRank [6], which is based on iterative sparse matrix and vector multiplication operations.</p><blockquote><p>一个好的例子就是计算PageRank, 它是基于迭代稀疏矩阵和向量乘法运算。</p></blockquote><p>TLV covers several other algorithms that require a similar computational architecture, for example, shortest path algorithms, and over the years many optimizations of this paradigm have been proposed [17, 26, 36, 42].</p><blockquote><p>TLV 涉及了一些其他的算法，它需要相似的计算结构，例如，最短路径算法，多年来，这种模式的许多优化已被提出来。</p></blockquote><p>Despite this progress, there remains an important class of algorithms that cannot be readily formulated using the TLV paradigm.</p><blockquote><p>尽管这些进展，这里依然有一类重要的算法不可以使用TLV范例制定。</p></blockquote><p>These are graph mining algorithms used to discover relevant patterns that comprise both structurebased and label-based properties of the graph.</p><blockquote><p>这些就是用于发现相关模式的基于结构和基于表的图的性质的图挖掘算法。</p></blockquote><p> Graph mining is widely used for several applications, for example, discovering 3D motifs in protein structures or chemical compounds, extracting network motifs or significant subgraphs from protein-protein or gene interaction networks, mining attributed patterns over semantic data (e.g., in Resource Description Framework or RDF format), finding structure content relationships in social media data, dense subgraph mining for community and link spam detection in web data,among others.</p><blockquote><p>图挖掘广泛的用于一些应用，例如发现蛋白质结构中或者化学物质中的3D图案，从蛋白质或者基因交互网络中提取网络图案或者重要的子图，(例如在资源描述框架或者R D F 格式中)，正在使用发音在社会媒体数据，密集的子图挖掘社区和链接的垃圾邮件检测在Web数据中发现结构内容的关系，等等。</p></blockquote><p> Graph mining algorithms typically take a labeled and immutable graph as input, and mine patterns that have some algorithm-specific property (e.g., frequency above some threshold) by finding all instances of these patterns<br>in the input graph.</p><blockquote><p>图形挖掘算法通常采用一个标记和不可变的图形作为输入,和具有一些算法特性的挖掘模式（例如频率高于某个阈值），通过在输入图中的样式的所有实例。</p></blockquote><p>Some algorithms also compute aggregated metrics based on these subgraphs。<br>一些算法也计算基于这些子图的综合指标。</p><p>Figure 1: Exponential growth of the intermediate state in<br>graph mining problems (motifs counting, clique finding,<br>FSM: Frequent subgraph mining) on different datasets.</p><blockquote><p>图一：在不同的数据集中图挖掘问题中的中间状态的指数增长（图案计数，派系的发现，频繁子图挖掘）</p></blockquote><p>Designing graph mining algorithms is a challenging and<br>active area of research.</p><blockquote><p>设计图挖掘算法在研究中是一个具有挑战性和活跃的领域</p></blockquote><p>In particular, scaling graph mining<br>algorithms to even moderately large graphs is hard.</p><blockquote><p>尤其是，将图挖掘算法应用于中等大小的图是困难的</p></blockquote><p>The set of possible patterns and their subgraphs in a graph can be<br>exponential in the size of the original graph, resulting in an<br>explosion of the computation and intermediate state.</p><blockquote><p>图表中的可能的模式集和他们的子图有可能是原始图大小的指数倍，导致了爆炸性的计算和中间状态。</p></blockquote><p>Figure 1 shows the exponential growth of the number of “interesting”<br>subgraphs of different sizes in some of the graph<br>mining problems and datasets we will evaluate in this paper.</p><blockquote><p>图一展示在一些图挖掘问题中不同大小的“intersting”子图的数量的爆炸性增长并且我们将评估文本的数据集。</p></blockquote><p>Even graphs with few thousands of edges can quickly generate<br>hundreds of millions of interesting subgraphs.</p><blockquote><p>即使是数千个边的图也能生成数亿的”intersting”子图。</p></blockquote><p>The need for enumerating a large number of subgraphs characterizes<br>graph mining problems and distinguishes them from graph<br>computation problems.</p><blockquote><p>图挖掘问题以需要枚举很大数量的子图为特征并且将其于图计算问题区别开来。</p></blockquote><p> Despite this state explosion problem,<br>most graph mining algorithms are centralized because of the<br>complexity of distributed solutions.</p><blockquote><p>尽管这个状态是爆炸性的问题，但是大多数图形挖掘算法是集中式的，因为<br>分布式解决方案的复杂性。</p></blockquote><p>In this paper, we propose automatic subgraph exploration<br>as a generic building block for solving graph mining<br>problems, and introduce Arabesque, the first embedding exploration<br>system specifically designed for distributed graph<br>mining.</p><blockquote><p>在这篇文章中，我们把自动子图搜索看作是一个解决图挖掘问题的通用构建，并且介绍阿拉伯图案<br>，它是第一个为分布式图挖掘设计的嵌入式的探索系统</p></blockquote><p>Conceptually, we move from TLV to “think like an<br>embedding” (TLE), where by embedding we denote a subgraph<br>representing a particular instance of a more general<br>template subgraph called a pattern (see Figure 2).</p><blockquote><p>从概念上讲，我们从TLV移动到了“像嵌入一样思考”（TLE）,通过嵌入，我们表示一个子图<br>通过表示一个称之为模式的更一般的模板的特别的实例（看图二）</p></blockquote><p>Arabesque defines a high-level filter-process computational<br>model.</p><blockquote><p>阿拉伯图案定义了一个高层次的过滤过程计算模型。</p></blockquote><p>Given an input graph, the system takes care<br>of automatically and systematically visiting all the embeddings<br>that need to be explored by the user-defined algorithm,<br>performing this exploration in a distributed manner.</p><blockquote><p>给出一个输入图，系统会自动的，系统性的关注访问所有的那些需要通过通过分布式的方式进行的自定义算法探索的嵌入部分。</p></blockquote><p>The system passes all the embeddings it explores to the application,<br>which consists primarily of two functions: filter, which indicates whether an embedding should be processed, and process,which examines an embedding and may produce some output.</p><blockquote><p>系统通过所有的嵌入部分并暴露给应用，应用主要由两个函数组成：过滤器，指示是否嵌入部分应该被处理。处理，审查一个嵌入部分和有可能处理一些输出。</p></blockquote><p>For example, in the case of finding cliques the filter function prunes embeddings that are not cliques, since none of their extensions can be cliques, and the process function outputs all explored embeddings, which are cliques by construction.</p><blockquote><p>例如，在发现子图派系的案例中过滤器的功能用于修剪不是派系的嵌入部分，因为他们的拓展没有可能是派系，并且处理函数输出所有探索的嵌入部分，那些嵌入的部分通过建设而形成派系。</p></blockquote><p>Arabesque also supports the pruning of the exploration<br>space based on user-defined metrics aggregated across multiple embeddings.</p><blockquote><p>Arabesque还支持修剪基于用户定义的度量标准聚合的空间多次嵌入的探测。</p></blockquote><p>Figure 2: Graph mining concepts: an input graph, an example<br>pattern, and the embeddings of the pattern. Colors represent<br>labels. Numbers denote vertex ids. Patterns and embeddings<br>are two types of subgraphs. However, a pattern is<br>a template, whereas an embedding is an instance. In this example,<br>the two embeddings are automorphic.</p><blockquote><p>图二：</p></blockquote><p>The Arabesque API simplifies and thus democratizes the<br>design of graph mining algorithms, and automates their execution<br>in a distributed setting. We used Arabesque to implement<br>and evaluate scalable solutions to three fundamental<br>and diverse graph mining problems: frequent subgraph mining,<br>counting motifs, and finding cliques. These problems<br>are defined precisely in Section 2. Some of these algorithms<br>are the first distributed solutions available in the literature,<br>which shows the simplicity and generality of Arabesque.</p><p>Arabesque’s embedding-centered API facilitates a highly<br>scalable implementation. The system scales by spreading<br>embeddings uniformly across workers, thus avoiding<br>hotspots. By making it explicit that embeddings are the fundamental<br>unit of exploration, Arabesque is able to use fast<br>coordination-free techniques, based on the notion of embedding<br>canonicality, to avoid redundant work and minimize<br>communication costs. It also enables us to store embeddings<br>efficiently using a new data structure called Overapproximating<br>Directed Acyclic Graph (ODAG), and to devise a<br>new two-level optimization for pattern-based aggregation,<br>which is a common operation in graph mining algorithms.</p><p>Arabesque is implemented as a layer on top of Apache<br>Giraph [3], a Pregel-inspired graph computation system,<br>thus allowing both graph computation and graph mining<br>algorithms to run on top of the same infrastructure. The<br>implementation does not use a TLV approach: it considers<br>Giraph just as a regular data parallel system implementing<br>the Bulk Synchronous Processing model.<br>To summarize, we make the following contributions:</p><p>• We propose embedding exploration, or “think like an embedding”,<br>as an effective basic building block for graph<br>mining. We introduce the filter-process computational<br>model (Section 3), design an API that enables embedding<br>exploration to be expressed effectively and succinctly,<br>and present three example graph mining applications<br>that can be elegantly expressed using the Arabesque<br>API (Section 4).</p><p>• We introduce techniques to make distributed embedding<br>exploration scalable: coordination-free work sharing, ef-<br>ficient storage of embeddings, and an important optimization<br>for pattern-based aggregation (Section 5).</p><p>• We demonstrate the scalability of Arabesque on various<br>graphs. We show that Arabesque scales to hundreds of<br>cores over a cluster, obtaining orders of magnitude reduction<br>of running time over the centralized baselines (Section<br>6), and can analyze trillions of embeddings on large<br>graphs.</p><p>The Arabesque system, together with all applications<br>used for this paper, is publicly available at the project’s website:<br>www.arabesque.io.</p><h4 id="段落翻译-1"><a href="#段落翻译-1" class="headerlink" title="段落翻译"></a>段落翻译</h4><p>图形数据在许多领域普遍存在，从网站到广告业和生物学，并且分析图形正在变得越来越重要。<br>图形分析算法的发展催生了大量的研究，尤其是在近些年来。但是，图形分析历年来是具有挑战性的，由那些能够为了手上的问题设计新的专门的算法或者从非常庞大的文献中选择一个适当并且健全的解决方案的专家去解决。当输入图形或者中间状态或者计算复杂度非常大的时候，可测量性是一额外的挑战。</p><p>图形处理系统的发展例如pregel 改变了这种方案，并且使设计可升级的图形分析算法更加的简单。<br>Pregel 提供了一个简单的”像顶点一样思考”的编程范例，每一个输入图的顶点是一个保持局部状态的处理单元并且在图形中和它的邻点进行通讯。T L V 对那些通过线性代数表示的问题能够完美的匹配，在那些图形建模为邻接矩阵（或者一些其他变形像路普拉斯矩阵）和每一个顶点的当前状态被表示为一个向量的问题中。我们称这一类的方法叫做图计算问题一个好的例子就是计算PageRank, 它是基于迭代稀疏矩阵和向量乘法运算。TLV 涉及了一些其他的算法，它需要相似的计算结构，例如，最短路径算法，多年来，这种模式的许多优化已被提出来。</p><p>尽管这些进展，这里依然有一类重要的算法不可以使用TLV范例制定。这些就是用于发现相关模式的基于结构和基于表的图的性质的图挖掘算法.图挖掘广泛的用于一些应用，例如发现蛋白质结构中或者化学物质中的3D图案，从蛋白质或者基因交互网络中提取网络图案或者重要的子图，(例如在资源描述框架或者R D F 格式中)，正在使用发音在社会媒体数据，密集的子图挖掘社区和链接的垃圾邮件检测在Web数据中发现结构内容的关系，等等。图形挖掘算法通常采用一个标记和不可变的图形作为输入,和具有一些算法特性的挖掘模式（例如频率高于某个阈值），通过在输入图中的样式的所有实例。一些算法也计算基于这些子图的综合指标。</p><p><img src="./img/1.PNG" alt="图1"></p><p>图一：在不同的数据集中图挖掘问题中的中间状态的指数增长（图案计数，派系的发现，频繁子图挖掘）</p><p>设计图挖掘算法在研究中是一个具有挑战性和活跃的领域。尤其是，将图挖掘算法应用于中等大小的图是很困难的。图表中的可能的模式集和他们的子图有可能是原始图大小的指数倍，导致了爆炸性的计算和中间状态。图一展示在一些图挖掘问题中不同大小的“intersting”子图的数量的爆炸性增长并且我们将评估文本的数据集。即使是数千个边的图也能生成数亿的”intersting”子图。图挖掘问题以需要枚举很大数量的子图为特征并且将其于图计算问题区别开来。尽管这个状态是爆炸性的问题，但是大多数图形挖掘算法是集中式的，因为分布式解决方案的复杂性。</p><p>在这篇文章中，我们把自动子图搜索看作是一个解决图挖掘问题的通用构建，并且介绍阿拉伯图案<br>，它是第一个为分布式图挖掘设计的嵌入式的探索系统。从概念上讲，我们从TLV移动到了“像嵌入一样思考”（TLE）,通过嵌入，我们表示一个子图通过表示一个称之为模式的更一般的模板的特别的实例（看图二）。阿拉伯图案定义了一个高层次的过滤过程计算模型。给出一个输入图，系统会自动的，系统性的关注访问所有的那些需要通过通过分布式的方式进行的自定义算法探索的嵌入部分。系统通过所有的嵌入部分并暴露给应用，应用主要由两个函数组成：过滤器，指示是否嵌入部分应该被处理。处理，审查一个嵌入部分和有可能处理一些输出。例如，在发现子图派系的案例中过滤器的功能用于修剪不是派系的嵌入部分，因为他们的拓展没有可能是派系，并且处理函数输出所有探索的嵌入部分，那些嵌入的部分通过建设而形成派系。</p><p><img src="./img/2.PNG" alt="图2"></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Arabesque-A-System-for-Distributed-Graph-Mining&quot;&gt;&lt;a href=&quot;#Arabesque-A-System-for-Distributed-Graph-Mining&quot; class=&quot;headerlink&quot; title=&quot;Arabesque: A System for Distributed Graph Mining&quot;&gt;&lt;/a&gt;Arabesque: A System for Distributed Graph Mining&lt;/h2&gt;&lt;blockquote&gt;
&lt;h3 id=&quot;Arabesque-分布式的图挖掘系统&quot;&gt;&lt;a href=&quot;#Arabesque-分布式的图挖掘系统&quot; class=&quot;headerlink&quot; title=&quot;Arabesque:分布式的图挖掘系统&quot;&gt;&lt;/a&gt;Arabesque:分布式的图挖掘系统&lt;/h3&gt;&lt;/blockquote&gt;
&lt;h4 id=&quot;原文链接：点我跳转&quot;&gt;&lt;a href=&quot;#原文链接：点我跳转&quot; class=&quot;headerlink&quot; title=&quot;原文链接：点我跳转&quot;&gt;&lt;/a&gt;原文链接：&lt;a href=&quot;https://github.com/jChanJi/jchanji.github.com/blob/master/meterial/093-teixeira.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;点我跳转&lt;/a&gt;&lt;/h4&gt;&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&quot;#Abstract&quot;&gt;Abstract&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;1.&lt;a href=&quot;#introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="-arcticals" scheme="http://jchanji.github.io/categories/arcticals/"/>
    
    
      <category term="Graph Mining" scheme="http://jchanji.github.io/tags/Graph-Mining/"/>
    
      <category term="distribute system" scheme="http://jchanji.github.io/tags/distribute-system/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu只显示桌面，没有菜单栏</title>
    <link href="http://jchanji.github.io/year/09/03/ubuntu_only_background/"/>
    <id>http://jchanji.github.io/year/09/03/ubuntu_only_background/</id>
    <published>2017-09-03T06:44:05.410Z</published>
    <updated>2017-09-03T07:13:09.263Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>在裝輸入法的時候好像刪了什麼東西導致電腦重啓的時候只能顯示桌面背景和文件，導航等都沒了，頓時嚇壞我了，找了好多教程終於成功了。</p></blockquote><a id="more"></a><p>##安裝unity<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install unity</span><br></pre></td></tr></table></figure></p><p>##刪除配置文件<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf .conf</span><br><span class="line">sudo rm -rf .gconfg</span><br><span class="line">sudo rm -rf ~/.Xauthority</span><br><span class="line">reboot</span><br></pre></td></tr></table></figure></p><p>本教程不一定對其他情況也適合</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在裝輸入法的時候好像刪了什麼東西導致電腦重啓的時候只能顯示桌面背景和文件，導航等都沒了，頓時嚇壞我了，找了好多教程終於成功了。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="-ubuntu" scheme="http://jchanji.github.io/tags/ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>伪分布式spark安装配置</title>
    <link href="http://jchanji.github.io/year/09/03/spark_step/"/>
    <id>http://jchanji.github.io/year/09/03/spark_step/</id>
    <published>2017-09-03T06:44:05.395Z</published>
    <updated>2018-01-08T16:28:47.197Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>本教程为spark的伪分布式教程，学生党条件有限所以伪分布式应该是比较好的选择。其中要注意的是版本匹配的问题。</p></blockquote><a id="more"></a><h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol><li>CentOS7</li><li>jdk:jdk1.8.0_131</li><li>hadoop：2.6.0</li><li>scala:2.11.11</li><li>spark:2.1.1</li></ol><h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2><ol><li><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">spark2.1.1</a></li><li><a href="http://www.scala-lang.org/download/all.html" target="_blank" rel="noopener">scala2.11.11</a></li></ol><h2 id="三、安装配置"><a href="#三、安装配置" class="headerlink" title="三、安装配置"></a>三、安装配置</h2><h3 id="1、解压-修改权限"><a href="#1、解压-修改权限" class="headerlink" title="1、解压,修改权限"></a>1、解压,修改权限</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf spark-2.1.1-bin-hadoop2.6.tgz -C /usr/local</span><br><span class="line">cd /usr/local</span><br><span class="line">sudo chown -R hadoop:hadoop ./spark</span><br></pre></td></tr></table></figure><h3 id="2、在解压的spark目录下新建文件-test-hellospark-写上内容"><a href="#2、在解压的spark目录下新建文件-test-hellospark-写上内容" class="headerlink" title="2、在解压的spark目录下新建文件/test/hellospark,写上内容"></a>2、在解压的spark目录下新建文件/test/hellospark,写上内容</h3><h3 id="3、进入scala模式"><a href="#3、进入scala模式" class="headerlink" title="3、进入scala模式"></a>3、进入scala模式</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /spark/bin</span><br><span class="line">./spark-shell</span><br></pre></td></tr></table></figure><h3 id="4、运行代码"><a href="#4、运行代码" class="headerlink" title="4、运行代码"></a>4、运行代码</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">5、val lines = sc.textFile(“../test/hellospark”)</span><br><span class="line">   lines.count()</span><br><span class="line">   lines.first()</span><br></pre></td></tr></table></figure><h3 id="5、修日志级别"><a href="#5、修日志级别" class="headerlink" title="5、修日志级别"></a>5、修日志级别</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/spark/conf</span><br><span class="line">cp  log4j.properties.template  log4j.properties   </span><br><span class="line">sudo vim log4j.properties</span><br></pre></td></tr></table></figure><p>   将其中的rootCategory=INFO 改为 WARN</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;本教程为spark的伪分布式教程，学生党条件有限所以伪分布式应该是比较好的选择。其中要注意的是版本匹配的问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="spark" scheme="http://jchanji.github.io/tags/spark/"/>
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
  </entry>
  
  <entry>
    <title>sumlime text3 配置Markdown</title>
    <link href="http://jchanji.github.io/year/09/03/markdown/"/>
    <id>http://jchanji.github.io/year/09/03/markdown/</id>
    <published>2017-09-03T06:44:05.362Z</published>
    <updated>2017-09-03T06:57:25.190Z</updated>
    
    <content type="html"><![CDATA[<h2 id="一、sumlime-text3-配置Markdown和常用快捷键"><a href="#一、sumlime-text3-配置Markdown和常用快捷键" class="headerlink" title="一、sumlime text3 配置Markdown和常用快捷键"></a>一、sumlime text3 配置Markdown和常用快捷键</h2><h3 id="1、sumlime-text3-配置Markdown"><a href="#1、sumlime-text3-配置Markdown" class="headerlink" title="1、sumlime text3 配置Markdown"></a>1、sumlime text3 配置Markdown</h3><blockquote><h4 id="1、安装package-Control"><a href="#1、安装package-Control" class="headerlink" title="1、安装package Control"></a>1、安装package Control<br></h4><h4 id="2、安装Markdown-Preview"><a href="#2、安装Markdown-Preview" class="headerlink" title="2、安装Markdown Preview"></a>2、安装Markdown Preview<br></h4><p>&ensp; 2.1、 按Shif + Alt + P打开<br><br>&ensp; 2.2、输入pcip,回车（进入install package）<br></p></blockquote><a id="more"></a><blockquote><h4 id="3、安装Markdown-Editing"><a href="#3、安装Markdown-Editing" class="headerlink" title="3、安装Markdown  Editing"></a>3、安装Markdown  Editing<br></h4><p>&ensp;3.1、进入 install package<br><br>&ensp;3.2、输入 Markdown Editing // Markdown编辑和语法高亮支持<br></p><h4 id="4、安装Markdown-Previewer"><a href="#4、安装Markdown-Previewer" class="headerlink" title="4、安装Markdown  Previewer"></a>4、安装Markdown  Previewer<br></h4><p>&ensp; 4.1、进入 install package<br><br>&ensp; 4.2、Markdown  Previewer  //Markdown导出html预览支持<br></p><h4 id="5、安装OmniMarkup-Previewer"><a href="#5、安装OmniMarkup-Previewer" class="headerlink" title="5、安装OmniMarkup Previewer"></a>5、安装OmniMarkup Previewer<br></h4><p>&ensp; 5.1、进入 install package<br><br>&ensp; 5.2、OmniMarkup Previewer //在浏览器中实时预览</p></blockquote><h3 id="2、常用快捷键"><a href="#2、常用快捷键" class="headerlink" title="2、常用快捷键"></a>2、常用快捷键</h3><blockquote><ol><li>Ctrl + Alt + O //在浏览器中打开</li><li>Alt + M  //生成html文件</li><li>Ctrl+Alt+O: Preview Markup in Browser.</li><li>Ctrl+Alt+X: Export Markup as HTML.</li><li>Ctrl+Alt+C: Copy Markup as HTML.</li></ol></blockquote><h2 id="二、使用Cmd-markdown在线编辑"><a href="#二、使用Cmd-markdown在线编辑" class="headerlink" title="二、使用Cmd markdown在线编辑"></a>二、使用Cmd markdown在线编辑</h2><blockquote><ol><li>在线<a href="https://www.zybuluo.com/mdeditor" title="Cmd Markdown" target="_blank" rel="noopener">编辑</a> 网址</li><li>也可以下载客户端离线编辑<a href="https://www.zybuluo.com/cmd/" title="下载" target="_blank" rel="noopener">客户端</a></li><li>如果想转成html等其他功能需要付费，不过基础功能已经差不多够用了</li></ol></blockquote><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="1、Ctrl-Alt-O后打开了浏览器但是不能够预览markdown页面"><a href="#1、Ctrl-Alt-O后打开了浏览器但是不能够预览markdown页面" class="headerlink" title="1、Ctrl+Alt+O后打开了浏览器但是不能够预览ｍａｒｋｄｏｗｎ页面"></a>1、Ctrl+Alt+O后打开了浏览器但是不能够预览ｍａｒｋｄｏｗｎ页面</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在 Preferences &gt; Package Settings &gt; OmniMarkupPreviewer &gt; Settings - User 中粘贴以下代码即可</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">"renderer_options-MarkdownRenderer": &#123;</span><br><span class="line"><span class="code">    "extensions": ["tables", "fenced_code", "codehilite"]</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2、在sublime-text3-中切换不了中文"><a href="#2、在sublime-text3-中切换不了中文" class="headerlink" title="2、在sublime text3 中切换不了中文"></a>2、在sublime text3 中切换不了中文</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update &amp;&amp; sudo apt-get upgrade</span><br><span class="line">git clone https://github.com/lyfeyaj/sublime-text-imfix.git</span><br><span class="line">cd ~/sublime-text-imfix</span><br><span class="line">sudo ./ sublime-imfix</span><br><span class="line">然后重启sublime text3</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;一、sumlime-text3-配置Markdown和常用快捷键&quot;&gt;&lt;a href=&quot;#一、sumlime-text3-配置Markdown和常用快捷键&quot; class=&quot;headerlink&quot; title=&quot;一、sumlime text3 配置Markdown和常用快捷键&quot;&gt;&lt;/a&gt;一、sumlime text3 配置Markdown和常用快捷键&lt;/h2&gt;&lt;h3 id=&quot;1、sumlime-text3-配置Markdown&quot;&gt;&lt;a href=&quot;#1、sumlime-text3-配置Markdown&quot; class=&quot;headerlink&quot; title=&quot;1、sumlime text3 配置Markdown&quot;&gt;&lt;/a&gt;1、sumlime text3 配置Markdown&lt;/h3&gt;&lt;blockquote&gt;
&lt;h4 id=&quot;1、安装package-Control&quot;&gt;&lt;a href=&quot;#1、安装package-Control&quot; class=&quot;headerlink&quot; title=&quot;1、安装package Control&quot;&gt;&lt;/a&gt;1、安装package Control&lt;br&gt;&lt;/h4&gt;&lt;h4 id=&quot;2、安装Markdown-Preview&quot;&gt;&lt;a href=&quot;#2、安装Markdown-Preview&quot; class=&quot;headerlink&quot; title=&quot;2、安装Markdown Preview&quot;&gt;&lt;/a&gt;2、安装Markdown Preview&lt;br&gt;&lt;/h4&gt;&lt;p&gt;&amp;ensp; 2.1、 按Shif + Alt + P打开&lt;br&gt;&lt;br&gt;&amp;ensp; 2.2、输入pcip,回车（进入install package）&lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="sublime text3" scheme="http://jchanji.github.io/tags/sublime-text3/"/>
    
      <category term="markdown" scheme="http://jchanji.github.io/tags/markdown/"/>
    
  </entry>
  
  <entry>
    <title>从互联到新工业革命-读后感</title>
    <link href="http://jchanji.github.io/year/09/03/internet_of_everthing_for_new_industrial_revolution/"/>
    <id>http://jchanji.github.io/year/09/03/internet_of_everthing_for_new_industrial_revolution/</id>
    <published>2017-09-03T06:44:05.336Z</published>
    <updated>2017-09-03T06:53:50.510Z</updated>
    
    <content type="html"><![CDATA[<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><h3 id="一、工业4-0”网红”的养成之路"><a href="#一、工业4-0”网红”的养成之路" class="headerlink" title="一、工业4.0”网红”的养成之路"></a><a href="#one">一、工业4.0”网红”的养成之路</a></h3><h3 id="二、”工业互联网”-VS-“工业4-0”"><a href="#二、”工业互联网”-VS-“工业4-0”" class="headerlink" title="二、”工业互联网” VS “工业4.0”"></a><a href="#two">二、”工业互联网” VS “工业4.0”</a></h3><h3 id="三、中国制造2025"><a href="#三、中国制造2025" class="headerlink" title="三、中国制造2025"></a><a href="#three">三、中国制造2025</a></h3><h3 id="四、工业革命升级技能点"><a href="#四、工业革命升级技能点" class="headerlink" title="四、工业革命升级技能点"></a><a href="#forth">四、工业革命升级技能点</a></h3><h3 id="五、人工智能"><a href="#五、人工智能" class="headerlink" title="五、人工智能"></a><a href="#firth">五、人工智能</a></h3><h3 id="六、工业互联网的智能网络"><a href="#六、工业互联网的智能网络" class="headerlink" title="六、工业互联网的智能网络"></a><a href="#six">六、工业互联网的智能网络</a></h3><h3 id="七、结束语"><a href="#七、结束语" class="headerlink" title="七、结束语"></a><a href="#end">七、结束语</a></h3><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;有幸能够参加东华大学计算机科学与技术学院举办的“大数据与智能制造”暑期夏令营。虽然只有短短的两天时间，但是收获颇多，尤其是听了燕山大学机械学院院长张立杰教授“智能制造和传统制造”的演讲和美国佛罗里达大学教授李晓林教授“Creating Intellignece via big learning”的演讲，对智能制造和人工智能领域了更加深刻的印象。由于我比较愚钝，具体的演讲内容不能详细的复述出来。再次由衷的感谢东华大学的常珊老师免费提供给我们《从互联到新工业革命》这本书，今天在火车上读完了这本通俗易懂而又见解深刻的书,无意发现,张立杰教授，李晓林教授，刘云浩教授，在智能制造方面的见解英雄所见略同，而为了更加体系的介绍和便于自己思路的清理，下面更多介绍清华大学软件学院院长刘云浩教授对互联网时代和新工业革命大潮的理解和体会并且再加上一点我个人的浅陋的见解。 由于本人文采有限，不能够很全面的写出刘老师书中的方方面面也无法用诙谐的语句吸引读者兴趣，所以我极力推荐大家读一读刘云浩老师的作品《从互联到新工业革命》（清华大学出版社）。由于我也只是泛读了一遍,所以写博客的时候也是第二次更加粗略的阅读，当中有什么见解不到的地方欢迎联系我（邮箱见文章底部）。</p></blockquote><a id="more"></a><h2 id="导言"><a href="#导言" class="headerlink" title="导言"></a>导言</h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;由于我也是刚接触大数据，人工智能不久，虽然对其中的技术细节还不是非常的了解，但引用刘国华教授”纸上谈兵”的观点，如果不去”纸上谈兵”而直接去埋头编程那么有可能永远都完成不了项目，或者当中算法效率是很低的。同样我觉得，学习一个完全陌生的专业，如果连它的发展趋势和技术线路都没有搞清楚的话，那么也只是一头雾水的填鸭式的学习,不利于以后潜力的挖掘和能力的提升。下面我们就”空谈”一些对技术能力提高”无用”的，空泛的，所谓”夸夸而谈”的观点。</p></blockquote><h2 id="一、工业4-0”网红”的养成之路-1"><a href="#一、工业4-0”网红”的养成之路-1" class="headerlink" title=" 一、工业4.0”网红”的养成之路"></a><span id="one"> 一、工业4.0”网红”的养成之路</span></h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;目前为止确切的有三次工业革命：1769年瓦特发明蒸汽机，标志着机械化的时代到来,机器代替了人类一部分的体力活动，人类向机械化迈进;1869年，德国西门子公司发明了第一台交流发动机，电器取代了机器，电器动力取代了蒸汽动力，从此促进了大规模，批量化的生产。也是从此时开始，东方开始落后西方；1969年，第一块可编程逻辑控制器Modicon 084问世，这标志这电子信息技术的发明并且直接导致了产品生产的高度自动化。此外还有一件划时代的事发生了，便是阿帕网的形成，也就是互联网的雏形。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;近两百年之前的工业革命，在之后的150年里使西方人均收入增长了13倍，而1800年以前，西方人均经济翻一倍则需要800年。这足以见得这几次工业革命对人类社会生产力的解放是多么的恐怖，这也预示着，人类社会的发展速度将会越来越快。那么第四次工业革命会是在2069年吗？显然，就目前的形式来看完全等不到2069年。新工业革命已经隐约到来，虽然我们不能够从未来的角度来看现在来，判定是否是第四次工业革命，就像前三次工业革命一样，发生之后才意识具有多么伟大的意义，但是，我们已经确切的感受到新工业革命了。”人类第一次成功的在事前预测了一次革命，而不是像以前一样事后才认识到是一场革命”,正如刘老师所言。”工业4.0”<br>由孔翰宁(Henning Kagermann),沃夫冈.瓦尔斯特(“Wolfgang Wahlster”),沃尔夫迪特尔.卢卡斯(Wolf-Dieter Lukas)三位博士提出,由于”产官学”（ 产业界，政府，学术界）属性的与生俱来，很快便由德国工程院，弗劳恩获夫协会，西门子公司等接手，组成了工业4.0小组,于是工业4.0迅速的冲出了德国，走向了世界。所以”工业4.0”正是天时地利人和的结果。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;这里插入一些我的人生观，不想听大道理的可以跳过了。其实很多事情都是水到而渠成的,尤其是知识积累更是如此。面对飞速发展的软件行业，作为初级程序员,很多人都想一口吃个胖子,想要快速的掌握开发技能,喜欢看速成的视频,教程，包括我也是如此,但很多时候却走了很多的弯路。因为基础知识不扎实而找东找西,就是一根筋的想要找和自己的问题一摸一样的解答，却不知道或者懒得去变通一下代码，或者花点时间去专研一下代码中的逻辑思维，从中受到启发。与其花半天时间去研读代码。却更愿意去花一天时间尝遍百度上的所有教程。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;官方对于”工业4.0”的解释是,”工业4.0包括将信息物理系统(Cyber physical System，CPS)技术一体化应用与制造业和物流行业,以及在工业生产过程中采用物联网和服务技术”。从这段定义中个我们可以看到很熟悉的一个词”物联网”,所以从此也可以看出以后的IT热门方向。物联网从前几年开始变得炙手可热，但发展一直没有想象中的那么迅猛，这和很多方面因素有关，包括硬件支持，传输技术，等等。但这丝毫不影响其发展趋势，因为物联网还是在不断发展的，而且越来越快,其模型成熟的时间决定着万物互联时代到来的时间。”工业4.0”产生的”智能工厂”和”智能生产”将改变传统的批量统一化的生产模式,实现高度灵活的个性化和数字化生产及服务，最终使生产更智能，更高效，跟快速，更经济。</p></blockquote><h2 id="ps1"><a href="#ps1" class="headerlink" title="ps1"></a>ps1</h2><blockquote><p>由于我的手速有限，时间紧迫，今天只能谈到这。作为有点学术性质而又不深入具体细节的博客，希望大家能够当成故事看，了解当今的IT界的发展方向。我每天晚上会抽出11点之后断网的时间续写，第二点早上9点跟新,时间有限，我会尽快的完结。</p></blockquote><h2 id="二、”工业互联网”-VS-“工业4-0”-1"><a href="#二、”工业互联网”-VS-“工业4-0”-1" class="headerlink" title=" 二、”工业互联网” VS “工业4.0”"></a><span id="two"> 二、”工业互联网” VS “工业4.0”</span></h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;面对的德国的高歌猛进，世界第一大国美国怎能无动于衷了？毕竟在这个星球上主导权决定着发言权，就算是没有足够的主导权也不能牵制于人。由沙利文（Frost &amp; Sullivan）这家咨询公司在一份报告中创造性的提出了“工业互联网”这个概念。也因此给沙利文公司在工业制造领域带来了话语权。公司还顺带的设立了“制造领袖奖”,2016年通用公司就很高兴的领了这个奖。2014年3月由通用电气（提供综合技术与服务）联合AT&amp;T(M2M的解决方案)、Cisco(提供网络解决方案)、Intel(半导体、芯片和处理器)、IBM(智慧地球)成立了“工业互联网联盟(Industrial Internet Consortium,IIC)”。很显然工业互联网这块大蛋糕美国怎么会任由他人分割了,到2015年初，该联盟成员已经达到130家，西门子，华为等号称要自己做工业互联网平台的企业也没能抵制住诱惑。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;很失望的是中国还是一如既往的落后与西方国家，作为发展中国家，不得不承认在先进技术和理念方面中国目前只能去模仿，距离成为“中国制造2025”目标中的世界一流的工业水准还是有很大的差距的。毕竟，不要说“工业4.0”，中国大部分企业还停留在“工业2.0”的水准，“工业3.0”水平也是很弱，这和很多因素有关，但我们还是对未来充满希望的，毕竟科技的快速发展网络的普及化，信息的透明化，以及人才的全球流通，给发展中国家带来的好处是可以快速的跟上队伍。我国并不缺少运行产业联盟的企业，但是成功的却非常少，其中企业自身创新能力弱，国际视野的局限性大是一方面，缺乏一个良好的利益共享机制，无法发挥每个企业的特长也是国内产业联盟难以落地的重要原因。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;”工业互联网”和”工业4.0”中国到底应该站在哪一边了？这就要进一步的了解这两个热门词汇了。“工业互联网”可以说是自顶向下，侧重于利用互联网的技术来改善生产设备和产品服务。从物联网、云计算、大数据分析等信息技术的角度出发，将之应用于工业领域，改造工业生产的产品服务和管理过程等。“工业4.0”则是自下而上，侧重于在生产与制造过程的智能化、数字化。以生产设备为核心的CPS为出发点，推进数据融合和服务共享，从而推及工业生产过程以及产品服务等。虽然由于两国的的产业优势不同导致的工业互联网的结构正好颠倒，但其中的核心思想还是十分相似的。2016年3月，”工业4.0平台”和”工业互联网联盟”在瑞士苏黎世初步达成合作意向，开始了强强联合。这也是应了”马太效应”，”凡有的，还要加给他，叫他有余；凡没有的，连他所有的也要夺去”。</p></blockquote><h2 id="ps2"><a href="#ps2" class="headerlink" title="ps2"></a>ps2</h2><blockquote><p>不知不觉已经12点了，为了不打扰舍友休息，今天就到这里。</p></blockquote><h2 id="三、中国制造2025-1"><a href="#三、中国制造2025-1" class="headerlink" title=" 三、中国制造2025"></a><span id="three"> 三、中国制造2025</span></h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;世界上很多国家都指定了符合本国国情的工业互联网规划，但基本上都是依据“工业4.0”或者“工业互联网”进行改编。同样，中国也不例外。“中国制造2025”以促进制造业创新发展为主题，以提质增效为中心，以加快新一代信息技术与制造业深度融合为主线，以推进智能制造为主攻方向，以满足经济社会发展和国防建设对重大技术装备的需求为目标，强化工业基础能力，提高综合集成水平，完善多层次多类型的培养体系，促进产业转型升级，培育有中国特色的制造文化，实现制造业由大变强的历史跨越。坚持“创新驱动、质量为先、绿色发展、结构优化、人才为本”的基本方针，坚持“市场主导、政府引导，立足当前、着眼长远，整体推进、重点突破，自主发展、开放合作”的基本原则，通过“三步走”实现制造强国的战略目标：第一步，到2025年迈入制造强国行列；第二步，到2035年中国制造业整体达到世界制造强国阵营中等水平；第三步，到新中国成立一百年时，综合实力进入世界制造强国前列。看到这一大推的雄伟措辞，不得不说中国最强的专家真的不是盖的，由50多位院士100多位领域专家共同指定的规划，将中国制造转变为中国智造的伟大目标高调的向全世界展示。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;中国制造业的现状其实和国足相差无几，所以注定不能自上顶而下的对中国的工业基础进行改革，薄弱的工业基础实在是堪忧。从中国的物流业占GDP比重是美国德国的发达国家的两倍左右就可以看出其中的差距。所以“中国制造2025”更倾向于“工业4.0”的自下而上的进行改革。</p></blockquote><h2 id="四、工业革命升级技能点-1"><a href="#四、工业革命升级技能点-1" class="headerlink" title="四、工业革命升级技能点"></a><span id="forth">四、工业革命升级技能点</span></h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;从技术角度上来说，第四次工业革命是一场从嵌入式系统到信息物理融合系统的技术变革，通过物联网，云计算，大数据在工业中的运用，促成基于网络化的变革。其关键的技术难点和重点在于实现智能化设备自知而治，泛在化网络（无处不在的网络）互联互通，中心化数据实时实效，开放化服务相辅相成，建立能够在联网对象彼此之间，网络对象和网络环境之间，联网对象和人之间共享的工业互联网，形成物联网，数据联网，服务联网以及人员联网的网络化开放平台。</p></blockquote><h2 id="PS3"><a href="#PS3" class="headerlink" title="PS3"></a>PS3</h2><blockquote><p>第二遍浏览时发现想要阐述出书中的核心思想还是很难的，越来越发现写不下去了。所以还是得放下键盘，再仔细思考一番，理清思路。</p></blockquote><h2 id="五、人工智能-1"><a href="#五、人工智能-1" class="headerlink" title="五、人工智能"></a><span id="firth">五、人工智能</span></h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;谈到智能工业，智能电网，使用的往往是Smart这个词，而人工智能则使用Aritifical Intelligence（AI）。其原因是人工智能突出的是机器的反映方式能够类似人的智能。而半个世纪以来，人工智能的发展历程很坎坷，机器是否智能一直是一个很有争议的话题。之前之所以认为机器不具有智能是因为机器所完成的任务都是人类所定义好的，并没有超出人类的认知范围或者能力限制。而现在有些深度学习训练出来模型很多已经超出了人的认知范围，因为人能通过参数，阈值的对结果值进行調优，但是算法内部到底是怎样实现的却很难被人所知。随之而来的问题就是，不能够确保其训练出来的模型能够永远的正常使用，所以在金融行业，医疗行业等安全系数要求很高的行业中使用起来是需要对其进行风险评估的。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;人工智能发展的阻碍主要有三个方面。第一、计算机的计算能力。随着硬件的不断升级计算计的运算能力显著提升，但是这不一定代表就可以解决全部的问题。仍然有很多无法优化的算法是需要大量的计算资源的，深度学习出来之后CPU就已经不适合用作为机器学习开发的硬件了，GPU（图形处理单元）将逐步的取代CPU在机器学习中的地位。第二、计算机对真实世界的感知能力。道现在为止人类研究的人工智能在“智力“上已经很高了，但是还是无法像人一样感知世界。对计算机而言实现逻辑推理等人类高级智慧只需要相对较少的计算能力，而现实感知、运动等人类低级智慧却需要巨大的计算资源。第三、推理和逻辑框架。人工智能也无法像人类一样在没有老师的情况下还能够自行的推理并且联想学习，也就是不具备迁移学习的能力。所以人工智能要模拟人的智能其难点不在于人脑进行的各种必然性推理，而是最能体现人的能动性和创造性的不确定推理。</p></blockquote><h2 id="六、工业互联网的智能网络-1"><a href="#六、工业互联网的智能网络-1" class="headerlink" title="六、工业互联网的智能网络"></a><span id="six">六、工业互联网的智能网络</span></h2><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;一个个孤立的点所包含的信息是很少的，但是将这些点之间相互连接起来，其中包含的信息量就极其巨大了。工业互联网的核心就是将原本割裂的工业数据实现流通，从而变成一个智能网络。我们可以概括为四个环节，即”感，联，知，控“。感，即感知层，机器，机组，物料，人员等物体之间能够相互感知，交互协作，从而实现不同生产实体之间的深度协同。联，即网络互联层，旨在将多元对象组成的异构复杂网络之间形成彼此互联互通的泛在化网络（可以简单理解为无处不在的网络）。知，即数据分析层，网络化的数据有些在传输过程中被即时处理，更多的是汇聚到中心节点后被集中处理。数据分析层负责工业大数据的存储、处理、建模、挖掘、和优化等方面。控，即开放服务层，基于工业大数据形成的决策依据，通过多种面向工业生产应用的开放式，共享型的标准化服务，被工业生产部门调用和实施，反馈到工业生产的各个环节，从而实现对工业生产的控制和调节。从网络角度出发，形成了实体联网，数据联网，服务联网的三重联网。</p></blockquote><h3 id="七、结束语-1"><a href="#七、结束语-1" class="headerlink" title="七、结束语"></a><span id="end">七、结束语</span></h3><blockquote><p>&nbsp;&nbsp;&nbsp;&nbsp;按照书本上的章节的话应该还有几章的内容没有讲到，主要内容是作者结合了中国工业的现状，对前几年发生的事情进行分析，对未来的展望。所以，到此为止就算是完结散花了。但总是感觉自己写的还很不到位，估计是因为心境的问题吧，有时候心境达不到，确实不能够写出什么深层次的东西，总是感觉很是词穷。下面又是我个人的主观瞎想了，没有兴趣的可以拜拜了。</p><p>&nbsp;&nbsp;&nbsp;&nbsp;有一句话叫做“软件定义世界”，而以后可能是“人工智能定义软件“。现在人工智能已经逐渐的深入到人们的生活当中来，语音识别，图像识别，等传统的机器学习任务现在使用深度学习对其进行训练，其在人脸识别，语音识别，游戏等很多方面已经超过了人类。所以深度学习的出现，可以说是又一次的焕发了机器学习的活力。毕竟人类从上个世界五六十年代一直到到现在在人工智能方面，能够显著的看出成果并取得重大突破的就是现在，几乎每天的新闻都是某某某使用深度学习神经网络实现了啥啥啥历史性的突破。<br>而人工智能，大数据，云计算近几年来的火爆都不是”横空出世“的。个人觉得其最根本的原因在于互联。正是因为互联，所以数据呈现爆炸式的增长，所以单机的性能远远的不能满足大数据的需求。所以便出现了分布式集群，hadoop框架的诞生更是刺激了大数据的飞速发展。然而个人公司想要购置维护一个机器集群其话费可想而知，估计初创公司在第一步的购置硬件支持上就已经阻力重重了。这时候云计算平台的出现无异于雪中送炭，你只需要根据你所需要的服务，按照资源分配的多少，租用的时常，支付相应的费用便可，这样就大大的降低了业务快速上线的难度。云计算不仅仅给大数据提供了可扩展的平台，也是给人工智能提供了便捷，使得个人的算法实践能够方便，快捷，低成本的运行起来，而不必担心购买昂贵的GPU,显卡等硬件配置并且考虑使用完后的处置。我发像我已经犯困了，舍友以后也能好好睡觉了，完结。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h2&gt;&lt;h3 id=&quot;一、工业4-0”网红”的养成之路&quot;&gt;&lt;a href=&quot;#一、工业4-0”网红”的养成之路&quot; class=&quot;headerlink&quot; title=&quot;一、工业4.0”网红”的养成之路&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#one&quot;&gt;一、工业4.0”网红”的养成之路&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;二、”工业互联网”-VS-“工业4-0”&quot;&gt;&lt;a href=&quot;#二、”工业互联网”-VS-“工业4-0”&quot; class=&quot;headerlink&quot; title=&quot;二、”工业互联网” VS “工业4.0”&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#two&quot;&gt;二、”工业互联网” VS “工业4.0”&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;三、中国制造2025&quot;&gt;&lt;a href=&quot;#三、中国制造2025&quot; class=&quot;headerlink&quot; title=&quot;三、中国制造2025&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#three&quot;&gt;三、中国制造2025&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;四、工业革命升级技能点&quot;&gt;&lt;a href=&quot;#四、工业革命升级技能点&quot; class=&quot;headerlink&quot; title=&quot;四、工业革命升级技能点&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#forth&quot;&gt;四、工业革命升级技能点&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;五、人工智能&quot;&gt;&lt;a href=&quot;#五、人工智能&quot; class=&quot;headerlink&quot; title=&quot;五、人工智能&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#firth&quot;&gt;五、人工智能&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;六、工业互联网的智能网络&quot;&gt;&lt;a href=&quot;#六、工业互联网的智能网络&quot; class=&quot;headerlink&quot; title=&quot;六、工业互联网的智能网络&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#six&quot;&gt;六、工业互联网的智能网络&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;七、结束语&quot;&gt;&lt;a href=&quot;#七、结束语&quot; class=&quot;headerlink&quot; title=&quot;七、结束语&quot;&gt;&lt;/a&gt;&lt;a href=&quot;#end&quot;&gt;七、结束语&lt;/a&gt;&lt;/h3&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;有幸能够参加东华大学计算机科学与技术学院举办的“大数据与智能制造”暑期夏令营。虽然只有短短的两天时间，但是收获颇多，尤其是听了燕山大学机械学院院长张立杰教授“智能制造和传统制造”的演讲和美国佛罗里达大学教授李晓林教授“Creating Intellignece via big learning”的演讲，对智能制造和人工智能领域了更加深刻的印象。由于我比较愚钝，具体的演讲内容不能详细的复述出来。再次由衷的感谢东华大学的常珊老师免费提供给我们《从互联到新工业革命》这本书，今天在火车上读完了这本通俗易懂而又见解深刻的书,无意发现,张立杰教授，李晓林教授，刘云浩教授，在智能制造方面的见解英雄所见略同，而为了更加体系的介绍和便于自己思路的清理，下面更多介绍清华大学软件学院院长刘云浩教授对互联网时代和新工业革命大潮的理解和体会并且再加上一点我个人的浅陋的见解。 由于本人文采有限，不能够很全面的写出刘老师书中的方方面面也无法用诙谐的语句吸引读者兴趣，所以我极力推荐大家读一读刘云浩老师的作品《从互联到新工业革命》（清华大学出版社）。由于我也只是泛读了一遍,所以写博客的时候也是第二次更加粗略的阅读，当中有什么见解不到的地方欢迎联系我（邮箱见文章底部）。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-articals" scheme="http://jchanji.github.io/categories/articals/"/>
    
    
      <category term="AI" scheme="http://jchanji.github.io/tags/AI/"/>
    
      <category term="Made in China 2025" scheme="http://jchanji.github.io/tags/Made-in-China-2025/"/>
    
  </entry>
  
  <entry>
    <title>win10 安装ubuntu 16.04</title>
    <link href="http://jchanji.github.io/year/09/03/install_ubuntu/"/>
    <id>http://jchanji.github.io/year/09/03/install_ubuntu/</id>
    <published>2017-09-03T06:44:05.312Z</published>
    <updated>2017-09-03T06:47:07.748Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>在win10下安装ubuntu双系统。在笔记本安装ubuntu的时候遇到了很多的挫折，曾经也放弃过，但很不幸的是，在未来的今天又碰上了。有时候问题的答案很简单，但是需要大量的时间去得到它，并不是你的能力不行，而是网上的干扰答案实在是太多，无法分辨谁对谁假的时候，往往会一个个的试过去。我不认为这是个很笨的方法，因为多花点时间总能体会更多的东西。貌似废话有点多，下面直接上干货。</p></blockquote><a id="more"></a><h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol><li>win10 企业版</li><li>ubuntu 16.04</li><li>UltraSO 9.6.6.3300</li><li>显卡：GTX 965M</li><li>cup:i7-6700HQ</li></ol><h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2><ol><li><a href="http://releases.ubuntu.com/16.04.2/ubuntu-16.04.2-desktop-amd64.iso?_ga=2.92867550.254780022.1497589112-1524410519.1497589112" target="_blank" rel="noopener">ubuntu 16.04</a></li><li><a href="http://172.19.251.251/files/510300000015EB65/dl.softmgr.qq.com/original/Compression/uiso9_cn_9.6.6.3300.exe" target="_blank" rel="noopener">UltraSO</a></li></ol><h2 id="三、安装配置"><a href="#三、安装配置" class="headerlink" title="三、安装配置"></a>三、安装配置</h2><h3 id="1-将镜像刻录到u盘（至少4g）"><a href="#1-将镜像刻录到u盘（至少4g）" class="headerlink" title="1.将镜像刻录到u盘（至少4g）"></a>1.将镜像刻录到u盘（至少4g）</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">打开UltraSO，点击试用。</span><br><span class="line">1、文件-&gt;打开-&gt;镜像位置</span><br><span class="line">2、启动-&gt;写如硬盘映像-&gt;写入</span><br></pre></td></tr></table></figure><h3 id="2、分配空闲分区"><a href="#2、分配空闲分区" class="headerlink" title="2、分配空闲分区"></a>2、分配空闲分区</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、右击我的电脑-&gt;管理-&gt;磁盘管理</span><br><span class="line">2、选择非系统盘的主分区，右击-&gt;压缩卷，选择压缩大小，一般为50G,我的是100G,</span><br><span class="line">根据自己磁盘情况分配。</span><br></pre></td></tr></table></figure><h3 id="3、将电脑设置为U盘启动"><a href="#3、将电脑设置为U盘启动" class="headerlink" title="3、将电脑设置为U盘启动"></a>3、将电脑设置为U盘启动</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">插入U盘，进入BIOS将U盘设置为启动项</span><br><span class="line">注：不同主板的BIOS大多都不会相同，所以根据自己电脑型号到网上查找。</span><br></pre></td></tr></table></figure><h3 id="4、安装系统"><a href="#4、安装系统" class="headerlink" title="4、安装系统"></a>4、安装系统</h3><h4 id="1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。"><a href="#1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。" class="headerlink" title="1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。"></a>1、重新启动电脑，会进入安装界面，先择安装系统，进行安装。</h4><h4 id="2、卡在logo"><a href="#2、卡在logo" class="headerlink" title="2、卡在logo"></a>2、卡在logo</h4><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">重新启动电脑，在选择系统安装的界面按e,进入grup界面，让后在splash后面加上：空格nomodeset空格，按F10执行。后面重启出了最后一次也是一样操作</span><br></pre></td></tr></table></figure><h4 id="3、创建分区"><a href="#3、创建分区" class="headerlink" title="3、创建分区"></a>3、创建分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1、选择自己分配的空闲的磁盘，进行分盘</span><br><span class="line">2、分盘的注意点是：</span><br><span class="line">    2.1、 /：存储系统文件，建议10GB ~ 15GB,我分配16G；</span><br><span class="line">    2.2、 swap：交换分区，即Linux系统的虚拟内存，建议是物理内存的2倍,我分配16G；</span><br><span class="line">    2.3、 /home：建议最后分配所有剩下的空间；</span><br><span class="line">    2.4、 boot：包含系统内核和系统启动所需的文件，实现双系统的关键所在，建议200M,我分配400M。</span><br><span class="line">3、其他的默认或者根据自己需求设置，点击安装</span><br></pre></td></tr></table></figure><h3 id="5、分辨率问题"><a href="#5、分辨率问题" class="headerlink" title="5、分辨率问题"></a>5、分辨率问题</h3><p>一般比较新的N（英伟达）卡会出现没有安装驱动的问题，所以屏幕的分率很低，这时候就需要安装N卡驱动。直接安装会导致开机的时候卡在登陆界面进不去，所以必须借助于bumblebee(大黄蜂)，至于原因有兴趣的可以查一查，这里不多阐述。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install bumblebee bumblebee-nvidia primus linux-headers-generic</span><br><span class="line">Reboot</span><br></pre></td></tr></table></figure></p><p>重新启动后：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get purge nvidia-* #删除所有的N卡驱动</span><br><span class="line">sudo add-apt-repository ppa:graphics-drivers/ppa  #添加第三方驱动源</span><br><span class="line">sudo apt-get update #更新源</span><br><span class="line">sudo  apt-cache search nvidia-*  #查询nvidia驱动可用版本，这里推荐到英伟达官网查看自己显卡驱动的版本，我的是375</span><br><span class="line">sudo apt-get install nvidia-375 # 安装驱动</span><br></pre></td></tr></table></figure></p><p>打开软件更新器，然后将附加驱动－&gt;未知换成显卡的驱动<br><br>最后重新启动，什么都不用做，等开机！</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在win10下安装ubuntu双系统。在笔记本安装ubuntu的时候遇到了很多的挫折，曾经也放弃过，但很不幸的是，在未来的今天又碰上了。有时候问题的答案很简单，但是需要大量的时间去得到它，并不是你的能力不行，而是网上的干扰答案实在是太多，无法分辨谁对谁假的时候，往往会一个个的试过去。我不认为这是个很笨的方法，因为多花点时间总能体会更多的东西。貌似废话有点多，下面直接上干货。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="ubuntu" scheme="http://jchanji.github.io/tags/ubuntu/"/>
    
      <category term="win 10" scheme="http://jchanji.github.io/tags/win-10/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7 安装vnc</title>
    <link href="http://jchanji.github.io/year/09/03/vnc/"/>
    <id>http://jchanji.github.io/year/09/03/vnc/</id>
    <published>2017-09-03T06:44:05.306Z</published>
    <updated>2017-09-03T07:08:42.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-首先执行这一句防止系统文件被修改"><a href="#1-首先执行这一句防止系统文件被修改" class="headerlink" title="1.首先执行这一句防止系统文件被修改"></a>1.首先执行这一句防止系统文件被修改</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chattr +i /etc/resolv.conf</span><br></pre></td></tr></table></figure><h2 id="2、然后安装tigervnc"><a href="#2、然后安装tigervnc" class="headerlink" title="2、然后安装tigervnc"></a>2、然后安装tigervnc</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install -y tigervnc tigervnc-server</span><br></pre></td></tr></table></figure><a id="more"></a><h2 id="3-查看自己的服务器支持安装哪些包"><a href="#3-查看自己的服务器支持安装哪些包" class="headerlink" title="3.查看自己的服务器支持安装哪些包"></a>3.查看自己的服务器支持安装哪些包</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum grouplist</span><br></pre></td></tr></table></figure><h4 id="查看自己的服务器里中Available-Environment-Groups下面有哪些可以安装的Desktop-我这里的是GNOME-Desktop"><a href="#查看自己的服务器里中Available-Environment-Groups下面有哪些可以安装的Desktop-我这里的是GNOME-Desktop" class="headerlink" title="查看自己的服务器里中Available Environment Groups下面有哪些可以安装的Desktop,我这里的是GNOME Desktop"></a>查看自己的服务器里中Available Environment Groups下面有哪些可以安装的Desktop,我这里的是GNOME Desktop</h4><h2 id="4-安装GNOME-Desktop"><a href="#4-安装GNOME-Desktop" class="headerlink" title="4. 安装GNOME Desktop"></a>4. 安装GNOME Desktop</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum groupinstall GNOME Desktop</span><br></pre></td></tr></table></figure><h2 id="5-启动服务"><a href="#5-启动服务" class="headerlink" title="5. 启动服务"></a>5. 启动服务</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vncserver</span><br></pre></td></tr></table></figure><h2 id="6-连接vnc"><a href="#6-连接vnc" class="headerlink" title="6.连接vnc"></a>6.连接vnc</h2><blockquote><p>第一次执行会提示输入密码，然后再验证输入一次回车，vnc服务端就算搭建好了！<br>接下来在手机或者电脑上下载vnc客户端，输入你的IP:5901连接<br>然后输入密码就可以看到你的服务器界面了！</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-首先执行这一句防止系统文件被修改&quot;&gt;&lt;a href=&quot;#1-首先执行这一句防止系统文件被修改&quot; class=&quot;headerlink&quot; title=&quot;1.首先执行这一句防止系统文件被修改&quot;&gt;&lt;/a&gt;1.首先执行这一句防止系统文件被修改&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;chattr +i /etc/resolv.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;h2 id=&quot;2、然后安装tigervnc&quot;&gt;&lt;a href=&quot;#2、然后安装tigervnc&quot; class=&quot;headerlink&quot; title=&quot;2、然后安装tigervnc&quot;&gt;&lt;/a&gt;2、然后安装tigervnc&lt;/h2&gt;&lt;figure class=&quot;highlight markdown&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo yum install -y tigervnc tigervnc-server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="CentOS" scheme="http://jchanji.github.io/tags/CentOS/"/>
    
      <category term="vnc" scheme="http://jchanji.github.io/tags/vnc/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 7 安装eclipse mars 2</title>
    <link href="http://jchanji.github.io/year/09/03/install_eclipse/"/>
    <id>http://jchanji.github.io/year/09/03/install_eclipse/</id>
    <published>2017-09-03T06:40:47.978Z</published>
    <updated>2017-09-03T07:02:49.922Z</updated>
    
    <content type="html"><![CDATA[<h4 id="操作系统：CentOS-7"><a href="#操作系统：CentOS-7" class="headerlink" title="操作系统：CentOS 7"></a>操作系统：CentOS 7</h4><h4 id="eclispe版本：Eclipse-Mars-2"><a href="#eclispe版本：Eclipse-Mars-2" class="headerlink" title="eclispe版本：Eclipse Mars 2"></a>eclispe版本：Eclipse Mars 2</h4><a id="more"></a><h2 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h2><blockquote><h3 id="1-下载安装eclipse"><a href="#1-下载安装eclipse" class="headerlink" title="1.下载安装eclipse"></a>1.下载安装<a href="http://mirrors.ustc.edu.cn/eclipse/technology/epp/downloads/release/mars/2/eclipse-jee-mars-2-linux-gtk-x86_64.tar.gz" title="eclise下载" target="_blank" rel="noopener">eclipse</a></h3><h3 id="2-解压"><a href="#2-解压" class="headerlink" title="2.解压"></a>2.解压</h3></blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo tar -zxvf [下载的安装包名称] -C [安装的目录]</span><br></pre></td></tr></table></figure><blockquote><h3 id="3-创建软链接"><a href="#3-创建软链接" class="headerlink" title="3.创建软链接"></a>3.创建软链接</h3></blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ln -s /安装的目录/eclipse/eclipse  /usr/bin/eclipse</span><br></pre></td></tr></table></figure><blockquote><h3 id="4-添加图标"><a href="#4-添加图标" class="headerlink" title="4.添加图标"></a>4.添加图标</h3></blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit /usr/share/applications/eclipse.desktop</span><br></pre></td></tr></table></figure><p>将下面内容添加到文件中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[Desktop Entry]</span><br><span class="line">Encoding=UTF-8</span><br><span class="line">Name=Eclipse</span><br><span class="line">Comment=Eclipse Mar2</span><br><span class="line">Exec=/usr/bin/eclipse</span><br><span class="line">Icon=/[解压的目录]/eclipse/icon.xpm</span><br><span class="line">Categories=Application;Development;Java;IDE</span><br><span class="line">Version=1.0</span><br><span class="line">Type=Application</span><br><span class="line">Terminal=0</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;操作系统：CentOS-7&quot;&gt;&lt;a href=&quot;#操作系统：CentOS-7&quot; class=&quot;headerlink&quot; title=&quot;操作系统：CentOS 7&quot;&gt;&lt;/a&gt;操作系统：CentOS 7&lt;/h4&gt;&lt;h4 id=&quot;eclispe版本：Eclipse-Mars-2&quot;&gt;&lt;a href=&quot;#eclispe版本：Eclipse-Mars-2&quot; class=&quot;headerlink&quot; title=&quot;eclispe版本：Eclipse Mars 2&quot;&gt;&lt;/a&gt;eclispe版本：Eclipse Mars 2&lt;/h4&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="eclispe" scheme="http://jchanji.github.io/tags/eclispe/"/>
    
      <category term="CentOS 7" scheme="http://jchanji.github.io/tags/CentOS-7/"/>
    
  </entry>
  
  <entry>
    <title>伪分布式hbase安装配置</title>
    <link href="http://jchanji.github.io/year/08/30/hbase_step/"/>
    <id>http://jchanji.github.io/year/08/30/hbase_step/</id>
    <published>2017-08-30T14:36:02.631Z</published>
    <updated>2017-09-03T06:37:53.568Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>网上有很多的教程，大体流程都差不多，但是在很多细节配置方面有点区别，本教程适用于伪分布式环境下（一般自己电脑上练习伪分布式够了）的hbase的基本安装配置。hadoop伪分布式环境已经搭建好,如果没有搭建好，推荐教程 <a href="http://www.powerxing.com/install-hadoop-in-centos/" target="_blank" rel="noopener">hadoop伪分布式教程</a>,hbase官方<a href="http://abloz.com/hbase/book.html" target="_blank" rel="noopener">中文文档</a></p></blockquote><a id="more"></a><h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol><li>CentOS7</li><li>jdk:openjdk1.7.0_141</li><li>hadoop：2.6.0</li><li>hbase:0.98.13</li><li>一定要注意jdk,hadoop和hbase的版本匹配问题,可到官网查看！</li></ol><h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2><p>1.<a href="http://archive.apache.org/dist/hbase/0.98.13/hbase-0.98.13-hadoop2-bin.tar.gz" target="_blank" rel="noopener">hbase-0.98.13-hadoop2-bin.tar.gz</a></p><h2 id="三、安装配置"><a href="#三、安装配置" class="headerlink" title="三、安装配置"></a>三、安装配置</h2><h3 id="1、解压文件到指定目录"><a href="#1、解压文件到指定目录" class="headerlink" title="1、解压文件到指定目录"></a>1、解压文件到指定目录</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hbase-0.98.13-hadoop2-bin.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure><h3 id="2、重命名"><a href="#2、重命名" class="headerlink" title="2、重命名"></a>2、重命名</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo mv [解压后的文件名] [hbase]</span><br></pre></td></tr></table></figure><h3 id="3、修改hbase-site-xml"><a href="#3、修改hbase-site-xml" class="headerlink" title="3、修改hbase-site.xml"></a>3、修改hbase-site.xml</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /hbase/conf</span><br><span class="line">sudo vim hbase-site.xml</span><br></pre></td></tr></table></figure><p>将内容改为<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span></span><br><span class="line"><span class="code">    &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt;</span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span></span><br><span class="line"><span class="code">    &lt;value&gt;/usr/local/hbase/data/zkData&lt;/value&gt;</span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="code">    &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span></span><br><span class="line"><span class="code">    &lt;value&gt;true&lt;/value&gt;</span></span><br><span class="line">  <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure></p><p>说明：<br><br>1、很多教程的hbase.rootdir的hdfs的端口都和官网配置一样是8020，这里根据你自己的实际端口号配置，我的默认的为9000（一般都是），如果端口配置错误的话，之后的进程都能启动，但是在hdfs中没有创建hbase文件，也不能通过60010端口访问web UI.<br><br>2、dataDir的目录可以自己定义，不需要预先创建，hbase会根据配置自动生成。</p><h3 id="3、修改hbase-env-sh"><a href="#3、修改hbase-env-sh" class="headerlink" title="3、修改hbase-env.sh"></a>3、修改hbase-env.sh</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim hbase-env.sh</span><br></pre></td></tr></table></figure><p>添加自己的JAVA_HOME路径<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk/</span><br></pre></td></tr></table></figure></p><h3 id="4、修改regionservers"><a href="#4、修改regionservers" class="headerlink" title="4、修改regionservers"></a>4、修改regionservers</h3><p>在/etc/hosts文件中添加主机名映射，再regionservers中默认的localhost改为主机名<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim etc/hosts</span><br></pre></td></tr></table></figure></p><p>在最后一行添加 127.0.0.1 master<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim regionservers</span><br></pre></td></tr></table></figure></p><p>将localhost改为mater<br><br>说明：如果ip映射出现问题后面的regionserver会启动不了</p><h3 id="5、启动服务"><a href="#5、启动服务" class="headerlink" title="5、启动服务"></a>5、启动服务</h3><p>首先先启动hadoop<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-all.sh</span><br></pre></td></tr></table></figure></p><p>再启动hbase<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/hbase/bin</span><br><span class="line">./hbase-daemon.sh start zookeeper</span><br><span class="line">./hbase-daemon.sh start regionserver</span><br><span class="line">./hbase-daemon.sh start master</span><br></pre></td></tr></table></figure></p><h3 id="6、查看web-UI"><a href="#6、查看web-UI" class="headerlink" title="6、查看web UI"></a>6、查看web UI</h3><p>在浏览器中输入localhost:60010<br><br>如果能正常显示页面说明配置成功<br><br>说明：刚开启服务后由于hadoop处于安全模式导致不能访问，可以等几十秒再次访问或者通过命令<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop dfsadmin -safemode leave</span><br></pre></td></tr></table></figure></p><p>解除保护</p><h2 id="四、常用的一些命令"><a href="#四、常用的一些命令" class="headerlink" title="四、常用的一些命令"></a>四、常用的一些命令</h2><h3 id="1、从hdfs导入导出表"><a href="#1、从hdfs导入导出表" class="headerlink" title="1、从hdfs导入导出表"></a>1、从hdfs导入导出表</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1）导入</span><br><span class="line">./hbase org.apache.hadoop.hbase.mapreduce.Driver import 表名    数据文件位置</span><br><span class="line"></span><br><span class="line">2)导出</span><br><span class="line">./hbase org.apache.hadoop.hbase.mapreduce.Driver export 表名    数据文件位置</span><br></pre></td></tr></table></figure><p>注意：直接操作会报没有jar包的错误，根据提示将hbase的jar包put进提示的hdfs路径中即可</p><h2 id="五、遇到的错误和解决办法"><a href="#五、遇到的错误和解决办法" class="headerlink" title="五、遇到的错误和解决办法"></a>五、遇到的错误和解决办法</h2><h3 id="1、无法启动HRegionServer和HMaster"><a href="#1、无法启动HRegionServer和HMaster" class="headerlink" title="1、无法启动HRegionServer和HMaster"></a>1、无法启动HRegionServer和HMaster</h3><p>报错日志<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">2017-06-13 19:10:12,458 ERROR [main] master.HMasterCommandLine: Master exiting</span><br><span class="line">java.lang.RuntimeException: Failed construction of Master: class org.apache.hadoop.hbase.master.HMaster</span><br><span class="line">  at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:3033)</span><br><span class="line">  at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:193)</span><br><span class="line">  at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:135)</span><br><span class="line">  at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)</span><br><span class="line">  at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:126)</span><br><span class="line">  at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:3047)</span><br><span class="line">Caused by: java.net.BindException: 无法指定被请求的地址</span><br><span class="line">  at sun.nio.ch.Net.bind0(Native Method)</span><br><span class="line">  at sun.nio.ch.Net.bind(Net.java:463)</span><br><span class="line">  at sun.nio.ch.Net.bind(Net.java:455)</span><br><span class="line">  at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223)</span><br><span class="line">  at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:74)</span><br><span class="line">  at org.apache.hadoop.hbase.ipc.RpcServer.bind(RpcServer.java:2488)</span><br><span class="line">  at org.apache.hadoop.hbase.ipc.RpcServer$Listener.<span class="xml"><span class="tag">&lt;<span class="name">init</span>&gt;</span></span>(RpcServer.java:590)</span><br><span class="line">  at org.apache.hadoop.hbase.ipc.RpcServer.<span class="xml"><span class="tag">&lt;<span class="name">init</span>&gt;</span></span>(RpcServer.java:1956)</span><br><span class="line">  at org.apache.hadoop.hbase.master.HMaster.<span class="xml"><span class="tag">&lt;<span class="name">init</span>&gt;</span></span>(HMaster.java:507)</span><br><span class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)</span><br><span class="line">  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">  at java.lang.reflect.Constructor.newInstance(Constructor.java:526)</span><br><span class="line">  at org.apache.hadoop.hbase.master.HMaster.constructMaster(HMaster.java:3028)</span><br><span class="line">  ... 5 more</span><br></pre></td></tr></table></figure></p><p>解决办法<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们可以看到Caused by: java.net.BindException: 无法指定被请求的地址，所以有可能是外网的的影响，所以先关闭网络连接，再启动服务，发现成功了，然后再开启网络。</span><br></pre></td></tr></table></figure></p><h3 id="2、启动hbase服务时找不到pid文件"><a href="#2、启动hbase服务时找不到pid文件" class="headerlink" title="2、启动hbase服务时找不到pid文件"></a>2、启动hbase服务时找不到pid文件</h3><p>问题原因<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1. </span>hbase进行大量的插入时region server 所分配的内存堆过小</span><br><span class="line"><span class="bullet">2. </span>pid文件保存在tmp目录下容易丢失。</span><br></pre></td></tr></table></figure></p><p>解决办法<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1. </span>在hb的hbase-env.sh中</span><br><span class="line"><span class="section"># The maximum amount of heap to use, in MB. Default is 1000.</span></span><br><span class="line"><span class="section"># export HBASE_HEAPSIZE=1000</span></span><br><span class="line">将1000改成30720</span><br><span class="line"></span><br><span class="line"><span class="bullet">2. </span>在hbase-env.sh中修改pid文件的存放路径：</span><br><span class="line">在hbase-env.sh中下面的文字默认是注释掉的，放开即可，也可以自己指定存放位置：</span><br><span class="line"><span class="section"># The directory where pid files are stored. /tmp by default.  </span></span><br><span class="line"> export HBASE<span class="emphasis">_PID_</span>DIR=/var/hadoop/pids</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;网上有很多的教程，大体流程都差不多，但是在很多细节配置方面有点区别，本教程适用于伪分布式环境下（一般自己电脑上练习伪分布式够了）的hbase的基本安装配置。hadoop伪分布式环境已经搭建好,如果没有搭建好，推荐教程 &lt;a href=&quot;http://www.powerxing.com/install-hadoop-in-centos/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;hadoop伪分布式教程&lt;/a&gt;,hbase官方&lt;a href=&quot;http://abloz.com/hbase/book.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;中文文档&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
      <category term="hbase" scheme="http://jchanji.github.io/tags/hbase/"/>
    
  </entry>
  
  <entry>
    <title>使用java将文件夹下的文件批量的从gbk编码转化成utf-8编码</title>
    <link href="http://jchanji.github.io/year/08/30/codeparse_gbk2utf/"/>
    <id>http://jchanji.github.io/year/08/30/codeparse_gbk2utf/</id>
    <published>2017-08-30T14:32:48.163Z</published>
    <updated>2017-09-03T05:51:28.647Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>使用java,对文件遍历，修改文件编码</p></blockquote><a id="more"></a><h3 id="一、建立java项目，导入commons-io-jar"><a href="#一、建立java项目，导入commons-io-jar" class="headerlink" title="一、建立java项目，导入commons-io-*.jar"></a>一、建立java项目，导入<a href="http://mirror.bit.edu.cn/apache//commons/io/binaries/commons-io-2.5-bin.zip" title="commons-io-*.jar" target="_blank" rel="noopener">commons-io-*.jar</a></h3><h3 id="二、新建class，文件名随便起，我的是Codeparse-包名为exchangecode"><a href="#二、新建class，文件名随便起，我的是Codeparse-包名为exchangecode" class="headerlink" title="二、新建class，文件名随便起，我的是Codeparse,包名为exchangecode"></a>二、新建class，文件名随便起，我的是Codeparse,包名为exchangecode</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">package exchangecode;</span><br><span class="line"></span><br><span class="line">import java.io.File;</span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Collection;</span><br><span class="line"></span><br><span class="line">import org.apache.commons.io.FileUtils;</span><br><span class="line"></span><br><span class="line">public class Codeparse &#123;</span><br><span class="line"></span><br><span class="line"><span class="code">    public static void main(String[] args) throws IOException &#123;</span></span><br><span class="line"><span class="code">        //GBK编码格式源码路径,根据自己的文件路径写 </span></span><br><span class="line"><span class="code">        String srcDirPath = "F:\\test"; </span></span><br><span class="line"><span class="code">        //转为UTF-8编码格式源码路径，根据自己的文件路径写 </span></span><br><span class="line"><span class="code">        String utf8DirPath ="F:\\out"; </span></span><br><span class="line"><span class="code">                </span></span><br><span class="line"><span class="code">        //获取所有txt文件,如果是其他类型的文件，将&#123;“txt”&#125;中的txt换为其他文件的后缀名</span></span><br><span class="line"><span class="code">        @SuppressWarnings("unchecked")</span></span><br><span class="line"><span class="code">        Collection&lt;File&gt; javaGbkFileCol =  FileUtils.listFiles(new File(srcDirPath), new String[]&#123;"txt"&#125;, true); </span></span><br><span class="line"><span class="code">                </span></span><br><span class="line"><span class="code">        for (File javaGbkFile : javaGbkFileCol) &#123; </span></span><br><span class="line"><span class="code">              //UTF8格式文件路径 </span></span><br><span class="line"><span class="code">              String utf8FilePath = utf8DirPath+javaGbkFile.getAbsolutePath().substring(srcDirPath.length()); </span></span><br><span class="line"><span class="code">              </span></span><br><span class="line"><span class="code">              //使用GBK读取数据，然后用UTF-8写入数据 </span></span><br><span class="line"><span class="code">              FileUtils.writeLines(new File(utf8FilePath), "UTF-8", FileUtils.readLines(javaGbkFile, "GBK"));        </span></span><br><span class="line"><span class="code">        &#125;</span></span><br><span class="line"><span class="code">        System.out.println("success!");</span></span><br><span class="line"><span class="code">    &#125;</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="三、运行"><a href="#三、运行" class="headerlink" title="三、运行"></a>三、运行</h3>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;使用java,对文件遍历，修改文件编码&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-others" scheme="http://jchanji.github.io/categories/others/"/>
    
    
      <category term="-java" scheme="http://jchanji.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu16.04 anaconda环境下安装tensoflow(GPU)</title>
    <link href="http://jchanji.github.io/year/08/29/tensorflow_step/"/>
    <id>http://jchanji.github.io/year/08/29/tensorflow_step/</id>
    <published>2017-08-29T13:38:28.881Z</published>
    <updated>2017-09-03T06:36:41.389Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><blockquote><p>目前深度学习炙手可热的框架毫无疑问是tensorflow,在本教程主要介绍tensorflow在anaconda中的安装，在火车上实在是无聊，电脑又没有网络，只能打发一下时间。</p></blockquote><a id="more"></a><h2 id="一、版本"><a href="#一、版本" class="headerlink" title="一、版本"></a>一、版本</h2><ol><li>anaconda 4.3.21</li><li>python 3.5</li><li>tensorflow 1.2.0(github上目前最新版本)</li><li>ubuntu 16.04<h2 id="二、下载"><a href="#二、下载" class="headerlink" title="二、下载"></a>二、下载</h2></li><li><a href="">Anaconda3-4.4.0-Linux-x86_64.sh</a></li><li><a href="">tensorflow_gpu-1.2.1-cp35-cp35m-linux_x86_64.whl</a><br>##三、注意事项<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、电脑上已经安装了cadu8.0和cudnn5.1环境</span><br><span class="line">2、tensorflow1.2.0版本支持cadu8.0,其他低版本的tensorflow会发生找不到依赖的错误。</span><br><span class="line">３、安装后运行会出现CPU computations,cpu指令集优化的警告，目前没有很好的解决办法，不过影响不大，因为我们主要使用的是GPU.</span><br></pre></td></tr></table></figure></li></ol><h2 id="四、安装配置"><a href="#四、安装配置" class="headerlink" title="四、安装配置"></a>四、安装配置</h2><h3 id="1、安装anaconda"><a href="#1、安装anaconda" class="headerlink" title="1、安装anaconda"></a>1、安装anaconda</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo bash Anaconda3-4.4.0-Linux-x86_64.sh</span><br></pre></td></tr></table></figure><h3 id="2、-安装python3-5环境"><a href="#2、-安装python3-5环境" class="headerlink" title="2、　安装python3.5环境"></a>2、　安装python3.5环境</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n tensorflow python = 3.5</span><br></pre></td></tr></table></figure><h3 id="3、安装tensorflow"><a href="#3、安装tensorflow" class="headerlink" title="3、安装tensorflow"></a>3、安装tensorflow</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source activate tensorflow #进入刚才安装好的环境</span><br><span class="line">cd ~/下载　＃进入tensorflow　的pip安装文件的目录</span><br><span class="line">pip install tensorflow<span class="emphasis">_gpu-1.2.1-cp35-cp35m-linux_</span>x86_64.whl #安装tensorflow</span><br></pre></td></tr></table></figure><h2 id="五、测试"><a href="#五、测试" class="headerlink" title="五、测试"></a>五、测试</h2><p>进入python环境<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python</span><br></pre></td></tr></table></figure></p><p>运行代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">sess = tf.Session()</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">10</span>)</span><br><span class="line">b = tf.constant(<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">print(sess.run(a+b))</span><br></pre></td></tr></table></figure></p><p>输出结果<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;目前深度学习炙手可热的框架毫无疑问是tensorflow,在本教程主要介绍tensorflow在anaconda中的安装，在火车上实在是无聊，电脑又没有网络，只能打发一下时间。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="deeplearning" scheme="http://jchanji.github.io/tags/deeplearning/"/>
    
      <category term="tensorflow" scheme="http://jchanji.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>K-means算法-对31省消费水平分类</title>
    <link href="http://jchanji.github.io/year/08/29/citycosumption/"/>
    <id>http://jchanji.github.io/year/08/29/citycosumption/</id>
    <published>2017-08-29T13:17:37.902Z</published>
    <updated>2018-01-08T16:14:14.475Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言："><a href="#前言：" class="headerlink" title="前言："></a>前言：</h2><blockquote><p>此篇笔记主要根据南京大学礼欣老师的<a href="http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce" target="_blank" rel="noopener">《Python机器学习应用》</a>整理而成，详细内容请看礼欣老师的mooc课程。</p></blockquote><a id="more"></a><h2 id="数据介绍："><a href="#数据介绍：" class="headerlink" title="数据介绍："></a>数据介绍：</h2><p>现有1999年全国31个省份城镇居民家庭平均每人全年消费性支出的八个主<br>要变量数据，这八个变量分别是：食品、衣着、家庭设备用品及服务、医疗<br>保健、交通和通讯、娱乐教育文化服务、居住以及杂项商品和服务。利用已<br>有数据，对31个省份进行聚类。。数据下载<a href="https://github.com/jChanJi/static_resource/blob/master/clustering/TestData.txt" target="_blank" rel="noopener">点击我</a></p><h2 id="主要参数"><a href="#主要参数" class="headerlink" title="主要参数"></a>主要参数</h2><ol><li>n_clusters：用于指定聚类中心的个数</li><li>init：初始聚类中心的初始化方法</li><li>max_iter：最大的迭代次数</li><li>一般调用时只用给出n_clusters即可，init<br>默认是k-means++，max_iter默认是300</li><li>data：加载的数据</li><li>label：聚类后各数据所属的标签</li><li>axis: 按行求和</li><li>fit_predict()：计算簇中心以及为簇分配序号</li></ol><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filePath)</span>:</span></span><br><span class="line">    fr = open(filePath,<span class="string">'r+'</span>)</span><br><span class="line">    lines = fr.readlines()</span><br><span class="line">    retData = []</span><br><span class="line">    retCityName = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        items = line.strip().split(<span class="string">","</span>)</span><br><span class="line">        retCityName.append(items[<span class="number">0</span>])</span><br><span class="line">        retData.append([float(items[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(items))])</span><br><span class="line">    <span class="keyword">return</span> retData,retCityName</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    data,cityName = loadData(<span class="string">'F:/data/clustering/city.txt'</span>)</span><br><span class="line">    km = KMeans(n_clusters=<span class="number">4</span>)</span><br><span class="line">    label = km.fit_predict(data)</span><br><span class="line">    expenses = np.sum(km.cluster_centers_,axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#print(expenses)</span></span><br><span class="line">    CityCluster = [[],[],[],[]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(cityName)):</span><br><span class="line">        CityCluster[label[i]].append(cityName[i])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(CityCluster)):</span><br><span class="line">        print(<span class="string">"Expenses:%.2f"</span> % expenses[i])</span><br><span class="line">        print(CityCluster[i])</span><br></pre></td></tr></table></figure><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Expenses:4441.04</span><br><span class="line">['安徽', '湖南', '湖北', '广西', '海南', '四川', '云南']</span><br><span class="line">Expenses:7754.66</span><br><span class="line">['北京', '上海', '广东']</span><br><span class="line">Expenses:5567.33</span><br><span class="line">['天津', '江苏', '浙江', '福建', '重庆', '西藏']</span><br><span class="line">Expenses:3788.76</span><br><span class="line">['河北', '山西', '内蒙古', '辽宁', '吉林', '黑龙江', '江西', '山东', '河南', '贵州', '陕西', '甘肃', '青海', '宁夏', '新疆']</span><br></pre></td></tr></table></figure><h2 id="注：当改变簇n-clusters为8-CityCluster长度也设置为8-时结果"><a href="#注：当改变簇n-clusters为8-CityCluster长度也设置为8-时结果" class="headerlink" title="注：当改变簇n_clusters为8(CityCluster长度也设置为8)时结果"></a>注：当改变簇n_clusters为8(CityCluster长度也设置为8)时结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Expenses:<span class="number">3497.85</span></span><br><span class="line">[<span class="string">'山西'</span>, <span class="string">'内蒙古'</span>, <span class="string">'黑龙江'</span>, <span class="string">'河南'</span>, <span class="string">'宁夏'</span>]</span><br><span class="line">Expenses:<span class="number">5311.98</span></span><br><span class="line">[<span class="string">'天津'</span>, <span class="string">'江苏'</span>, <span class="string">'重庆'</span>, <span class="string">'云南'</span>]</span><br><span class="line">Expenses:<span class="number">7010.02</span></span><br><span class="line">[<span class="string">'北京'</span>, <span class="string">'浙江'</span>]</span><br><span class="line">Expenses:<span class="number">7517.80</span></span><br><span class="line">[<span class="string">'广东'</span>]</span><br><span class="line">Expenses:<span class="number">4357.67</span></span><br><span class="line">[<span class="string">'安徽'</span>, <span class="string">'湖南'</span>, <span class="string">'湖北'</span>, <span class="string">'广西'</span>, <span class="string">'海南'</span>, <span class="string">'四川'</span>]</span><br><span class="line">Expenses:<span class="number">5287.90</span></span><br><span class="line">[<span class="string">'福建'</span>, <span class="string">'西藏'</span>]</span><br><span class="line">Expenses:<span class="number">8247.69</span></span><br><span class="line">[<span class="string">'上海'</span>]</span><br><span class="line">Expenses:<span class="number">3934.21</span></span><br><span class="line">[<span class="string">'河北'</span>, <span class="string">'辽宁'</span>, <span class="string">'吉林'</span>, <span class="string">'江西'</span>, <span class="string">'山东'</span>, <span class="string">'贵州'</span>, <span class="string">'陕西'</span>, <span class="string">'甘肃'</span>, <span class="string">'青海'</span>, <span class="string">'新疆'</span>]</span><br></pre></td></tr></table></figure><p>我们发现簇多所分的层次就越多</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;前言：&quot;&gt;&lt;a href=&quot;#前言：&quot; class=&quot;headerlink&quot; title=&quot;前言：&quot;&gt;&lt;/a&gt;前言：&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;此篇笔记主要根据南京大学礼欣老师的&lt;a href=&quot;http://www.icourse163.org/learn/BIT-1001872001?tid=1001965001#/learn/announce&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;《Python机器学习应用》&lt;/a&gt;整理而成，详细内容请看礼欣老师的mooc课程。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="-machinelearning" scheme="http://jchanji.github.io/categories/machinelearning/"/>
    
    
      <category term="python" scheme="http://jchanji.github.io/tags/python/"/>
    
      <category term="machinelearning" scheme="http://jchanji.github.io/tags/machinelearning/"/>
    
      <category term="K-means" scheme="http://jchanji.github.io/tags/K-means/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7安装配置hadoop集群</title>
    <link href="http://jchanji.github.io/year/08/29/hadoop/"/>
    <id>http://jchanji.github.io/year/08/29/hadoop/</id>
    <published>2017-08-29T11:10:45.044Z</published>
    <updated>2017-09-03T06:37:47.230Z</updated>
    
    <content type="html"><![CDATA[<h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><h3 id="简单的的vim命令"><a href="#简单的的vim命令" class="headerlink" title="简单的的vim命令"></a><a href="http://www.cnblogs.com/jeakon/archive/2012/08/13/2816802.html" target="_blank" rel="noopener">简单的的vim命令</a></h3><h3 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a><a href="http://www.weixuehao.com/archives/25" target="_blank" rel="noopener">linux常用命令</a></h3><h3 id="linux命令查找网站"><a href="#linux命令查找网站" class="headerlink" title="linux命令查找网站"></a><a href="http://man.linuxde.net/" target="_blank" rel="noopener">linux命令查找网站</a></h3><h1 id="CentOS-下安装hadoop"><a href="#CentOS-下安装hadoop" class="headerlink" title="CentOS 下安装hadoop"></a>CentOS 下安装hadoop<br></h1><a id="more"></a><h2 id="一、安装Vmware-12"><a href="#一、安装Vmware-12" class="headerlink" title="一、安装Vmware 12"></a>一、安装Vmware 12</h2><blockquote><ol><li>官网下载<a href="http://www.vmware.com/cn/products/workstation/workstation-evaluation.html" title="Vmware 下载地址" target="_blank" rel="noopener">VMware-Workstation-Full-*.bundle</a></li><li>sudo ssh./VMware-Workstation-Full-*.bundle</li><li>破解：破解工具<a href="http://chanji-1252400803.costj.myqcloud.com/VMware12.Keymaker.exe" title="VMware12.Keymaker" target="_blank" rel="noopener">VMware12.Keymaker</a> </li><li>根据提示安装</li></ol></blockquote><h2 id="二、安装CentOS-7"><a href="#二、安装CentOS-7" class="headerlink" title="二、安装CentOS 7"></a>二、安装CentOS 7</h2><blockquote><ol><li>下载<a href="http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-DVD-1611.iso" title="CentOS 7" target="_blank" rel="noopener">镜像</a></li><li>新建虚拟机，根据提示操作（注意选择安装GNOME桌面），设置主机名为CentOSMaster点击安装</li><li>设置root密码和添加hadoop用户（设置为管理员）</li><li>等待安装，完成后重启，连接网络，完成配置 </li><li>语言选择汉语（pinyin）</li></ol></blockquote><h2 id="三、-安装hadoop集群"><a href="#三、-安装hadoop集群" class="headerlink" title="三、 安装hadoop集群"></a>三、 安装hadoop集群</h2><h4 id="参考教程："><a href="#参考教程：" class="headerlink" title="参考教程："></a>参考教程：</h4><h4 id="单机-伪分布式：http-www-powerxing-com-install-hadoop-in-centos"><a href="#单机-伪分布式：http-www-powerxing-com-install-hadoop-in-centos" class="headerlink" title="单机/伪分布式：http://www.powerxing.com/install-hadoop-in-centos/"></a>单机/伪分布式：<a href="http://www.powerxing.com/install-hadoop-in-centos/" target="_blank" rel="noopener">http://www.powerxing.com/install-hadoop-in-centos/</a></h4><h4 id="分布式集群：http-www-powerxing-com-install-hadoop-cluster"><a href="#分布式集群：http-www-powerxing-com-install-hadoop-cluster" class="headerlink" title="分布式集群：http://www.powerxing.com/install-hadoop-cluster/"></a>分布式集群：<a href="http://www.powerxing.com/install-hadoop-cluster/" target="_blank" rel="noopener">http://www.powerxing.com/install-hadoop-cluster/</a></h4><blockquote><h3 id="1-创建hadoop用户-如果没有"><a href="#1-创建hadoop用户-如果没有" class="headerlink" title="1. 创建hadoop用户(如果没有)"></a>1. 创建hadoop用户(如果没有)</h3></blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">1. </span>su                               # 上述提到的以 root 用户登录</span><br><span class="line"><span class="bullet">2. </span>useradd -m hadoop -s /bin/bash   # 创建新用户hadoop</span><br><span class="line"><span class="bullet">3. </span>passwd hadoop                    #设置密码</span><br><span class="line"><span class="bullet">4. </span>visudo                           #增加管理员权限</span><br></pre></td></tr></table></figure><p>找到 root  ALL=(ALL)  ALL 这行,下一行增加:hadoop ensp; ensp; ALL=(ALL)  ensp;ensp; ALL<br></p><blockquote><h3 id="2-安装Java环境-在hadoop用户下"><a href="#2-安装Java环境-在hadoop用户下" class="headerlink" title="2. 安装Java环境(在hadoop用户下)"></a>2. 安装Java环境(在hadoop用户下)</h3></blockquote><ol><li>安装openjdk<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install java-1.7.0-openjdk java-1.7.0-openjdk-devel</span><br></pre></td></tr></table></figure></li></ol><p>如果遇到yum进程被占用，删除yum.pid<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -rf /var/run/yum.pid</span><br></pre></td></tr></table></figure></p><ol><li><p>配置JAVA_HOME<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim ~/.bashrc</span><br></pre></td></tr></table></figure><p> 在文件最后面添加如下单独一行（指向 JDK 的安装位置)<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk</span><br></pre></td></tr></table></figure></li><li><p>使配置生效<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li><li><p>检验是否配置成功<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo $JAVA_HOME  #检验变量值</span><br><span class="line">java -version </span><br><span class="line">   %JAVA_HOME/bin/java -version</span><br></pre></td></tr></table></figure></li></ol><p>如果java -version 和 %JAVA_HOME/bin/java -version一样表示成功,否则看5<br><br></p><ol><li><p>如果和以前的jdk版本冲突的:<br><br> 查找当前的安装的jdk版本<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">    rpm -q |grep java</span></span><br><span class="line"><span class="code">```  </span></span><br><span class="line"><span class="code">    删除openjdk版本意外的版本&lt;br&gt;</span></span><br><span class="line"><span class="code">```markdown</span></span><br><span class="line"><span class="code">    rpm -e --nodeps java版本的名称</span></span><br><span class="line"><span class="code">```  </span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">&gt;### 3.安装配置hadoop2集群</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">1. 下载hadoop压缩包，选择[hadoop-2.x.y.tar.gz][5]文件,这里我选择的是2.6.1版本&lt;br&gt;&lt;br&gt;</span></span><br><span class="line"><span class="code">2. 解压&lt;br&gt;</span></span><br><span class="line"><span class="code">```markdown</span></span><br><span class="line"><span class="code">    sudo tar -zxf ~/下载/hadoop-2.6.1.tar.gz -C /usr/local    # 解压到/usr/local中</span></span><br><span class="line"><span class="code">    cd /usr/local/  #打开/usr/local目录</span></span><br><span class="line"><span class="code">    sudo mv ./hadoop-2.6.1/ ./hadoop  # 将文件夹名改为hadoop</span></span><br><span class="line"><span class="code">    sudo chown -R hadoop:hadoop ./hadoop  # 修改文件权限，冒号后没有空格</span></span><br></pre></td></tr></table></figure></li><li><p>显示版本<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd  /usr/local/hadoop</span><br><span class="line">./bin/hadoop version</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gedit ~/.bashrc (vim ~/.bashrc)</span><br></pre></td></tr></table></figure><p> 在文件中添加：<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">#Hadoop Environment Variables</span></span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export HADOOP<span class="emphasis">_INSTALL=$HADOOP_</span>HOME</span><br><span class="line">export HADOOP<span class="emphasis">_MAPRED_</span>HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP<span class="emphasis">_COMMON_</span>HOME=$HADOOP_HOME</span><br><span class="line">export HADOOP<span class="emphasis">_HDFS_</span>HOME=$HADOOP_HOME</span><br><span class="line">export YARN<span class="emphasis">_HOME=$HADOOP_</span>HOME</span><br><span class="line">export HADOOP<span class="emphasis">_COMMON_</span>LIB<span class="emphasis">_NATIVE_</span>DIR=$HADOOP_HOME/lib/native</span><br><span class="line">export PATH=$PATH:$HADOOP<span class="emphasis">_HOME/sbin:$HADOOP_</span>HOME/bin</span><br></pre></td></tr></table></figure></li><li><p>使配置生效<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">source ~/.bashrc</span><br><span class="line">hadoop version #验证</span><br></pre></td></tr></table></figure></li><li><p>关闭虚拟机，克隆两个虚拟机，命名为CentOSSlave1,CentOSSlave2,注意要选择完整克隆<br><br></p></li><li>依次打开CentOSMaster,CentOSSlave1,CentOSSlave2,查看各自的ip,(ens**下的inet内容)<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">    ifconfig</span></span><br><span class="line"><span class="code">```   </span></span><br><span class="line"><span class="code">9. 修改各自的主机名(Matser，Slave1，Slave2)：&lt;br&gt;</span></span><br><span class="line"><span class="code">在master节点上：&lt;br&gt;</span></span><br><span class="line"><span class="code">```markdown</span></span><br><span class="line"><span class="code">    sudo hostnamectl  set-hostname Master</span></span><br></pre></td></tr></table></figure></li></ol><p>在Slave1节点上：<br><br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hostnamectl  set-hostname Slave1</span><br></pre></td></tr></table></figure></p><p>在Slave2节点上：<br><br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo hostnamectl  set-hostname Slave2</span><br></pre></td></tr></table></figure></p><ol><li><p>修改ip映射（三个节点都要修改）：<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/hosts</span><br></pre></td></tr></table></figure><p>在末尾加上:<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip1   Master</span><br><span class="line">ip2   Slave1</span><br><span class="line">   ip3   Slave2</span><br></pre></td></tr></table></figure></li><li><p>设置开机启动网络<br><br>修改 /etc/sysconfig/network-scripts/ifcfg-ens*（具体文件名每个人有可能不同）,将最后一行的ONBOOT 改为yes    </p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">    vim  /etc/sysconfig/network-scripts/ifcfg-ens33  #我的文件名称为ifcfg-ens33</span></span><br><span class="line"><span class="code">``` </span></span><br><span class="line"><span class="code">12. 通过在终端分别执行ping Master，ping Slave1，ping Slave2,看是否能通，ctrl+C停止&lt;br&gt;&lt;br&gt;</span></span><br><span class="line"><span class="code">13. 主节点Master使用ssh无密钥登陆节点（注意ssh登陆的用户名）&lt;br&gt;&lt;br&gt;</span></span><br><span class="line"><span class="code">    a. 首先生成 Master 节点的公匙，在 Master 节点的终端中执行：&lt;br&gt;</span></span><br><span class="line"><span class="code">```markdown</span></span><br><span class="line"><span class="code">       su hadoop               #登陆到hadoop用户,所有操作都使hadoop用户的行为</span></span><br><span class="line"><span class="code">       cd ~/.ssh               # 如果没有该目录，先执行一次ssh Master</span></span><br><span class="line"><span class="code">       rm ./id_rsa*            # 删除之前生成的公匙（如果有）</span></span><br><span class="line"><span class="code">       ssh-keygen -t rsa       # 一直按回车就可以</span></span><br></pre></td></tr></table></figure><p>b. 让Master节点需能无密码ssh本机，在 Master 节点上执行：<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat ./id<span class="emphasis">_rsa.pub &gt;&gt; ./authorized_</span>keys</span><br><span class="line">chmod 600 ./authorized_keys    # 修改文件权限</span><br></pre></td></tr></table></figure><p>  完成后可执行 ssh Master 验证一下（可能需要输入 yes，成功后执行 exit 返回原来的终端）。<br><br><br>c. 将上公匙传输到 Slave1 节点(Slave2也是一样操作将Slave1改成Slave2):<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="code">       scp ~/.ssh/id_rsa.pub hadoop@Slave1:/home/hadoop/&lt;br&gt;&lt;br&gt;</span></span><br><span class="line"><span class="code">``` </span></span><br><span class="line"><span class="code">    d. 在Slave1和Slave2节点上 操作：&lt;br&gt;</span></span><br><span class="line"><span class="code">```markdown</span></span><br><span class="line"><span class="code">       mkdir ~/.ssh       # 如果不存在该文件夹需先创建，若已存在则忽略</span></span><br><span class="line"><span class="code">       cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span></span><br><span class="line"><span class="code">       rm ~/id_rsa.pub    # 用完就可以删掉了</span></span><br><span class="line"><span class="code">`</span></span><br></pre></td></tr></table></figure><p>e. 在Master节点上ssh Slave1和Slave2，验证是否能连接上<br><br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh Slave1</span><br><span class="line"></span><br><span class="line">ssh Slave2</span><br></pre></td></tr></table></figure></li><li><p>在Master节点上操作，cd /usr/local/hadoop/etc/hadoop,进入root模式<br><br>a. 修改slaves文件,将localhost注释，添加Slave1,换行，Slave2<br><br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim slaves</span><br></pre></td></tr></table></figure><p>b. 修改core-site.xml<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="xml"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span></span><br><span class="line"><span class="code">       &lt;property&gt;</span></span><br><span class="line"><span class="code">            &lt;name&gt;fs.defaultFS&lt;/name&gt;</span></span><br><span class="line"><span class="code">            &lt;value&gt;hdfs://Master:9000&lt;/value&gt;</span></span><br><span class="line"><span class="code">        &lt;/property&gt;</span></span><br><span class="line"><span class="code">        &lt;property&gt;</span></span><br><span class="line"><span class="code">            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span></span><br><span class="line"><span class="code">            &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt;</span></span><br><span class="line">                <span class="xml"><span class="tag">&lt;<span class="name">description</span>&gt;</span></span>tmp directories<span class="xml"><span class="tag">&lt;/<span class="name">description</span>&gt;</span></span></span><br><span class="line">            <span class="xml"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span></span><br><span class="line"><span class="xml"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span></span><br></pre></td></tr></table></figure><p>c. 修改hdfs-site.xml,其中的dfs.replication的value根据Slave的个数填写<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"> <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/usr/local/hadoop/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">`</span><br></pre></td></tr></table></figure><p>d. 重命名 mapred-site.xml.template为mapred-site.xml,并修改mapred-site.xml为：<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">`</span><br></pre></td></tr></table></figure><p>e. 修改yarn.site.xml为：<br></p><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>Master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>配置好后,将Master上的/usr/local/hadoop文件夹复制到各个节点上。如果有临时文件和日志文件先删除,在Master节点上执行:<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local</span><br><span class="line">sudo rm -r ./hadoop/tmp                    # 删除 Hadoop 临时文件</span><br><span class="line">sudo rm -r ./hadoop/logs/*                 # 删除日志文件</span><br><span class="line">tar -zcf ~/hadoop.master.tar.gz ./hadoop   # 先压缩再复制</span><br><span class="line">cd ~</span><br><span class="line">scp ./hadoop.master.tar.gz Slave1:/home/hadoop</span><br></pre></td></tr></table></figure><p>如果有其他节点再执行：scp ./hadoop.master.tar.gz Slave(n):/home/hadoop<br><br></p></li><li><p>分别在slave节点上执行<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo rm -r /usr/local/hadoop    # 删掉旧的（如果存在）</span><br><span class="line">sudo tar -zxf ~/hadoop.master.tar.gz -C /usr/local</span><br><span class="line">sudo chown -R hadoop /usr/local/hadoop   #给hadoop用户读写/usr/local/hadoop的权限</span><br></pre></td></tr></table></figure></li><li><p>首次启动需要先在 Master 节点执行 NameNode 的格式化：<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format       # 首次运行需要执行初始化，之后不需要，status=0，表示成功</span><br></pre></td></tr></table></figure></li><li><p>关闭防火墙(所有机器)：<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld.service    # 关闭firewall<span class="xml"><span class="tag">&lt;</span></span></span><br><span class="line"><span class="xml">systemctl disable firewalld.service # 禁止firewall开机启动</span></span><br></pre></td></tr></table></figure></li><li><p>启动服务<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh</span><br><span class="line">start-dfs.sh</span><br><span class="line">mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure></li><li><p>在master节点上查看java进程<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>如果有JobHistoryServer,SecondaryNameNode,Jsp,ResourceManager,NameNode四个进程代表Master上没问题<br><br></p></li><li><p>在slave节点上执行<br></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure><p>如果有Jps，DataNode,NodeManager,三个节点表示配置成功<br><br></p></li><li>关闭服务<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stop-yarn.sh</span><br><span class="line">stop-dfs.sh</span><br><span class="line">mr-jobhistory-daemon.sh stop historyserver</span><br></pre></td></tr></table></figure></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;基本操作&quot;&gt;&lt;a href=&quot;#基本操作&quot; class=&quot;headerlink&quot; title=&quot;基本操作&quot;&gt;&lt;/a&gt;基本操作&lt;/h2&gt;&lt;h3 id=&quot;简单的的vim命令&quot;&gt;&lt;a href=&quot;#简单的的vim命令&quot; class=&quot;headerlink&quot; title=&quot;简单的的vim命令&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.cnblogs.com/jeakon/archive/2012/08/13/2816802.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;简单的的vim命令&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;linux常用命令&quot;&gt;&lt;a href=&quot;#linux常用命令&quot; class=&quot;headerlink&quot; title=&quot;linux常用命令&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://www.weixuehao.com/archives/25&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;linux常用命令&lt;/a&gt;&lt;/h3&gt;&lt;h3 id=&quot;linux命令查找网站&quot;&gt;&lt;a href=&quot;#linux命令查找网站&quot; class=&quot;headerlink&quot; title=&quot;linux命令查找网站&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://man.linuxde.net/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;linux命令查找网站&lt;/a&gt;&lt;/h3&gt;&lt;h1 id=&quot;CentOS-下安装hadoop&quot;&gt;&lt;a href=&quot;#CentOS-下安装hadoop&quot; class=&quot;headerlink&quot; title=&quot;CentOS 下安装hadoop&quot;&gt;&lt;/a&gt;CentOS 下安装hadoop&lt;br&gt;&lt;/h1&gt;
    
    </summary>
    
      <category term="-bigdata" scheme="http://jchanji.github.io/categories/bigdata/"/>
    
    
      <category term="big data" scheme="http://jchanji.github.io/tags/big-data/"/>
    
      <category term="hadoop" scheme="http://jchanji.github.io/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://jchanji.github.io/year/08/29/hello-world/"/>
    <id>http://jchanji.github.io/year/08/29/hello-world/</id>
    <published>2017-08-29T07:35:40.594Z</published>
    <updated>2017-08-29T13:31:43.559Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
</feed>
